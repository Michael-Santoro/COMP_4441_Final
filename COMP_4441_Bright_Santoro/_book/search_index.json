[["introduction.html", "COMP 4441 Final Project Chapter 1 Introduction", " COMP 4441 Final Project Emma Bright and Michael Santoro 2021-05-29 Chapter 1 Introduction Purpose Wine is an alcoholic beverage made from fermented grape juice. (Puckette) Generally, a wines quality is determined by taste, smell, and visual tests performed by wine experts, sommeliers. These tests grade the wine on subjective measures: Acidity, Sweetness, Alcohol, Tannin and Aroma. Though these tests are subjective, there is overlap between the observation of these characteristics and a wines physiochemical properties. For example, a wines perceived acidity or tartness is based upon the pH of the wine and a wines perceived sweetness is based upon the residual sugar of the wine. The goal of our research is to determine: Can we predict the subjective quality rating of wine based solely on its physiochemical properties? An accurate predictive model for determining an objective quality of wine would enable small wineries and vineyards, especially those from less established wine regions, to more accurately price their inventory and ensure a better return on investment. Statistical or Analytical Method We will be using the the following statistical methods to create predictive models for quality of wine: LASSO Regression, Random Forest, and K-Nearest Neighbor. "],["descriptive-statistics.html", "Chapter 2 Descriptive Statistics 2.1 Dataset 2.2 Exploratory Analysis", " Chapter 2 Descriptive Statistics 2.1 Dataset The dataset utilized for our analysis was collected from the UCI Machine Learning Repository (see References). The dataset consists of 12 elements, 11 physiochemical properties and 1 subjective quality measurement, for both red Vinho Verde wine samples from Portugal. Read in Data redWine&lt;-read.table(&quot;data/winequality-red.csv&quot;,stringsAsFactors = FALSE, sep=&quot;,&quot;,header = TRUE) whiteWine&lt;-read.table(&quot;data/winequality-white.csv&quot;,stringsAsFactors = FALSE, sep=&quot;;&quot;,header = TRUE) redWine$type=&#39;red&#39; whiteWine$type=&#39;white&#39; # create a field that shows whether a wine is red or white based on initial # datasets wine &lt;- rbind(redWine, whiteWine) str(wine) ## &#39;data.frame&#39;: 6497 obs. of 13 variables: ## $ fixed.acidity : num 7.4 7.8 7.8 11.2 7.4 7.4 7.9 7.3 7.8 7.5 ... ## $ volatile.acidity : num 0.7 0.88 0.76 0.28 0.7 0.66 0.6 0.65 0.58 0.5 ... ## $ citric.acid : num 0 0 0.04 0.56 0 0 0.06 0 0.02 0.36 ... ## $ residual.sugar : num 1.9 2.6 2.3 1.9 1.9 1.8 1.6 1.2 2 6.1 ... ## $ chlorides : num 0.076 0.098 0.092 0.075 0.076 0.075 0.069 0.065 0.073 0.071 ... ## $ free.sulfur.dioxide : num 11 25 15 17 11 13 15 15 9 17 ... ## $ total.sulfur.dioxide: num 34 67 54 60 34 40 59 21 18 102 ... ## $ density : num 0.998 0.997 0.997 0.998 0.998 ... ## $ pH : num 3.51 3.2 3.26 3.16 3.51 3.51 3.3 3.39 3.36 3.35 ... ## $ sulphates : num 0.56 0.68 0.65 0.58 0.56 0.56 0.46 0.47 0.57 0.8 ... ## $ alcohol : num 9.4 9.8 9.8 9.8 9.4 9.4 9.4 10 9.5 10.5 ... ## $ quality : int 5 5 5 6 5 5 5 7 7 5 ... ## $ type : chr &quot;red&quot; &quot;red&quot; &quot;red&quot; &quot;red&quot; ... 2.2 Exploratory Analysis 2.2.1 Descriptive Statistics In order to better understand our dataset we performed some initial descriptive statistics. A descriptive statistic is a summary level statistic, such as mean or median, that describes a variable or feature of a dataset. Descriptive statistics is the processing of analyzing the descriptive statistic taken from your dataset. (Descriptive Statistics) The table below shows descriptive statistic measurements for each of the variables in our dataset. stat.desc(wine) ## fixed.acidity volatile.acidity citric.acid residual.sugar chlorides free.sulfur.dioxide total.sulfur.dioxide density ## nbr.val 6.497000e+03 6.497000e+03 6.497000e+03 6.497000e+03 6.497000e+03 6.497000e+03 6.497000e+03 6.497000e+03 ## nbr.null 0.000000e+00 0.000000e+00 1.510000e+02 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 ## nbr.na 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 ## min 3.800000e+00 8.000000e-02 0.000000e+00 6.000000e-01 9.000000e-03 1.000000e+00 6.000000e+00 9.871100e-01 ## max 1.590000e+01 1.580000e+00 1.660000e+00 6.580000e+01 6.110000e-01 2.890000e+02 4.400000e+02 1.038980e+00 ## range 1.210000e+01 1.500000e+00 1.660000e+00 6.520000e+01 6.020000e-01 2.880000e+02 4.340000e+02 5.187000e-02 ## sum 4.687785e+04 2.206810e+03 2.070160e+03 3.536470e+04 3.640520e+02 1.983230e+05 7.519925e+05 6.462544e+03 ## median 7.000000e+00 2.900000e-01 3.100000e-01 3.000000e+00 4.700000e-02 2.900000e+01 1.180000e+02 9.948900e-01 ## mean 7.215307e+00 3.396660e-01 3.186332e-01 5.443235e+00 5.603386e-02 3.052532e+01 1.157446e+02 9.946966e-01 ## SE.mean 1.608399e-02 2.042536e-03 1.802862e-03 5.902692e-02 4.346387e-04 2.202050e-01 7.012292e-01 3.720255e-05 ## CI.mean.0.95 3.152992e-02 4.004042e-03 3.534204e-03 1.157122e-01 8.520349e-04 4.316744e-01 1.374640e+00 7.292924e-05 ## var 1.680740e+00 2.710517e-02 2.111728e-02 2.263670e+01 1.227353e-03 3.150412e+02 3.194720e+03 8.992040e-06 ## std.dev 1.296434e+00 1.646365e-01 1.453179e-01 4.757804e+00 3.503360e-02 1.774940e+01 5.652185e+01 2.998673e-03 ## coef.var 1.796783e-01 4.847011e-01 4.560663e-01 8.740764e-01 6.252220e-01 5.814648e-01 4.883326e-01 3.014661e-03 ## pH sulphates alcohol quality type ## nbr.val 6.497000e+03 6.497000e+03 6.497000e+03 6.497000e+03 NA ## nbr.null 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 NA ## nbr.na 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 NA ## min 2.720000e+00 2.200000e-01 8.000000e+00 3.000000e+00 NA ## max 4.010000e+00 2.000000e+00 1.490000e+01 9.000000e+00 NA ## range 1.290000e+00 1.780000e+00 6.900000e+00 6.000000e+00 NA ## sum 2.091060e+04 3.451650e+03 6.816523e+04 3.780200e+04 NA ## median 3.210000e+00 5.100000e-01 1.030000e+01 6.000000e+00 NA ## mean 3.218501e+00 5.312683e-01 1.049180e+01 5.818378e+00 NA ## SE.mean 1.994780e-03 1.846136e-03 1.479718e-02 1.083390e-02 NA ## CI.mean.0.95 3.910426e-03 3.619034e-03 2.900735e-02 2.123801e-02 NA ## var 2.585252e-02 2.214319e-02 1.422561e+00 7.625748e-01 NA ## std.dev 1.607872e-01 1.488059e-01 1.192712e+00 8.732553e-01 NA ## coef.var 4.995717e-02 2.800955e-01 1.136804e-01 1.500857e-01 NA Univariate Exploration We can use univariate exploration to explore the distribution of a single variable or desriptive statistic. The variables for the three plots below were selected judgmentally from the dataset. mu &lt;- ddply(wine, &quot;type&quot;, summarise, grp.mean=mean(alcohol)) p&lt;-ggplot(wine, aes(x=alcohol, fill=type, color=type)) + geom_histogram(position=&quot;dodge&quot;)+ theme(legend.position=&quot;top&quot;) + geom_vline(data=mu, aes(xintercept=grp.mean, color=type), linetype=&quot;dashed&quot;)+ theme(legend.position=&quot;top&quot;) p ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. mu &lt;- ddply(wine, &quot;type&quot;, summarise, grp.mean=mean(volatile.acidity)) p&lt;-ggplot(wine, aes(x=volatile.acidity, fill=type, color=type)) + geom_histogram(position=&quot;dodge&quot;)+ theme(legend.position=&quot;top&quot;) + geom_vline(data=mu, aes(xintercept=grp.mean, color=type), linetype=&quot;dashed&quot;)+ theme(legend.position=&quot;top&quot;) p ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. mu &lt;- ddply(wine, &quot;type&quot;, summarise, grp.mean=mean(density)) p&lt;-ggplot(wine, aes(x=density, fill=type, color=type)) + geom_histogram(position=&quot;dodge&quot;)+ theme(legend.position=&quot;top&quot;) + geom_vline(data=mu, aes(xintercept=grp.mean, color=type), linetype=&quot;dashed&quot;)+ theme(legend.position=&quot;top&quot;) p ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Bivariate Exploration Bivariate exploration allows us to explore the relationship between two descriptive statistics in a dataset. To better determine which variable relationships we should explore, we ran correlation analysis. # ++++++++++++++++++++++++++++ # flattenCorrMatrix # ++++++++++++++++++++++++++++ # cormat : matrix of the correlation coefficients # pmat : matrix of the correlation p-values flattenCorrMatrix &lt;- function(cormat, pmat) { ut &lt;- upper.tri(cormat) data.frame( row = rownames(cormat)[row(cormat)[ut]], column = rownames(cormat)[col(cormat)[ut]], cor =(cormat)[ut], p = pmat[ut] ) } # exclude the type of wine from the correlation variables res2 &lt;- rcorr(as.matrix(wine[,1:12])) cor.m &lt;- flattenCorrMatrix(res2$r, res2$P) cor.m[order(cor.m[,3],decreasing=TRUE),] ## row column cor p ## 21 free.sulfur.dioxide total.sulfur.dioxide 0.720934081 0.000000e+00 ## 25 residual.sugar density 0.552516950 0.000000e+00 ## 19 residual.sugar total.sulfur.dioxide 0.495481587 0.000000e+00 ## 22 fixed.acidity density 0.458909982 0.000000e+00 ## 66 alcohol quality 0.444318520 0.000000e+00 ## 14 residual.sugar free.sulfur.dioxide 0.402870640 0.000000e+00 ## 41 chlorides sulphates 0.395593307 0.000000e+00 ## 8 volatile.acidity chlorides 0.377124276 0.000000e+00 ## 26 chlorides density 0.362614656 0.000000e+00 ## 2 fixed.acidity citric.acid 0.324435725 0.000000e+00 ## 37 fixed.acidity sulphates 0.299567744 0.000000e+00 ## 7 fixed.acidity chlorides 0.298194772 0.000000e+00 ## 23 volatile.acidity density 0.271295648 0.000000e+00 ## 30 volatile.acidity pH 0.261454403 0.000000e+00 ## 44 density sulphates 0.259478495 0.000000e+00 ## 38 volatile.acidity sulphates 0.225983680 0.000000e+00 ## 1 fixed.acidity volatile.acidity 0.219008256 0.000000e+00 ## 18 citric.acid total.sulfur.dioxide 0.195241976 0.000000e+00 ## 45 pH sulphates 0.192123407 0.000000e+00 ## 6 citric.acid residual.sugar 0.142451226 0.000000e+00 ## 13 citric.acid free.sulfur.dioxide 0.133125810 0.000000e+00 ## 54 pH alcohol 0.121248467 0.000000e+00 ## 24 citric.acid density 0.096153929 7.993606e-15 ## 58 citric.acid quality 0.085531717 5.001777e-12 ## 39 citric.acid sulphates 0.056197300 5.830741e-06 ## 61 free.sulfur.dioxide quality 0.055463059 7.708445e-06 ## 33 chlorides pH 0.044707980 3.124540e-04 ## 9 citric.acid chlorides 0.038998014 1.666635e-03 ## 65 sulphates quality 0.038485446 1.918079e-03 ## 28 total.sulfur.dioxide density 0.032394512 9.019631e-03 ## 27 free.sulfur.dioxide density 0.025716842 3.818871e-02 ## 64 pH quality 0.019505704 1.159310e-01 ## 36 density pH 0.011686081 3.462974e-01 ## 55 sulphates alcohol -0.003029195 8.071389e-01 ## 48 citric.acid alcohol -0.010493492 3.977326e-01 ## 59 residual.sugar quality -0.036980485 2.871025e-03 ## 47 volatile.acidity alcohol -0.037640386 2.409699e-03 ## 62 total.sulfur.dioxide quality -0.041385454 8.480397e-04 ## 56 fixed.acidity quality -0.076743208 5.874849e-10 ## 46 fixed.acidity alcohol -0.095451523 1.243450e-14 ## 4 fixed.acidity residual.sugar -0.111981281 0.000000e+00 ## 10 residual.sugar chlorides -0.128940500 0.000000e+00 ## 34 free.sulfur.dioxide pH -0.145853896 0.000000e+00 ## 51 free.sulfur.dioxide alcohol -0.179838435 0.000000e+00 ## 40 residual.sugar sulphates -0.185927405 0.000000e+00 ## 42 free.sulfur.dioxide sulphates -0.188457249 0.000000e+00 ## 15 chlorides free.sulfur.dioxide -0.195044785 0.000000e+00 ## 5 volatile.acidity residual.sugar -0.196011174 0.000000e+00 ## 60 chlorides quality -0.200665500 0.000000e+00 ## 35 total.sulfur.dioxide pH -0.238413103 0.000000e+00 ## 29 fixed.acidity pH -0.252700468 0.000000e+00 ## 50 chlorides alcohol -0.256915580 0.000000e+00 ## 57 volatile.acidity quality -0.265699478 0.000000e+00 ## 52 total.sulfur.dioxide alcohol -0.265739639 0.000000e+00 ## 32 residual.sugar pH -0.267319837 0.000000e+00 ## 43 total.sulfur.dioxide sulphates -0.275726820 0.000000e+00 ## 20 chlorides total.sulfur.dioxide -0.279630447 0.000000e+00 ## 11 fixed.acidity free.sulfur.dioxide -0.282735428 0.000000e+00 ## 63 density quality -0.305857906 0.000000e+00 ## 16 fixed.acidity total.sulfur.dioxide -0.329053901 0.000000e+00 ## 31 citric.acid pH -0.329808191 0.000000e+00 ## 12 volatile.acidity free.sulfur.dioxide -0.352557306 0.000000e+00 ## 49 residual.sugar alcohol -0.359414771 0.000000e+00 ## 3 volatile.acidity citric.acid -0.377981317 0.000000e+00 ## 17 volatile.acidity total.sulfur.dioxide -0.414476195 0.000000e+00 ## 53 density alcohol -0.686745421 0.000000e+00 Visualize Correlations # Insignificant correlations are leaved blank corrplot(res2$r, type=&quot;upper&quot;, order=&quot;hclust&quot;, p.mat = res2$P, sig.level = 0.01, insig = &quot;blank&quot;) Based on the correlation matrix and the correlation plots it appears that there is a strong negative correlation between alcohol and density and a strong positive correlation between residual sugar and density. These are plotted below. ggplot(wine, aes(x = alcohol, y = density, color = type)) + geom_point(size = 3, alpha = .6) + labs(title = &quot;Wine Composition by alcohol, density, and type&quot;) ggplot(wine, aes(x = residual.sugar, y = density, color = type)) + geom_point(size = 3, alpha = .6) + labs(title = &quot;Wine Composition by residual.sugar, density, and type&quot;) "],["data-cleaning-and-partitioning.html", "Chapter 3 Data Cleaning and Partitioning 3.1 Data Normalization 3.2 Data Partitioning", " Chapter 3 Data Cleaning and Partitioning 3.1 Data Normalization Our dataset already contains only predictive values and output, so we do not need to remove any descriptive columns. We must normalize the values within the dataset to avoid any bias and remove the output variable (quality) since its the prediction. set.seed(123) # do not include wine type in partitioned data. wine &lt;-wine[,1:12] #Normalization normalize &lt;- function(x) { return ((x - min(x)) / (max(x) - min(x))) } #only normalize variables wine.normal &lt;- as.data.frame(lapply(wine[,1:11], normalize)) # add quality back wine.normal$quality = wine$quality head(wine.normal) ## fixed.acidity volatile.acidity citric.acid residual.sugar chlorides free.sulfur.dioxide total.sulfur.dioxide density pH sulphates ## 1 0.2975207 0.4133333 0.00000000 0.01993865 0.1112957 0.03472222 0.06451613 0.2060922 0.6124031 0.1910112 ## 2 0.3305785 0.5333333 0.00000000 0.03067485 0.1478405 0.08333333 0.14055300 0.1868132 0.3720930 0.2584270 ## 3 0.3305785 0.4533333 0.02409639 0.02607362 0.1378738 0.04861111 0.11059908 0.1906690 0.4186047 0.2415730 ## 4 0.6115702 0.1333333 0.33734940 0.01993865 0.1096346 0.05555556 0.12442396 0.2099479 0.3410853 0.2022472 ## 5 0.2975207 0.4133333 0.00000000 0.01993865 0.1112957 0.03472222 0.06451613 0.2060922 0.6124031 0.1910112 ## 6 0.2975207 0.3866667 0.00000000 0.01840491 0.1096346 0.04166667 0.07834101 0.2060922 0.6124031 0.1910112 ## alcohol quality ## 1 0.2028986 5 ## 2 0.2608696 5 ## 3 0.2608696 5 ## 4 0.2608696 6 ## 5 0.2028986 5 ## 6 0.2028986 5 3.2 Data Partitioning In order to determine whether our models are effective we must partition our dataset into a training and test set. ind &lt;- sample(2, nrow(wine.normal), replace=TRUE, prob=c(0.7, 0.3)) train &lt;- wine.normal[ind==1, ] test &lt;- wine.normal[ind==2,] "],["random-forest.html", "Chapter 4 Random Forest 4.1 Run random forest function 4.2 Predicition and COnfusion matrix - train data 4.3 Predicition and COnfusion matrix - test data 4.4 Error Rate in Random Forest Model 4.5 Tune mtry 4.6 Random Forest 4.7 Rerun Predicition and Confusion matrix - train data 4.8 Rerun Predicition and COnfusion matrix - test data 4.9 Number of Nodes on the trees 4.10 Partial Dependence Plots", " Chapter 4 Random Forest 4.1 Run random forest function set.seed(444) train$quality&lt;-as.factor(train$quality) test$quality&lt;-as.factor(test$quality) # quality is a function of all other variables rf &lt;- randomForest(quality~., data=train) print(rf) ## ## Call: ## randomForest(formula = quality ~ ., data = train) ## Type of random forest: classification ## Number of trees: 500 ## No. of variables tried at each split: 3 ## ## OOB estimate of error rate: 32.91% ## Confusion matrix: ## 3 4 5 6 7 8 9 class.error ## 3 0 0 11 8 0 0 0 1.0000000 ## 4 0 24 82 43 2 0 0 0.8410596 ## 5 0 4 1086 421 7 0 0 0.2845850 ## 6 0 1 335 1526 112 1 0 0.2273418 ## 7 0 0 21 354 379 6 0 0.5013158 ## 8 0 0 3 52 34 47 0 0.6544118 ## 9 0 0 0 1 4 0 0 1.0000000 The reults show that the out of bag error rate is 33.27%. The model was most inaccurate when predicting wines with a quality values of 3, 4, and 8 with a 100% error rate. It was most accurate when predicting wines with a quality of 5, 19% error rate. 4.2 Predicition and COnfusion matrix - train data p1 &lt;- predict(rf, train) p1&lt;- droplevels(p1) #drop any unused levels head(p1) # predicted values ## 1 3 6 7 9 10 ## 5 5 5 5 7 5 ## Levels: 3 4 5 6 7 8 9 head(train$quality) # actual values ## [1] 5 5 5 5 7 5 ## Levels: 3 4 5 6 7 8 9 Coincidentally, all of the first 6 predictions were 100% accurate. #install.packages(&#39;caret&#39;, dependencies = TRUE) confusionMatrix(p1, train$quality ) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 3 4 5 6 7 8 9 ## 3 19 0 0 0 0 0 0 ## 4 0 151 0 0 0 0 0 ## 5 0 0 1518 0 0 0 0 ## 6 0 0 0 1975 0 0 0 ## 7 0 0 0 0 760 0 0 ## 8 0 0 0 0 0 136 0 ## 9 0 0 0 0 0 0 5 ## ## Overall Statistics ## ## Accuracy : 1 ## 95% CI : (0.9992, 1) ## No Information Rate : 0.4327 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 1 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: 3 Class: 4 Class: 5 Class: 6 Class: 7 Class: 8 Class: 9 ## Sensitivity 1.000000 1.00000 1.0000 1.0000 1.0000 1.0000 1.000000 ## Specificity 1.000000 1.00000 1.0000 1.0000 1.0000 1.0000 1.000000 ## Pos Pred Value 1.000000 1.00000 1.0000 1.0000 1.0000 1.0000 1.000000 ## Neg Pred Value 1.000000 1.00000 1.0000 1.0000 1.0000 1.0000 1.000000 ## Prevalence 0.004163 0.03309 0.3326 0.4327 0.1665 0.0298 0.001096 ## Detection Rate 0.004163 0.03309 0.3326 0.4327 0.1665 0.0298 0.001096 ## Detection Prevalence 0.004163 0.03309 0.3326 0.4327 0.1665 0.0298 0.001096 ## Balanced Accuracy 1.000000 1.00000 1.0000 1.0000 1.0000 1.0000 1.000000 There were no misclassifications of our training data, our model was 100% accurate. THe reason for the large discrepancy between accuracy markers for the OOB and the confusion matrix is that the confusion matrix for p1 is base on the training data random forest modelso it has already seen the training data data points. 4.3 Predicition and COnfusion matrix - test data p2 &lt;- predict(rf, test) p2&lt;-droplevels(p2) # drop unused levels head(p2) # predicted values ## 2 4 5 8 11 16 ## 5 5 5 5 5 5 ## Levels: 3 4 5 6 7 8 head(test$quality) # actual values ## [1] 5 6 5 7 5 5 ## Levels: 3 4 5 6 7 8 This model was able to predict 4 out of 6 of the first values accurately. confusionMatrix(p2, test$quality ) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 3 4 5 6 7 8 ## 3 0 1 0 0 0 0 ## 4 0 8 1 1 0 0 ## 5 3 31 448 136 9 0 ## 6 7 24 169 680 128 24 ## 7 1 1 2 44 178 15 ## 8 0 0 0 0 4 18 ## ## Overall Statistics ## ## Accuracy : 0.6891 ## 95% CI : (0.6679, 0.7097) ## No Information Rate : 0.4454 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.512 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: 3 Class: 4 Class: 5 Class: 6 Class: 7 Class: 8 ## Sensitivity 0.0000000 0.123077 0.7226 0.7898 0.55799 0.315789 ## Specificity 0.9994797 0.998929 0.8637 0.6716 0.96097 0.997868 ## Pos Pred Value 0.0000000 0.800000 0.7145 0.6589 0.73859 0.818182 ## Neg Pred Value 0.9943064 0.970359 0.8683 0.7991 0.91667 0.979592 ## Prevalence 0.0056906 0.033626 0.3207 0.4454 0.16503 0.029488 ## Detection Rate 0.0000000 0.004139 0.2318 0.3518 0.09208 0.009312 ## Detection Prevalence 0.0005173 0.005173 0.3244 0.5339 0.12468 0.011381 ## Balanced Accuracy 0.4997399 0.561003 0.7931 0.7307 0.75948 0.656829 4.4 Error Rate in Random Forest Model plot(rf) THe model has a drop off after about 300 trees and then is more or less constant, therefore, we can adjust tune our model. 4.5 Tune mtry set.seed(2222) t &lt;- tuneRF(train[,-12], train[,12], stepFactor = 0.5, plot=TRUE, ntreeTry = 100, trace=TRUE, improve=0.05) ## mtry = 3 OOB error = 33.26% ## Searching left ... ## mtry = 6 OOB error = 34.36% ## -0.03293808 0.05 ## Searching right ... ## mtry = 1 OOB error = 33.46% ## -0.005928854 0.05 So this means that the model hits its lowest error rate when mtry=3, so we can then go back and adjust our model to reflect this new mtry value. 4.6 Random Forest Rerun random forest with new tuning factors set.seed(444) # quality is a function of all other variables rf &lt;- randomForest(quality~., data=train, ntree=300, mTry=3, importance=TRUE, proximity=TRUE) print(rf) ## ## Call: ## randomForest(formula = quality ~ ., data = train, ntree = 300, mTry = 3, importance = TRUE, proximity = TRUE) ## Type of random forest: classification ## Number of trees: 300 ## No. of variables tried at each split: 3 ## ## OOB estimate of error rate: 32.23% ## Confusion matrix: ## 3 4 5 6 7 8 9 class.error ## 3 0 1 10 8 0 0 0 1.0000000 ## 4 0 21 85 45 0 0 0 0.8609272 ## 5 0 5 1100 407 6 0 0 0.2753623 ## 6 0 1 321 1537 115 1 0 0.2217722 ## 7 0 0 22 343 388 7 0 0.4894737 ## 8 0 0 4 55 30 47 0 0.6544118 ## 9 0 0 0 1 4 0 0 1.0000000 Our original OOB estimate of error rate was 33.27% and now it is 32.65%, so it was improved by about 0.5%. 4.7 Rerun Predicition and Confusion matrix - train data p1 &lt;- predict(rf, train) p1&lt;- droplevels(p1) # drop any unused levels head(p1) # predicted values ## 1 3 6 7 9 10 ## 5 5 5 5 7 5 ## Levels: 3 4 5 6 7 8 9 head(train$quality) # actual values ## [1] 5 5 5 5 7 5 ## Levels: 3 4 5 6 7 8 9 Coincidentally, all of the first 6 predictions were 100% accurate. confusionMatrix(p1, train$quality ) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 3 4 5 6 7 8 9 ## 3 19 0 0 0 0 0 0 ## 4 0 151 0 0 0 0 0 ## 5 0 0 1518 0 0 0 0 ## 6 0 0 0 1975 0 0 0 ## 7 0 0 0 0 760 0 0 ## 8 0 0 0 0 0 136 0 ## 9 0 0 0 0 0 0 5 ## ## Overall Statistics ## ## Accuracy : 1 ## 95% CI : (0.9992, 1) ## No Information Rate : 0.4327 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 1 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: 3 Class: 4 Class: 5 Class: 6 Class: 7 Class: 8 Class: 9 ## Sensitivity 1.000000 1.00000 1.0000 1.0000 1.0000 1.0000 1.000000 ## Specificity 1.000000 1.00000 1.0000 1.0000 1.0000 1.0000 1.000000 ## Pos Pred Value 1.000000 1.00000 1.0000 1.0000 1.0000 1.0000 1.000000 ## Neg Pred Value 1.000000 1.00000 1.0000 1.0000 1.0000 1.0000 1.000000 ## Prevalence 0.004163 0.03309 0.3326 0.4327 0.1665 0.0298 0.001096 ## Detection Rate 0.004163 0.03309 0.3326 0.4327 0.1665 0.0298 0.001096 ## Detection Prevalence 0.004163 0.03309 0.3326 0.4327 0.1665 0.0298 0.001096 ## Balanced Accuracy 1.000000 1.00000 1.0000 1.0000 1.0000 1.0000 1.000000 Again the accuracy is 100% but this is due to the training data already being seen by the model. 4.8 Rerun Predicition and COnfusion matrix - test data p2 &lt;- predict(rf, test) p2 &lt;- droplevels(p2) # drop any unuused levels head(p2) # predicted values ## 2 4 5 8 11 16 ## 5 5 5 5 5 5 ## Levels: 3 4 5 6 7 8 head(test$quality) # actual values ## [1] 5 6 5 7 5 5 ## Levels: 3 4 5 6 7 8 This model was able to predict 4 out of 6 of the first values accurately. confusionMatrix(p2, test$quality ) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 3 4 5 6 7 8 ## 3 0 1 0 0 0 0 ## 4 0 8 1 2 0 0 ## 5 3 32 447 139 10 0 ## 6 7 23 170 675 130 25 ## 7 1 1 2 45 175 14 ## 8 0 0 0 0 4 18 ## ## Overall Statistics ## ## Accuracy : 0.6844 ## 95% CI : (0.6632, 0.7051) ## No Information Rate : 0.4454 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.5047 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: 3 Class: 4 Class: 5 Class: 6 Class: 7 Class: 8 ## Sensitivity 0.0000000 0.123077 0.7210 0.7840 0.54859 0.315789 ## Specificity 0.9994797 0.998394 0.8599 0.6688 0.96097 0.997868 ## Pos Pred Value 0.0000000 0.727273 0.7084 0.6553 0.73529 0.818182 ## Neg Pred Value 0.9943064 0.970343 0.8671 0.7940 0.91504 0.979592 ## Prevalence 0.0056906 0.033626 0.3207 0.4454 0.16503 0.029488 ## Detection Rate 0.0000000 0.004139 0.2312 0.3492 0.09053 0.009312 ## Detection Prevalence 0.0005173 0.005691 0.3264 0.5329 0.12312 0.011381 ## Balanced Accuracy 0.4997399 0.560735 0.7904 0.7264 0.75478 0.656829 Overall, there was only a 0.5% increase in accuracy for the test data. However, there were improvements in sensitify for Class 5 and Class 7. 4.9 Number of Nodes on the trees hist(treesize(rf), main = &quot;No. of nodes for the trees&quot;, col=&quot;blue&quot;) ## Variable Importance varImpPlot(rf, main=&quot;Variable Importance&quot;) This tells us that alcohol has the greatest importance in our model. Removing this variable would result in a 30% mean decrease in accuracy. On the opposite end of the spectrum the pH has almost no affect in the models accuracy. 4.10 Partial Dependence Plots Produces partial plot for alcohol in training set data/rf model for classication 5 aka quality =5. partialPlot(rf, train, alcohol, 5) This plot tells u that when alcohol is less than 11% it predicts classification 5 more strongly than when it is greater than 11%. partialPlot(rf, train, alcohol, 7) This plot tells us that when alcohol is greater than 10% it predicts classification 7 more strongly than when it is less than 10%. "],["k-nearest-neighbor.html", "Chapter 5 K-Nearest Neighbor 5.1 Introduction 5.2 Load Data 5.3 Clean and Normalize the data. 5.4 Data Splice 5.5 Model Evaluation 5.6 Optimization", " Chapter 5 K-Nearest Neighbor 5.1 Introduction Just planning to use this file to walk through some tutorials of k-means clustering I found. 5.2 Load Data We have taken a data-set from Kaggle: https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009 This data-set inlcudes the measurements from the wine along with its quality rating. wine&lt;-read.table(&quot;data/winequality-red.csv&quot;,stringsAsFactors = FALSE, sep=&quot;,&quot;,header = TRUE) head(wine) ## fixed.acidity volatile.acidity citric.acid residual.sugar chlorides free.sulfur.dioxide total.sulfur.dioxide density pH sulphates alcohol ## 1 7.4 0.70 0.00 1.9 0.076 11 34 0.9978 3.51 0.56 9.4 ## 2 7.8 0.88 0.00 2.6 0.098 25 67 0.9968 3.20 0.68 9.8 ## 3 7.8 0.76 0.04 2.3 0.092 15 54 0.9970 3.26 0.65 9.8 ## 4 11.2 0.28 0.56 1.9 0.075 17 60 0.9980 3.16 0.58 9.8 ## 5 7.4 0.70 0.00 1.9 0.076 11 34 0.9978 3.51 0.56 9.4 ## 6 7.4 0.66 0.00 1.8 0.075 13 40 0.9978 3.51 0.56 9.4 ## quality ## 1 5 ## 2 5 ## 3 5 ## 4 6 ## 5 5 ## 6 5 5.3 Clean and Normalize the data. Our dataset already contains only predictive values and output, so we do not need to remove any descriptive columns. We must normalize the values within the dataset to avoid any bias and remove the output variable (quality) since its the prediction. #Normalization normalize &lt;- function(x) { return ((x - min(x)) / (max(x) - min(x))) } wine.normal &lt;- as.data.frame(lapply(wine[,1:11], normalize)) head(wine.normal) ## fixed.acidity volatile.acidity citric.acid residual.sugar chlorides free.sulfur.dioxide total.sulfur.dioxide density pH sulphates ## 1 0.2477876 0.3972603 0.00 0.06849315 0.1068447 0.1408451 0.09893993 0.5675477 0.6062992 0.1377246 ## 2 0.2831858 0.5205479 0.00 0.11643836 0.1435726 0.3380282 0.21554770 0.4941263 0.3622047 0.2095808 ## 3 0.2831858 0.4383562 0.04 0.09589041 0.1335559 0.1971831 0.16961131 0.5088106 0.4094488 0.1916168 ## 4 0.5840708 0.1095890 0.56 0.06849315 0.1051753 0.2253521 0.19081272 0.5822320 0.3307087 0.1497006 ## 5 0.2477876 0.3972603 0.00 0.06849315 0.1068447 0.1408451 0.09893993 0.5675477 0.6062992 0.1377246 ## 6 0.2477876 0.3698630 0.00 0.06164384 0.1051753 0.1690141 0.12014134 0.5675477 0.6062992 0.1377246 ## alcohol ## 1 0.1538462 ## 2 0.2153846 ## 3 0.2153846 ## 4 0.2153846 ## 5 0.1538462 ## 6 0.1538462 5.4 Data Splice Since our data-set our research question involves prediction will will randomly select a portion of data to use for overall effectiveness measurement. We plan to save about \\(5\\%\\) of the data for testing which ends up being \\(80\\) values. set.seed(123) dat.d &lt;- sample(1:nrow(wine.normal),size=nrow(wine.normal)*0.8,replace = FALSE) #random selection of 90% data. train.wine &lt;- wine.normal[dat.d,] # 90% training data test.wine &lt;- wine.normal[-dat.d,] # remaining 10% test data #Creating seperate dataframe for &#39;Quality&#39; feature which is our target. train.quality_label &lt;- wine[dat.d,12] test.quality_label &lt;-wine[-dat.d,12] Next, were going to calculate the number of observations in the training data set. The reason were doing this is that we want to initialize the value of K in the KNN model. One of the ways to find the optimal K value is to calculate the square root of the total number of observations in the data set. This square root will give you the K value. NROW(train.quality_label) ## [1] 1279 sqrt(NROW(train.quality_label) ) ## [1] 35.76311 The square root of 1493 is around 35.7 well create a model with a K value as 36. knn.36 &lt;- knn(train=train.wine, test=test.wine, cl=train.quality_label, k=36) 5.5 Model Evaluation #Calculate the proportion of correct classification for k =37 ACC.36 &lt;- 100 * sum(test.quality_label == knn.36)/NROW(test.quality_label) ACC.36 ## [1] 58.4375 As shown above, the accuracy for K = 36 is 58.435 5.6 Optimization i=1 k.optm=1 for (i in 1:37){ knn.mod &lt;- knn(train=train.wine, test=test.wine, cl=train.quality_label, k=i) k.optm[i] &lt;- 100 * sum(test.quality_label == knn.mod)/NROW(test.quality_label) k=i cat(k,&#39;=&#39;,k.optm[i],&#39;&#39;) } ## 1 = 61.875 2 = 56.875 3 = 56.875 4 = 61.25 5 = 59.0625 6 = 58.125 7 = 55.625 8 = 57.1875 9 = 57.5 10 = 56.875 11 = 59.0625 12 = 59.375 13 = 58.75 14 = 59.375 15 = 60 16 = 60.3125 17 = 61.25 18 = 60.9375 19 = 60 20 = 59.0625 21 = 59.375 22 = 59.375 23 = 59.6875 24 = 59.0625 25 = 58.125 26 = 58.125 27 = 59.0625 28 = 59.0625 29 = 58.4375 30 = 58.4375 31 = 58.75 32 = 59.375 33 = 60 34 = 60.3125 35 = 59.6875 36 = 59.0625 37 = 59.375 #Accuracy plot plot(k.optm, type=&quot;b&quot;, xlab=&quot;K- Value&quot;,ylab=&quot;Accuracy level&quot;) "],["lasso-regression.html", "Chapter 6 Lasso Regression 6.1 Load Data 6.2 Process Reference 6.3 Data Partitioning 6.4 Scaling the Numeric Features 6.5 Linear Regression 6.6 Regularization 6.7 Ridge Regression 6.8 Lasso Regression", " Chapter 6 Lasso Regression 6.1 Load Data We have taken a data-set from Kaggle: https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009 This data-set inlcudes the measurements from the wine along with its quality rating. wine&lt;-read.table(&quot;data/winequality-red.csv&quot;,stringsAsFactors = FALSE, sep=&quot;,&quot;,header = TRUE) glimpse(wine) ## Rows: 1,599 ## Columns: 12 ## $ fixed.acidity &lt;dbl&gt; 7.4, 7.8, 7.8, 11.2, 7.4, 7.4, 7.9, 7.3, 7.8, 7.5, 6.7, 7.5, 5.6, 7.8, 8.9, 8.9, 8.5, 8.1, 7.4, 7.9, 8.9, 7.6, 7.9, ~ ## $ volatile.acidity &lt;dbl&gt; 0.700, 0.880, 0.760, 0.280, 0.700, 0.660, 0.600, 0.650, 0.580, 0.500, 0.580, 0.500, 0.615, 0.610, 0.620, 0.620, 0.28~ ## $ citric.acid &lt;dbl&gt; 0.00, 0.00, 0.04, 0.56, 0.00, 0.00, 0.06, 0.00, 0.02, 0.36, 0.08, 0.36, 0.00, 0.29, 0.18, 0.19, 0.56, 0.28, 0.08, 0.~ ## $ residual.sugar &lt;dbl&gt; 1.9, 2.6, 2.3, 1.9, 1.9, 1.8, 1.6, 1.2, 2.0, 6.1, 1.8, 6.1, 1.6, 1.6, 3.8, 3.9, 1.8, 1.7, 4.4, 1.8, 1.8, 2.3, 1.6, 2~ ## $ chlorides &lt;dbl&gt; 0.076, 0.098, 0.092, 0.075, 0.076, 0.075, 0.069, 0.065, 0.073, 0.071, 0.097, 0.071, 0.089, 0.114, 0.176, 0.170, 0.09~ ## $ free.sulfur.dioxide &lt;dbl&gt; 11, 25, 15, 17, 11, 13, 15, 15, 9, 17, 15, 17, 16, 9, 52, 51, 35, 16, 6, 17, 29, 23, 10, 9, 21, 11, 4, 10, 14, 8, 17~ ## $ total.sulfur.dioxide &lt;dbl&gt; 34, 67, 54, 60, 34, 40, 59, 21, 18, 102, 65, 102, 59, 29, 145, 148, 103, 56, 29, 56, 60, 71, 37, 67, 40, 23, 11, 37,~ ## $ density &lt;dbl&gt; 0.9978, 0.9968, 0.9970, 0.9980, 0.9978, 0.9978, 0.9964, 0.9946, 0.9968, 0.9978, 0.9959, 0.9978, 0.9943, 0.9974, 0.99~ ## $ pH &lt;dbl&gt; 3.51, 3.20, 3.26, 3.16, 3.51, 3.51, 3.30, 3.39, 3.36, 3.35, 3.28, 3.35, 3.58, 3.26, 3.16, 3.17, 3.30, 3.11, 3.38, 3.~ ## $ sulphates &lt;dbl&gt; 0.56, 0.68, 0.65, 0.58, 0.56, 0.56, 0.46, 0.47, 0.57, 0.80, 0.54, 0.80, 0.52, 1.56, 0.88, 0.93, 0.75, 1.28, 0.50, 1.~ ## $ alcohol &lt;dbl&gt; 9.4, 9.8, 9.8, 9.8, 9.4, 9.4, 9.4, 10.0, 9.5, 10.5, 9.2, 10.5, 9.9, 9.1, 9.2, 9.2, 10.5, 9.3, 9.0, 9.2, 9.4, 9.7, 9.~ ## $ quality &lt;int&gt; 5, 5, 5, 6, 5, 5, 5, 7, 7, 5, 5, 5, 5, 5, 5, 5, 7, 5, 4, 6, 6, 5, 5, 5, 6, 5, 5, 5, 5, 6, 5, 6, 5, 6, 5, 6, 6, 7, 4,~ 6.2 Process Reference The process was followed from this site: https://www.pluralsight.com/guides/linear-lasso-and-ridge-regression-with-r 6.3 Data Partitioning The below code takes 70% of the data for training and 30% of the code for testing. set.seed(100) index = sample(1:nrow(wine), 0.7*nrow(wine)) train = wine[index,] # Create the training data test = wine[-index,] # Create the test data dim(train) ## [1] 1119 12 dim(test) ## [1] 480 12 6.4 Scaling the Numeric Features cols &lt;- colnames(wine) pre_proc_val &lt;- preProcess(train[,cols], method = c(&quot;center&quot;, &quot;scale&quot;)) train[,cols] = predict(pre_proc_val, train[,cols]) test[,cols] = predict(pre_proc_val, test[,cols]) summary(train) ## fixed.acidity volatile.acidity citric.acid residual.sugar chlorides free.sulfur.dioxide total.sulfur.dioxide ## Min. :-2.0802 Min. :-2.26583 Min. :-1.37809 Min. :-1.18596 Min. :-1.58727 Min. :-1.4109 Min. :-1.2446 ## 1st Qu.:-0.6928 1st Qu.:-0.76458 1st Qu.:-0.92192 1st Qu.:-0.45565 1st Qu.:-0.36546 1st Qu.:-0.8446 1st Qu.:-0.7471 ## Median :-0.2304 Median :-0.04176 Median :-0.06029 Median :-0.23655 Median :-0.15480 Median :-0.1840 Median :-0.2496 ## Mean : 0.0000 Mean : 0.00000 Mean : 0.00000 Mean : 0.00000 Mean : 0.00000 Mean : 0.0000 Mean : 0.0000 ## 3rd Qu.: 0.5790 3rd Qu.: 0.62546 3rd Qu.: 0.80135 3rd Qu.: 0.05557 3rd Qu.: 0.05586 3rd Qu.: 0.5710 3rd Qu.: 0.4656 ## Max. : 4.1630 Max. : 5.85201 Max. : 3.69037 Max. : 9.47663 Max. :11.03109 Max. : 5.2899 Max. : 7.5552 ## density pH sulphates alcohol quality ## Min. :-3.57012 Min. :-3.68218 Min. :-1.8820 Min. :-1.9054 Min. :-3.3046 ## 1st Qu.:-0.61381 1st Qu.:-0.66652 1st Qu.:-0.6199 1st Qu.:-0.8694 1st Qu.:-0.7831 ## Median :-0.01507 Median :-0.02489 Median :-0.2183 Median :-0.2101 Median : 0.4777 ## Mean : 0.00000 Mean : 0.00000 Mean : 0.0000 Mean : 0.0000 Mean : 0.0000 ## 3rd Qu.: 0.57566 3rd Qu.: 0.55258 3rd Qu.: 0.4127 3rd Qu.: 0.5747 3rd Qu.: 0.4777 ## Max. : 3.71106 Max. : 4.46653 Max. : 7.6982 Max. : 3.3687 Max. : 2.9993 6.5 Linear Regression 6.5.1 What is it? The simplest form of regression is linear regression, which assumes that the predictors have a linear relationship with the target variable. ### Assumptions * Input is assumed to have a Normal distribution and are not correlated with each other. We saw in the Descriptive Stats section that this was not the case With these assumptions being true we can model quality witht the following equation. \\[ q = a_1x_1 + a_2x_2 + a_3x_3 + \\dots + a_2nx_n + b\\] Where \\(a_1, a_2, \\dots, a_n\\) are coefficients from the model. \\(x_1, x_2, \\dots, x_n\\) are the input variables to the model. \\(b\\) is a factor of the model and \\(q\\) is equal to the quality output. 6.5.2 Single Variable In the code-block below we output the summary of just the measurement alchol into the linear model. summary(lm(quality~alcohol, data = train)) ## ## Call: ## lm(formula = quality ~ alcohol, data = train) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.5773 -0.4885 -0.2049 0.6305 3.2938 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.845e-16 2.587e-02 0.0 1 ## alcohol 5.019e-01 2.588e-02 19.4 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8653 on 1117 degrees of freedom ## Multiple R-squared: 0.252, Adjusted R-squared: 0.2513 ## F-statistic: 376.2 on 1 and 1117 DF, p-value: &lt; 2.2e-16 Focusing on the p-value above we can tell that it is unlikely that by chance we will observe a relationship between alcohol and quality. 6.5.3 Multiple Variables In the code block below we will input all of the variables and examine the output. lr &lt;- lm(quality~fixed.acidity + volatile.acidity + citric.acid + residual.sugar + chlorides + free.sulfur.dioxide + density + pH + sulphates + alcohol, data = train) summary(lr) ## ## Call: ## lm(formula = quality ~ fixed.acidity + volatile.acidity + citric.acid + ## residual.sugar + chlorides + free.sulfur.dioxide + density + ## pH + sulphates + alcohol, data = train) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.2099 -0.4497 -0.0791 0.5241 2.5651 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.898e-15 2.383e-02 0.000 1.00000 ## fixed.acidity 1.248e-01 6.537e-02 1.910 0.05645 . ## volatile.acidity -2.657e-01 3.093e-02 -8.589 &lt; 2e-16 *** ## citric.acid -9.537e-02 4.006e-02 -2.381 0.01744 * ## residual.sugar 5.473e-02 3.099e-02 1.766 0.07761 . ## chlorides -9.002e-02 2.936e-02 -3.066 0.00222 ** ## free.sulfur.dioxide -4.450e-02 2.517e-02 -1.768 0.07737 . ## density -9.071e-02 5.979e-02 -1.517 0.12948 ## pH -4.245e-02 4.281e-02 -0.992 0.32162 ## sulphates 1.813e-01 2.888e-02 6.279 4.87e-10 *** ## alcohol 3.934e-01 4.120e-02 9.548 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7972 on 1108 degrees of freedom ## Multiple R-squared: 0.3701, Adjusted R-squared: 0.3644 ## F-statistic: 65.1 on 10 and 1108 DF, p-value: &lt; 2.2e-16 Reviewing the output we find that the inputs: volatile.acidity, sulphates, and alcohol. 6.6 Regularization Linear regression algorithm works by selecting coefficients for each independent variable that minimizes a loss function. However, if the coefficients are large, they can lead to over-fitting on the training dataset, and such a model will not generalize well on the unseen test data. To overcome this shortcoming, well do regularization, which penalizes large coefficients. The following sections of the guide will discuss various regularization algorithms. We will be using the glmnet() package to build the regularized regression models. The glmnet function does not work with dataframes, so we need to create a numeric matrix for the training features and a vector of target values. The lines of code below perform the task of creating model matrix using the dummyVars function from the caret package. The predict function is then applied to create numeric model matrices for training and test. dummies &lt;- dummyVars(quality~., data = wine) train_dummies = predict(dummies, newdata = train) test_dummies = predict(dummies, newdata = test) print(dim(train_dummies)); print(dim(test_dummies)) ## [1] 1119 11 ## [1] 480 11 6.7 Ridge Regression Ridge regression is an extension of linear regression where the loss function is modified to minimize the complexity of the model. This modification is done by adding a penalty parameter that is equivalent to the square of the magnitude of the coefficients. eval_results &lt;- function(true, predicted, df) { SSE &lt;- sum((predicted - true)^2) SST &lt;- sum((true - mean(true))^2) R_square &lt;- 1 - SSE / SST RMSE = sqrt(SSE/nrow(df)) # Model performance metrics data.frame( RMSE = RMSE, Rsquare = R_square ) } x = as.matrix(train_dummies) y_train = train$quality x_test = as.matrix(test_dummies) y_test = test$unemploy lambdas &lt;- 10^seq(2, -3, by = -.1) ridge_reg = glmnet(x, y_train, nlambda = 25, alpha = 0, family = &#39;gaussian&#39;, lambda = lambdas) summary(ridge_reg) ## Length Class Mode ## a0 51 -none- numeric ## beta 561 dgCMatrix S4 ## df 51 -none- numeric ## dim 2 -none- numeric ## lambda 51 -none- numeric ## dev.ratio 51 -none- numeric ## nulldev 1 -none- numeric ## npasses 1 -none- numeric ## jerr 1 -none- numeric ## offset 1 -none- logical ## call 7 -none- call ## nobs 1 -none- numeric cv_ridge &lt;- cv.glmnet(x, y_train, alpha = 0, lambda = lambdas) optimal_lambda &lt;- cv_ridge$lambda.min optimal_lambda ## [1] 0.05011872 6.8 Lasso Regression lambdas &lt;- 10^seq(2, -3, by = -.1) # Setting alpha = 1 implements lasso regression lasso_reg &lt;- cv.glmnet(x, y_train, alpha = 1, lambda = lambdas, standardize = TRUE, nfolds = 5) # Best lambda_best &lt;- lasso_reg$lambda.min lambda_best ## [1] 0.001995262 lasso_model &lt;- glmnet(x, y_train, alpha = 1, lambda = lambda_best, standardize = TRUE) predictions_train &lt;- predict(lasso_model, s = lambda_best, newx = x) eval_results(y_train, predictions_train, train) ## RMSE Rsquare ## 1 0.7898025 0.3756541 predictions_test &lt;- predict(lasso_model, s = lambda_best, newx = x_test) eval_results(y_test, predictions_test, test) ## Warning in mean.default(true): argument is not numeric or logical: returning NA ## RMSE Rsquare ## 1 0 NaN 6.8.1 Features of the glmnet Package \\(\\lambda\\) is defined once and \\(\\alpha\\) where lasso is scaled by \\(\\alpha\\) and ridge penalty is scaled by \\((1-\\alpha\\)). 6.8.2 Elastic Net Regression Elastic net regression combines the properties of ridge and lasso regression # Set training control train_cont &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 5, search = &quot;random&quot;, verboseIter = TRUE) # Train the model elastic_reg &lt;- train(quality ~ ., data = train, method = &quot;glmnet&quot;, preProcess = c(&quot;center&quot;, &quot;scale&quot;), tuneLength = 10, trControl = train_cont) ## + Fold01.Rep1: alpha=0.4004, lambda=0.002439 ## - Fold01.Rep1: alpha=0.4004, lambda=0.002439 ## + Fold01.Rep1: alpha=0.1857, lambda=1.109010 ## - Fold01.Rep1: alpha=0.1857, lambda=1.109010 ## + Fold01.Rep1: alpha=0.9054, lambda=0.120703 ## - Fold01.Rep1: alpha=0.9054, lambda=0.120703 ## + Fold01.Rep1: alpha=0.5759, lambda=1.247636 ## - Fold01.Rep1: alpha=0.5759, lambda=1.247636 ## + Fold01.Rep1: alpha=0.2560, lambda=0.003753 ## - Fold01.Rep1: alpha=0.2560, lambda=0.003753 ## + Fold01.Rep1: alpha=0.7879, lambda=0.195914 ## - Fold01.Rep1: alpha=0.7879, lambda=0.195914 ## + Fold01.Rep1: alpha=0.4018, lambda=0.086926 ## - Fold01.Rep1: alpha=0.4018, lambda=0.086926 ## + Fold01.Rep1: alpha=0.1517, lambda=1.159105 ## - Fold01.Rep1: alpha=0.1517, lambda=1.159105 ## + Fold01.Rep1: alpha=0.3660, lambda=0.692297 ## - Fold01.Rep1: alpha=0.3660, lambda=0.692297 ## + Fold01.Rep1: alpha=0.2080, lambda=0.003654 ## - Fold01.Rep1: alpha=0.2080, lambda=0.003654 ## + Fold02.Rep1: alpha=0.4004, lambda=0.002439 ## - Fold02.Rep1: alpha=0.4004, lambda=0.002439 ## + Fold02.Rep1: alpha=0.1857, lambda=1.109010 ## - Fold02.Rep1: alpha=0.1857, lambda=1.109010 ## + Fold02.Rep1: alpha=0.9054, lambda=0.120703 ## - Fold02.Rep1: alpha=0.9054, lambda=0.120703 ## + Fold02.Rep1: alpha=0.5759, lambda=1.247636 ## - Fold02.Rep1: alpha=0.5759, lambda=1.247636 ## + Fold02.Rep1: alpha=0.2560, lambda=0.003753 ## - Fold02.Rep1: alpha=0.2560, lambda=0.003753 ## + Fold02.Rep1: alpha=0.7879, lambda=0.195914 ## - Fold02.Rep1: alpha=0.7879, lambda=0.195914 ## + Fold02.Rep1: alpha=0.4018, lambda=0.086926 ## - Fold02.Rep1: alpha=0.4018, lambda=0.086926 ## + Fold02.Rep1: alpha=0.1517, lambda=1.159105 ## - Fold02.Rep1: alpha=0.1517, lambda=1.159105 ## + Fold02.Rep1: alpha=0.3660, lambda=0.692297 ## - Fold02.Rep1: alpha=0.3660, lambda=0.692297 ## + Fold02.Rep1: alpha=0.2080, lambda=0.003654 ## - Fold02.Rep1: alpha=0.2080, lambda=0.003654 ## + Fold03.Rep1: alpha=0.4004, lambda=0.002439 ## - Fold03.Rep1: alpha=0.4004, lambda=0.002439 ## + Fold03.Rep1: alpha=0.1857, lambda=1.109010 ## - Fold03.Rep1: alpha=0.1857, lambda=1.109010 ## + Fold03.Rep1: alpha=0.9054, lambda=0.120703 ## - Fold03.Rep1: alpha=0.9054, lambda=0.120703 ## + Fold03.Rep1: alpha=0.5759, lambda=1.247636 ## - Fold03.Rep1: alpha=0.5759, lambda=1.247636 ## + Fold03.Rep1: alpha=0.2560, lambda=0.003753 ## - Fold03.Rep1: alpha=0.2560, lambda=0.003753 ## + Fold03.Rep1: alpha=0.7879, lambda=0.195914 ## - Fold03.Rep1: alpha=0.7879, lambda=0.195914 ## + Fold03.Rep1: alpha=0.4018, lambda=0.086926 ## - Fold03.Rep1: alpha=0.4018, lambda=0.086926 ## + Fold03.Rep1: alpha=0.1517, lambda=1.159105 ## - Fold03.Rep1: alpha=0.1517, lambda=1.159105 ## + Fold03.Rep1: alpha=0.3660, lambda=0.692297 ## - Fold03.Rep1: alpha=0.3660, lambda=0.692297 ## + Fold03.Rep1: alpha=0.2080, lambda=0.003654 ## - Fold03.Rep1: alpha=0.2080, lambda=0.003654 ## + Fold04.Rep1: alpha=0.4004, lambda=0.002439 ## - Fold04.Rep1: alpha=0.4004, lambda=0.002439 ## + Fold04.Rep1: alpha=0.1857, lambda=1.109010 ## - Fold04.Rep1: alpha=0.1857, lambda=1.109010 ## + Fold04.Rep1: alpha=0.9054, lambda=0.120703 ## - Fold04.Rep1: alpha=0.9054, lambda=0.120703 ## + Fold04.Rep1: alpha=0.5759, lambda=1.247636 ## - Fold04.Rep1: alpha=0.5759, lambda=1.247636 ## + Fold04.Rep1: alpha=0.2560, lambda=0.003753 ## - Fold04.Rep1: alpha=0.2560, lambda=0.003753 ## + Fold04.Rep1: alpha=0.7879, lambda=0.195914 ## - Fold04.Rep1: alpha=0.7879, lambda=0.195914 ## + Fold04.Rep1: alpha=0.4018, lambda=0.086926 ## - Fold04.Rep1: alpha=0.4018, lambda=0.086926 ## + Fold04.Rep1: alpha=0.1517, lambda=1.159105 ## - Fold04.Rep1: alpha=0.1517, lambda=1.159105 ## + Fold04.Rep1: alpha=0.3660, lambda=0.692297 ## - Fold04.Rep1: alpha=0.3660, lambda=0.692297 ## + Fold04.Rep1: alpha=0.2080, lambda=0.003654 ## - Fold04.Rep1: alpha=0.2080, lambda=0.003654 ## + Fold05.Rep1: alpha=0.4004, lambda=0.002439 ## - Fold05.Rep1: alpha=0.4004, lambda=0.002439 ## + Fold05.Rep1: alpha=0.1857, lambda=1.109010 ## - Fold05.Rep1: alpha=0.1857, lambda=1.109010 ## + Fold05.Rep1: alpha=0.9054, lambda=0.120703 ## - Fold05.Rep1: alpha=0.9054, lambda=0.120703 ## + Fold05.Rep1: alpha=0.5759, lambda=1.247636 ## - Fold05.Rep1: alpha=0.5759, lambda=1.247636 ## + Fold05.Rep1: alpha=0.2560, lambda=0.003753 ## - Fold05.Rep1: alpha=0.2560, lambda=0.003753 ## + Fold05.Rep1: alpha=0.7879, lambda=0.195914 ## - Fold05.Rep1: alpha=0.7879, lambda=0.195914 ## + Fold05.Rep1: alpha=0.4018, lambda=0.086926 ## - Fold05.Rep1: alpha=0.4018, lambda=0.086926 ## + Fold05.Rep1: alpha=0.1517, lambda=1.159105 ## - Fold05.Rep1: alpha=0.1517, lambda=1.159105 ## + Fold05.Rep1: alpha=0.3660, lambda=0.692297 ## - Fold05.Rep1: alpha=0.3660, lambda=0.692297 ## + Fold05.Rep1: alpha=0.2080, lambda=0.003654 ## - Fold05.Rep1: alpha=0.2080, lambda=0.003654 ## + Fold06.Rep1: alpha=0.4004, lambda=0.002439 ## - Fold06.Rep1: alpha=0.4004, lambda=0.002439 ## + Fold06.Rep1: alpha=0.1857, lambda=1.109010 ## - Fold06.Rep1: alpha=0.1857, lambda=1.109010 ## + Fold06.Rep1: alpha=0.9054, lambda=0.120703 ## - Fold06.Rep1: alpha=0.9054, lambda=0.120703 ## + Fold06.Rep1: alpha=0.5759, lambda=1.247636 ## - Fold06.Rep1: alpha=0.5759, lambda=1.247636 ## + Fold06.Rep1: alpha=0.2560, lambda=0.003753 ## - Fold06.Rep1: alpha=0.2560, lambda=0.003753 ## + Fold06.Rep1: alpha=0.7879, lambda=0.195914 ## - Fold06.Rep1: alpha=0.7879, lambda=0.195914 ## + Fold06.Rep1: alpha=0.4018, lambda=0.086926 ## - Fold06.Rep1: alpha=0.4018, lambda=0.086926 ## + Fold06.Rep1: alpha=0.1517, lambda=1.159105 ## - Fold06.Rep1: alpha=0.1517, lambda=1.159105 ## + Fold06.Rep1: alpha=0.3660, lambda=0.692297 ## - Fold06.Rep1: alpha=0.3660, lambda=0.692297 ## + Fold06.Rep1: alpha=0.2080, lambda=0.003654 ## - Fold06.Rep1: alpha=0.2080, lambda=0.003654 ## + Fold07.Rep1: alpha=0.4004, lambda=0.002439 ## - Fold07.Rep1: alpha=0.4004, lambda=0.002439 ## + Fold07.Rep1: alpha=0.1857, lambda=1.109010 ## - Fold07.Rep1: alpha=0.1857, lambda=1.109010 ## + Fold07.Rep1: alpha=0.9054, lambda=0.120703 ## - Fold07.Rep1: alpha=0.9054, lambda=0.120703 ## + Fold07.Rep1: alpha=0.5759, lambda=1.247636 ## - Fold07.Rep1: alpha=0.5759, lambda=1.247636 ## + Fold07.Rep1: alpha=0.2560, lambda=0.003753 ## - Fold07.Rep1: alpha=0.2560, lambda=0.003753 ## + Fold07.Rep1: alpha=0.7879, lambda=0.195914 ## - Fold07.Rep1: alpha=0.7879, lambda=0.195914 ## + Fold07.Rep1: alpha=0.4018, lambda=0.086926 ## - Fold07.Rep1: alpha=0.4018, lambda=0.086926 ## + Fold07.Rep1: alpha=0.1517, lambda=1.159105 ## - Fold07.Rep1: alpha=0.1517, lambda=1.159105 ## + Fold07.Rep1: alpha=0.3660, lambda=0.692297 ## - Fold07.Rep1: alpha=0.3660, lambda=0.692297 ## + Fold07.Rep1: alpha=0.2080, lambda=0.003654 ## - Fold07.Rep1: alpha=0.2080, lambda=0.003654 ## + Fold08.Rep1: alpha=0.4004, lambda=0.002439 ## - Fold08.Rep1: alpha=0.4004, lambda=0.002439 ## + Fold08.Rep1: alpha=0.1857, lambda=1.109010 ## - Fold08.Rep1: alpha=0.1857, lambda=1.109010 ## + Fold08.Rep1: alpha=0.9054, lambda=0.120703 ## - Fold08.Rep1: alpha=0.9054, lambda=0.120703 ## + Fold08.Rep1: alpha=0.5759, lambda=1.247636 ## - Fold08.Rep1: alpha=0.5759, lambda=1.247636 ## + Fold08.Rep1: alpha=0.2560, lambda=0.003753 ## - Fold08.Rep1: alpha=0.2560, lambda=0.003753 ## + Fold08.Rep1: alpha=0.7879, lambda=0.195914 ## - Fold08.Rep1: alpha=0.7879, lambda=0.195914 ## + Fold08.Rep1: alpha=0.4018, lambda=0.086926 ## - Fold08.Rep1: alpha=0.4018, lambda=0.086926 ## + Fold08.Rep1: alpha=0.1517, lambda=1.159105 ## - Fold08.Rep1: alpha=0.1517, lambda=1.159105 ## + Fold08.Rep1: alpha=0.3660, lambda=0.692297 ## - Fold08.Rep1: alpha=0.3660, lambda=0.692297 ## + Fold08.Rep1: alpha=0.2080, lambda=0.003654 ## - Fold08.Rep1: alpha=0.2080, lambda=0.003654 ## + Fold09.Rep1: alpha=0.4004, lambda=0.002439 ## - Fold09.Rep1: alpha=0.4004, lambda=0.002439 ## + Fold09.Rep1: alpha=0.1857, lambda=1.109010 ## - Fold09.Rep1: alpha=0.1857, lambda=1.109010 ## + Fold09.Rep1: alpha=0.9054, lambda=0.120703 ## - Fold09.Rep1: alpha=0.9054, lambda=0.120703 ## + Fold09.Rep1: alpha=0.5759, lambda=1.247636 ## - Fold09.Rep1: alpha=0.5759, lambda=1.247636 ## + Fold09.Rep1: alpha=0.2560, lambda=0.003753 ## - Fold09.Rep1: alpha=0.2560, lambda=0.003753 ## + Fold09.Rep1: alpha=0.7879, lambda=0.195914 ## - Fold09.Rep1: alpha=0.7879, lambda=0.195914 ## + Fold09.Rep1: alpha=0.4018, lambda=0.086926 ## - Fold09.Rep1: alpha=0.4018, lambda=0.086926 ## + Fold09.Rep1: alpha=0.1517, lambda=1.159105 ## - Fold09.Rep1: alpha=0.1517, lambda=1.159105 ## + Fold09.Rep1: alpha=0.3660, lambda=0.692297 ## - Fold09.Rep1: alpha=0.3660, lambda=0.692297 ## + Fold09.Rep1: alpha=0.2080, lambda=0.003654 ## - Fold09.Rep1: alpha=0.2080, lambda=0.003654 ## + Fold10.Rep1: alpha=0.4004, lambda=0.002439 ## - Fold10.Rep1: alpha=0.4004, lambda=0.002439 ## + Fold10.Rep1: alpha=0.1857, lambda=1.109010 ## - Fold10.Rep1: alpha=0.1857, lambda=1.109010 ## + Fold10.Rep1: alpha=0.9054, lambda=0.120703 ## - Fold10.Rep1: alpha=0.9054, lambda=0.120703 ## + Fold10.Rep1: alpha=0.5759, lambda=1.247636 ## - Fold10.Rep1: alpha=0.5759, lambda=1.247636 ## + Fold10.Rep1: alpha=0.2560, lambda=0.003753 ## - Fold10.Rep1: alpha=0.2560, lambda=0.003753 ## + Fold10.Rep1: alpha=0.7879, lambda=0.195914 ## - Fold10.Rep1: alpha=0.7879, lambda=0.195914 ## + Fold10.Rep1: alpha=0.4018, lambda=0.086926 ## - Fold10.Rep1: alpha=0.4018, lambda=0.086926 ## + Fold10.Rep1: alpha=0.1517, lambda=1.159105 ## - Fold10.Rep1: alpha=0.1517, lambda=1.159105 ## + Fold10.Rep1: alpha=0.3660, lambda=0.692297 ## - Fold10.Rep1: alpha=0.3660, lambda=0.692297 ## + Fold10.Rep1: alpha=0.2080, lambda=0.003654 ## - Fold10.Rep1: alpha=0.2080, lambda=0.003654 ## + Fold01.Rep2: alpha=0.4004, lambda=0.002439 ## - Fold01.Rep2: alpha=0.4004, lambda=0.002439 ## + Fold01.Rep2: alpha=0.1857, lambda=1.109010 ## - Fold01.Rep2: alpha=0.1857, lambda=1.109010 ## + Fold01.Rep2: alpha=0.9054, lambda=0.120703 ## - Fold01.Rep2: alpha=0.9054, lambda=0.120703 ## + Fold01.Rep2: alpha=0.5759, lambda=1.247636 ## - Fold01.Rep2: alpha=0.5759, lambda=1.247636 ## + Fold01.Rep2: alpha=0.2560, lambda=0.003753 ## - Fold01.Rep2: alpha=0.2560, lambda=0.003753 ## + Fold01.Rep2: alpha=0.7879, lambda=0.195914 ## - Fold01.Rep2: alpha=0.7879, lambda=0.195914 ## + Fold01.Rep2: alpha=0.4018, lambda=0.086926 ## - Fold01.Rep2: alpha=0.4018, lambda=0.086926 ## + Fold01.Rep2: alpha=0.1517, lambda=1.159105 ## - Fold01.Rep2: alpha=0.1517, lambda=1.159105 ## + Fold01.Rep2: alpha=0.3660, lambda=0.692297 ## - Fold01.Rep2: alpha=0.3660, lambda=0.692297 ## + Fold01.Rep2: alpha=0.2080, lambda=0.003654 ## - Fold01.Rep2: alpha=0.2080, lambda=0.003654 ## + Fold02.Rep2: alpha=0.4004, lambda=0.002439 ## - Fold02.Rep2: alpha=0.4004, lambda=0.002439 ## + Fold02.Rep2: alpha=0.1857, lambda=1.109010 ## - Fold02.Rep2: alpha=0.1857, lambda=1.109010 ## + Fold02.Rep2: alpha=0.9054, lambda=0.120703 ## - Fold02.Rep2: alpha=0.9054, lambda=0.120703 ## + Fold02.Rep2: alpha=0.5759, lambda=1.247636 ## - Fold02.Rep2: alpha=0.5759, lambda=1.247636 ## + Fold02.Rep2: alpha=0.2560, lambda=0.003753 ## - Fold02.Rep2: alpha=0.2560, lambda=0.003753 ## + Fold02.Rep2: alpha=0.7879, lambda=0.195914 ## - Fold02.Rep2: alpha=0.7879, lambda=0.195914 ## + Fold02.Rep2: alpha=0.4018, lambda=0.086926 ## - Fold02.Rep2: alpha=0.4018, lambda=0.086926 ## + Fold02.Rep2: alpha=0.1517, lambda=1.159105 ## - Fold02.Rep2: alpha=0.1517, lambda=1.159105 ## + Fold02.Rep2: alpha=0.3660, lambda=0.692297 ## - Fold02.Rep2: alpha=0.3660, lambda=0.692297 ## + Fold02.Rep2: alpha=0.2080, lambda=0.003654 ## - Fold02.Rep2: alpha=0.2080, lambda=0.003654 ## + Fold03.Rep2: alpha=0.4004, lambda=0.002439 ## - Fold03.Rep2: alpha=0.4004, lambda=0.002439 ## + Fold03.Rep2: alpha=0.1857, lambda=1.109010 ## - Fold03.Rep2: alpha=0.1857, lambda=1.109010 ## + Fold03.Rep2: alpha=0.9054, lambda=0.120703 ## - Fold03.Rep2: alpha=0.9054, lambda=0.120703 ## + Fold03.Rep2: alpha=0.5759, lambda=1.247636 ## - Fold03.Rep2: alpha=0.5759, lambda=1.247636 ## + Fold03.Rep2: alpha=0.2560, lambda=0.003753 ## - Fold03.Rep2: alpha=0.2560, lambda=0.003753 ## + Fold03.Rep2: alpha=0.7879, lambda=0.195914 ## - Fold03.Rep2: alpha=0.7879, lambda=0.195914 ## + Fold03.Rep2: alpha=0.4018, lambda=0.086926 ## - Fold03.Rep2: alpha=0.4018, lambda=0.086926 ## + Fold03.Rep2: alpha=0.1517, lambda=1.159105 ## - Fold03.Rep2: alpha=0.1517, lambda=1.159105 ## + Fold03.Rep2: alpha=0.3660, lambda=0.692297 ## - Fold03.Rep2: alpha=0.3660, lambda=0.692297 ## + Fold03.Rep2: alpha=0.2080, lambda=0.003654 ## - Fold03.Rep2: alpha=0.2080, lambda=0.003654 ## + Fold04.Rep2: alpha=0.4004, lambda=0.002439 ## - Fold04.Rep2: alpha=0.4004, lambda=0.002439 ## + Fold04.Rep2: alpha=0.1857, lambda=1.109010 ## - Fold04.Rep2: alpha=0.1857, lambda=1.109010 ## + Fold04.Rep2: alpha=0.9054, lambda=0.120703 ## - Fold04.Rep2: alpha=0.9054, lambda=0.120703 ## + Fold04.Rep2: alpha=0.5759, lambda=1.247636 ## - Fold04.Rep2: alpha=0.5759, lambda=1.247636 ## + Fold04.Rep2: alpha=0.2560, lambda=0.003753 ## - Fold04.Rep2: alpha=0.2560, lambda=0.003753 ## + Fold04.Rep2: alpha=0.7879, lambda=0.195914 ## - Fold04.Rep2: alpha=0.7879, lambda=0.195914 ## + Fold04.Rep2: alpha=0.4018, lambda=0.086926 ## - Fold04.Rep2: alpha=0.4018, lambda=0.086926 ## + Fold04.Rep2: alpha=0.1517, lambda=1.159105 ## - Fold04.Rep2: alpha=0.1517, lambda=1.159105 ## + Fold04.Rep2: alpha=0.3660, lambda=0.692297 ## - Fold04.Rep2: alpha=0.3660, lambda=0.692297 ## + Fold04.Rep2: alpha=0.2080, lambda=0.003654 ## - Fold04.Rep2: alpha=0.2080, lambda=0.003654 ## + Fold05.Rep2: alpha=0.4004, lambda=0.002439 ## - Fold05.Rep2: alpha=0.4004, lambda=0.002439 ## + Fold05.Rep2: alpha=0.1857, lambda=1.109010 ## - Fold05.Rep2: alpha=0.1857, lambda=1.109010 ## + Fold05.Rep2: alpha=0.9054, lambda=0.120703 ## - Fold05.Rep2: alpha=0.9054, lambda=0.120703 ## + Fold05.Rep2: alpha=0.5759, lambda=1.247636 ## - Fold05.Rep2: alpha=0.5759, lambda=1.247636 ## + Fold05.Rep2: alpha=0.2560, lambda=0.003753 ## - Fold05.Rep2: alpha=0.2560, lambda=0.003753 ## + Fold05.Rep2: alpha=0.7879, lambda=0.195914 ## - Fold05.Rep2: alpha=0.7879, lambda=0.195914 ## + Fold05.Rep2: alpha=0.4018, lambda=0.086926 ## - Fold05.Rep2: alpha=0.4018, lambda=0.086926 ## + Fold05.Rep2: alpha=0.1517, lambda=1.159105 ## - Fold05.Rep2: alpha=0.1517, lambda=1.159105 ## + Fold05.Rep2: alpha=0.3660, lambda=0.692297 ## - Fold05.Rep2: alpha=0.3660, lambda=0.692297 ## + Fold05.Rep2: alpha=0.2080, lambda=0.003654 ## - Fold05.Rep2: alpha=0.2080, lambda=0.003654 ## + Fold06.Rep2: alpha=0.4004, lambda=0.002439 ## - Fold06.Rep2: alpha=0.4004, lambda=0.002439 ## + Fold06.Rep2: alpha=0.1857, lambda=1.109010 ## - Fold06.Rep2: alpha=0.1857, lambda=1.109010 ## + Fold06.Rep2: alpha=0.9054, lambda=0.120703 ## - Fold06.Rep2: alpha=0.9054, lambda=0.120703 ## + Fold06.Rep2: alpha=0.5759, lambda=1.247636 ## - Fold06.Rep2: alpha=0.5759, lambda=1.247636 ## + Fold06.Rep2: alpha=0.2560, lambda=0.003753 ## - Fold06.Rep2: alpha=0.2560, lambda=0.003753 ## + Fold06.Rep2: alpha=0.7879, lambda=0.195914 ## - Fold06.Rep2: alpha=0.7879, lambda=0.195914 ## + Fold06.Rep2: alpha=0.4018, lambda=0.086926 ## - Fold06.Rep2: alpha=0.4018, lambda=0.086926 ## + Fold06.Rep2: alpha=0.1517, lambda=1.159105 ## - Fold06.Rep2: alpha=0.1517, lambda=1.159105 ## + Fold06.Rep2: alpha=0.3660, lambda=0.692297 ## - Fold06.Rep2: alpha=0.3660, lambda=0.692297 ## + Fold06.Rep2: alpha=0.2080, lambda=0.003654 ## - Fold06.Rep2: alpha=0.2080, lambda=0.003654 ## + Fold07.Rep2: alpha=0.4004, lambda=0.002439 ## - Fold07.Rep2: alpha=0.4004, lambda=0.002439 ## + Fold07.Rep2: alpha=0.1857, lambda=1.109010 ## - Fold07.Rep2: alpha=0.1857, lambda=1.109010 ## + Fold07.Rep2: alpha=0.9054, lambda=0.120703 ## - Fold07.Rep2: alpha=0.9054, lambda=0.120703 ## + Fold07.Rep2: alpha=0.5759, lambda=1.247636 ## - Fold07.Rep2: alpha=0.5759, lambda=1.247636 ## + Fold07.Rep2: alpha=0.2560, lambda=0.003753 ## - Fold07.Rep2: alpha=0.2560, lambda=0.003753 ## + Fold07.Rep2: alpha=0.7879, lambda=0.195914 ## - Fold07.Rep2: alpha=0.7879, lambda=0.195914 ## + Fold07.Rep2: alpha=0.4018, lambda=0.086926 ## - Fold07.Rep2: alpha=0.4018, lambda=0.086926 ## + Fold07.Rep2: alpha=0.1517, lambda=1.159105 ## - Fold07.Rep2: alpha=0.1517, lambda=1.159105 ## + Fold07.Rep2: alpha=0.3660, lambda=0.692297 ## - Fold07.Rep2: alpha=0.3660, lambda=0.692297 ## + Fold07.Rep2: alpha=0.2080, lambda=0.003654 ## - Fold07.Rep2: alpha=0.2080, lambda=0.003654 ## + Fold08.Rep2: alpha=0.4004, lambda=0.002439 ## - Fold08.Rep2: alpha=0.4004, lambda=0.002439 ## + Fold08.Rep2: alpha=0.1857, lambda=1.109010 ## - Fold08.Rep2: alpha=0.1857, lambda=1.109010 ## + Fold08.Rep2: alpha=0.9054, lambda=0.120703 ## - Fold08.Rep2: alpha=0.9054, lambda=0.120703 ## + Fold08.Rep2: alpha=0.5759, lambda=1.247636 ## - Fold08.Rep2: alpha=0.5759, lambda=1.247636 ## + Fold08.Rep2: alpha=0.2560, lambda=0.003753 ## - Fold08.Rep2: alpha=0.2560, lambda=0.003753 ## + Fold08.Rep2: alpha=0.7879, lambda=0.195914 ## - Fold08.Rep2: alpha=0.7879, lambda=0.195914 ## + Fold08.Rep2: alpha=0.4018, lambda=0.086926 ## - Fold08.Rep2: alpha=0.4018, lambda=0.086926 ## + Fold08.Rep2: alpha=0.1517, lambda=1.159105 ## - Fold08.Rep2: alpha=0.1517, lambda=1.159105 ## + Fold08.Rep2: alpha=0.3660, lambda=0.692297 ## - Fold08.Rep2: alpha=0.3660, lambda=0.692297 ## + Fold08.Rep2: alpha=0.2080, lambda=0.003654 ## - Fold08.Rep2: alpha=0.2080, lambda=0.003654 ## + Fold09.Rep2: alpha=0.4004, lambda=0.002439 ## - Fold09.Rep2: alpha=0.4004, lambda=0.002439 ## + Fold09.Rep2: alpha=0.1857, lambda=1.109010 ## - Fold09.Rep2: alpha=0.1857, lambda=1.109010 ## + Fold09.Rep2: alpha=0.9054, lambda=0.120703 ## - Fold09.Rep2: alpha=0.9054, lambda=0.120703 ## + Fold09.Rep2: alpha=0.5759, lambda=1.247636 ## - Fold09.Rep2: alpha=0.5759, lambda=1.247636 ## + Fold09.Rep2: alpha=0.2560, lambda=0.003753 ## - Fold09.Rep2: alpha=0.2560, lambda=0.003753 ## + Fold09.Rep2: alpha=0.7879, lambda=0.195914 ## - Fold09.Rep2: alpha=0.7879, lambda=0.195914 ## + Fold09.Rep2: alpha=0.4018, lambda=0.086926 ## - Fold09.Rep2: alpha=0.4018, lambda=0.086926 ## + Fold09.Rep2: alpha=0.1517, lambda=1.159105 ## - Fold09.Rep2: alpha=0.1517, lambda=1.159105 ## + Fold09.Rep2: alpha=0.3660, lambda=0.692297 ## - Fold09.Rep2: alpha=0.3660, lambda=0.692297 ## + Fold09.Rep2: alpha=0.2080, lambda=0.003654 ## - Fold09.Rep2: alpha=0.2080, lambda=0.003654 ## + Fold10.Rep2: alpha=0.4004, lambda=0.002439 ## - Fold10.Rep2: alpha=0.4004, lambda=0.002439 ## + Fold10.Rep2: alpha=0.1857, lambda=1.109010 ## - Fold10.Rep2: alpha=0.1857, lambda=1.109010 ## + Fold10.Rep2: alpha=0.9054, lambda=0.120703 ## - Fold10.Rep2: alpha=0.9054, lambda=0.120703 ## + Fold10.Rep2: alpha=0.5759, lambda=1.247636 ## - Fold10.Rep2: alpha=0.5759, lambda=1.247636 ## + Fold10.Rep2: alpha=0.2560, lambda=0.003753 ## - Fold10.Rep2: alpha=0.2560, lambda=0.003753 ## + Fold10.Rep2: alpha=0.7879, lambda=0.195914 ## - Fold10.Rep2: alpha=0.7879, lambda=0.195914 ## + Fold10.Rep2: alpha=0.4018, lambda=0.086926 ## - Fold10.Rep2: alpha=0.4018, lambda=0.086926 ## + Fold10.Rep2: alpha=0.1517, lambda=1.159105 ## - Fold10.Rep2: alpha=0.1517, lambda=1.159105 ## + Fold10.Rep2: alpha=0.3660, lambda=0.692297 ## - Fold10.Rep2: alpha=0.3660, lambda=0.692297 ## + Fold10.Rep2: alpha=0.2080, lambda=0.003654 ## - Fold10.Rep2: alpha=0.2080, lambda=0.003654 ## + Fold01.Rep3: alpha=0.4004, lambda=0.002439 ## - Fold01.Rep3: alpha=0.4004, lambda=0.002439 ## + Fold01.Rep3: alpha=0.1857, lambda=1.109010 ## - Fold01.Rep3: alpha=0.1857, lambda=1.109010 ## + Fold01.Rep3: alpha=0.9054, lambda=0.120703 ## - Fold01.Rep3: alpha=0.9054, lambda=0.120703 ## + Fold01.Rep3: alpha=0.5759, lambda=1.247636 ## - Fold01.Rep3: alpha=0.5759, lambda=1.247636 ## + Fold01.Rep3: alpha=0.2560, lambda=0.003753 ## - Fold01.Rep3: alpha=0.2560, lambda=0.003753 ## + Fold01.Rep3: alpha=0.7879, lambda=0.195914 ## - Fold01.Rep3: alpha=0.7879, lambda=0.195914 ## + Fold01.Rep3: alpha=0.4018, lambda=0.086926 ## - Fold01.Rep3: alpha=0.4018, lambda=0.086926 ## + Fold01.Rep3: alpha=0.1517, lambda=1.159105 ## - Fold01.Rep3: alpha=0.1517, lambda=1.159105 ## + Fold01.Rep3: alpha=0.3660, lambda=0.692297 ## - Fold01.Rep3: alpha=0.3660, lambda=0.692297 ## + Fold01.Rep3: alpha=0.2080, lambda=0.003654 ## - Fold01.Rep3: alpha=0.2080, lambda=0.003654 ## + Fold02.Rep3: alpha=0.4004, lambda=0.002439 ## - Fold02.Rep3: alpha=0.4004, lambda=0.002439 ## + Fold02.Rep3: alpha=0.1857, lambda=1.109010 ## - Fold02.Rep3: alpha=0.1857, lambda=1.109010 ## + Fold02.Rep3: alpha=0.9054, lambda=0.120703 ## - Fold02.Rep3: alpha=0.9054, lambda=0.120703 ## + Fold02.Rep3: alpha=0.5759, lambda=1.247636 ## - Fold02.Rep3: alpha=0.5759, lambda=1.247636 ## + Fold02.Rep3: alpha=0.2560, lambda=0.003753 ## - Fold02.Rep3: alpha=0.2560, lambda=0.003753 ## + Fold02.Rep3: alpha=0.7879, lambda=0.195914 ## - Fold02.Rep3: alpha=0.7879, lambda=0.195914 ## + Fold02.Rep3: alpha=0.4018, lambda=0.086926 ## - Fold02.Rep3: alpha=0.4018, lambda=0.086926 ## + Fold02.Rep3: alpha=0.1517, lambda=1.159105 ## - Fold02.Rep3: alpha=0.1517, lambda=1.159105 ## + Fold02.Rep3: alpha=0.3660, lambda=0.692297 ## - Fold02.Rep3: alpha=0.3660, lambda=0.692297 ## + Fold02.Rep3: alpha=0.2080, lambda=0.003654 ## - Fold02.Rep3: alpha=0.2080, lambda=0.003654 ## + Fold03.Rep3: alpha=0.4004, lambda=0.002439 ## - Fold03.Rep3: alpha=0.4004, lambda=0.002439 ## + Fold03.Rep3: alpha=0.1857, lambda=1.109010 ## - Fold03.Rep3: alpha=0.1857, lambda=1.109010 ## + Fold03.Rep3: alpha=0.9054, lambda=0.120703 ## - Fold03.Rep3: alpha=0.9054, lambda=0.120703 ## + Fold03.Rep3: alpha=0.5759, lambda=1.247636 ## - Fold03.Rep3: alpha=0.5759, lambda=1.247636 ## + Fold03.Rep3: alpha=0.2560, lambda=0.003753 ## - Fold03.Rep3: alpha=0.2560, lambda=0.003753 ## + Fold03.Rep3: alpha=0.7879, lambda=0.195914 ## - Fold03.Rep3: alpha=0.7879, lambda=0.195914 ## + Fold03.Rep3: alpha=0.4018, lambda=0.086926 ## - Fold03.Rep3: alpha=0.4018, lambda=0.086926 ## + Fold03.Rep3: alpha=0.1517, lambda=1.159105 ## - Fold03.Rep3: alpha=0.1517, lambda=1.159105 ## + Fold03.Rep3: alpha=0.3660, lambda=0.692297 ## - Fold03.Rep3: alpha=0.3660, lambda=0.692297 ## + Fold03.Rep3: alpha=0.2080, lambda=0.003654 ## - Fold03.Rep3: alpha=0.2080, lambda=0.003654 ## + Fold04.Rep3: alpha=0.4004, lambda=0.002439 ## - Fold04.Rep3: alpha=0.4004, lambda=0.002439 ## + Fold04.Rep3: alpha=0.1857, lambda=1.109010 ## - Fold04.Rep3: alpha=0.1857, lambda=1.109010 ## + Fold04.Rep3: alpha=0.9054, lambda=0.120703 ## - Fold04.Rep3: alpha=0.9054, lambda=0.120703 ## + Fold04.Rep3: alpha=0.5759, lambda=1.247636 ## - Fold04.Rep3: alpha=0.5759, lambda=1.247636 ## + Fold04.Rep3: alpha=0.2560, lambda=0.003753 ## - Fold04.Rep3: alpha=0.2560, lambda=0.003753 ## + Fold04.Rep3: alpha=0.7879, lambda=0.195914 ## - Fold04.Rep3: alpha=0.7879, lambda=0.195914 ## + Fold04.Rep3: alpha=0.4018, lambda=0.086926 ## - Fold04.Rep3: alpha=0.4018, lambda=0.086926 ## + Fold04.Rep3: alpha=0.1517, lambda=1.159105 ## - Fold04.Rep3: alpha=0.1517, lambda=1.159105 ## + Fold04.Rep3: alpha=0.3660, lambda=0.692297 ## - Fold04.Rep3: alpha=0.3660, lambda=0.692297 ## + Fold04.Rep3: alpha=0.2080, lambda=0.003654 ## - Fold04.Rep3: alpha=0.2080, lambda=0.003654 ## + Fold05.Rep3: alpha=0.4004, lambda=0.002439 ## - Fold05.Rep3: alpha=0.4004, lambda=0.002439 ## + Fold05.Rep3: alpha=0.1857, lambda=1.109010 ## - Fold05.Rep3: alpha=0.1857, lambda=1.109010 ## + Fold05.Rep3: alpha=0.9054, lambda=0.120703 ## - Fold05.Rep3: alpha=0.9054, lambda=0.120703 ## + Fold05.Rep3: alpha=0.5759, lambda=1.247636 ## - Fold05.Rep3: alpha=0.5759, lambda=1.247636 ## + Fold05.Rep3: alpha=0.2560, lambda=0.003753 ## - Fold05.Rep3: alpha=0.2560, lambda=0.003753 ## + Fold05.Rep3: alpha=0.7879, lambda=0.195914 ## - Fold05.Rep3: alpha=0.7879, lambda=0.195914 ## + Fold05.Rep3: alpha=0.4018, lambda=0.086926 ## - Fold05.Rep3: alpha=0.4018, lambda=0.086926 ## + Fold05.Rep3: alpha=0.1517, lambda=1.159105 ## - Fold05.Rep3: alpha=0.1517, lambda=1.159105 ## + Fold05.Rep3: alpha=0.3660, lambda=0.692297 ## - Fold05.Rep3: alpha=0.3660, lambda=0.692297 ## + Fold05.Rep3: alpha=0.2080, lambda=0.003654 ## - Fold05.Rep3: alpha=0.2080, lambda=0.003654 ## + Fold06.Rep3: alpha=0.4004, lambda=0.002439 ## - Fold06.Rep3: alpha=0.4004, lambda=0.002439 ## + Fold06.Rep3: alpha=0.1857, lambda=1.109010 ## - Fold06.Rep3: alpha=0.1857, lambda=1.109010 ## + Fold06.Rep3: alpha=0.9054, lambda=0.120703 ## - Fold06.Rep3: alpha=0.9054, lambda=0.120703 ## + Fold06.Rep3: alpha=0.5759, lambda=1.247636 ## - Fold06.Rep3: alpha=0.5759, lambda=1.247636 ## + Fold06.Rep3: alpha=0.2560, lambda=0.003753 ## - Fold06.Rep3: alpha=0.2560, lambda=0.003753 ## + Fold06.Rep3: alpha=0.7879, lambda=0.195914 ## - Fold06.Rep3: alpha=0.7879, lambda=0.195914 ## + Fold06.Rep3: alpha=0.4018, lambda=0.086926 ## - Fold06.Rep3: alpha=0.4018, lambda=0.086926 ## + Fold06.Rep3: alpha=0.1517, lambda=1.159105 ## - Fold06.Rep3: alpha=0.1517, lambda=1.159105 ## + Fold06.Rep3: alpha=0.3660, lambda=0.692297 ## - Fold06.Rep3: alpha=0.3660, lambda=0.692297 ## + Fold06.Rep3: alpha=0.2080, lambda=0.003654 ## - Fold06.Rep3: alpha=0.2080, lambda=0.003654 ## + Fold07.Rep3: alpha=0.4004, lambda=0.002439 ## - Fold07.Rep3: alpha=0.4004, lambda=0.002439 ## + Fold07.Rep3: alpha=0.1857, lambda=1.109010 ## - Fold07.Rep3: alpha=0.1857, lambda=1.109010 ## + Fold07.Rep3: alpha=0.9054, lambda=0.120703 ## - Fold07.Rep3: alpha=0.9054, lambda=0.120703 ## + Fold07.Rep3: alpha=0.5759, lambda=1.247636 ## - Fold07.Rep3: alpha=0.5759, lambda=1.247636 ## + Fold07.Rep3: alpha=0.2560, lambda=0.003753 ## - Fold07.Rep3: alpha=0.2560, lambda=0.003753 ## + Fold07.Rep3: alpha=0.7879, lambda=0.195914 ## - Fold07.Rep3: alpha=0.7879, lambda=0.195914 ## + Fold07.Rep3: alpha=0.4018, lambda=0.086926 ## - Fold07.Rep3: alpha=0.4018, lambda=0.086926 ## + Fold07.Rep3: alpha=0.1517, lambda=1.159105 ## - Fold07.Rep3: alpha=0.1517, lambda=1.159105 ## + Fold07.Rep3: alpha=0.3660, lambda=0.692297 ## - Fold07.Rep3: alpha=0.3660, lambda=0.692297 ## + Fold07.Rep3: alpha=0.2080, lambda=0.003654 ## - Fold07.Rep3: alpha=0.2080, lambda=0.003654 ## + Fold08.Rep3: alpha=0.4004, lambda=0.002439 ## - Fold08.Rep3: alpha=0.4004, lambda=0.002439 ## + Fold08.Rep3: alpha=0.1857, lambda=1.109010 ## - Fold08.Rep3: alpha=0.1857, lambda=1.109010 ## + Fold08.Rep3: alpha=0.9054, lambda=0.120703 ## - Fold08.Rep3: alpha=0.9054, lambda=0.120703 ## + Fold08.Rep3: alpha=0.5759, lambda=1.247636 ## - Fold08.Rep3: alpha=0.5759, lambda=1.247636 ## + Fold08.Rep3: alpha=0.2560, lambda=0.003753 ## - Fold08.Rep3: alpha=0.2560, lambda=0.003753 ## + Fold08.Rep3: alpha=0.7879, lambda=0.195914 ## - Fold08.Rep3: alpha=0.7879, lambda=0.195914 ## + Fold08.Rep3: alpha=0.4018, lambda=0.086926 ## - Fold08.Rep3: alpha=0.4018, lambda=0.086926 ## + Fold08.Rep3: alpha=0.1517, lambda=1.159105 ## - Fold08.Rep3: alpha=0.1517, lambda=1.159105 ## + Fold08.Rep3: alpha=0.3660, lambda=0.692297 ## - Fold08.Rep3: alpha=0.3660, lambda=0.692297 ## + Fold08.Rep3: alpha=0.2080, lambda=0.003654 ## - Fold08.Rep3: alpha=0.2080, lambda=0.003654 ## + Fold09.Rep3: alpha=0.4004, lambda=0.002439 ## - Fold09.Rep3: alpha=0.4004, lambda=0.002439 ## + Fold09.Rep3: alpha=0.1857, lambda=1.109010 ## - Fold09.Rep3: alpha=0.1857, lambda=1.109010 ## + Fold09.Rep3: alpha=0.9054, lambda=0.120703 ## - Fold09.Rep3: alpha=0.9054, lambda=0.120703 ## + Fold09.Rep3: alpha=0.5759, lambda=1.247636 ## - Fold09.Rep3: alpha=0.5759, lambda=1.247636 ## + Fold09.Rep3: alpha=0.2560, lambda=0.003753 ## - Fold09.Rep3: alpha=0.2560, lambda=0.003753 ## + Fold09.Rep3: alpha=0.7879, lambda=0.195914 ## - Fold09.Rep3: alpha=0.7879, lambda=0.195914 ## + Fold09.Rep3: alpha=0.4018, lambda=0.086926 ## - Fold09.Rep3: alpha=0.4018, lambda=0.086926 ## + Fold09.Rep3: alpha=0.1517, lambda=1.159105 ## - Fold09.Rep3: alpha=0.1517, lambda=1.159105 ## + Fold09.Rep3: alpha=0.3660, lambda=0.692297 ## - Fold09.Rep3: alpha=0.3660, lambda=0.692297 ## + Fold09.Rep3: alpha=0.2080, lambda=0.003654 ## - Fold09.Rep3: alpha=0.2080, lambda=0.003654 ## + Fold10.Rep3: alpha=0.4004, lambda=0.002439 ## - Fold10.Rep3: alpha=0.4004, lambda=0.002439 ## + Fold10.Rep3: alpha=0.1857, lambda=1.109010 ## - Fold10.Rep3: alpha=0.1857, lambda=1.109010 ## + Fold10.Rep3: alpha=0.9054, lambda=0.120703 ## - Fold10.Rep3: alpha=0.9054, lambda=0.120703 ## + Fold10.Rep3: alpha=0.5759, lambda=1.247636 ## - Fold10.Rep3: alpha=0.5759, lambda=1.247636 ## + Fold10.Rep3: alpha=0.2560, lambda=0.003753 ## - Fold10.Rep3: alpha=0.2560, lambda=0.003753 ## + Fold10.Rep3: alpha=0.7879, lambda=0.195914 ## - Fold10.Rep3: alpha=0.7879, lambda=0.195914 ## + Fold10.Rep3: alpha=0.4018, lambda=0.086926 ## - Fold10.Rep3: alpha=0.4018, lambda=0.086926 ## + Fold10.Rep3: alpha=0.1517, lambda=1.159105 ## - Fold10.Rep3: alpha=0.1517, lambda=1.159105 ## + Fold10.Rep3: alpha=0.3660, lambda=0.692297 ## - Fold10.Rep3: alpha=0.3660, lambda=0.692297 ## + Fold10.Rep3: alpha=0.2080, lambda=0.003654 ## - Fold10.Rep3: alpha=0.2080, lambda=0.003654 ## + Fold01.Rep4: alpha=0.4004, lambda=0.002439 ## - Fold01.Rep4: alpha=0.4004, lambda=0.002439 ## + Fold01.Rep4: alpha=0.1857, lambda=1.109010 ## - Fold01.Rep4: alpha=0.1857, lambda=1.109010 ## + Fold01.Rep4: alpha=0.9054, lambda=0.120703 ## - Fold01.Rep4: alpha=0.9054, lambda=0.120703 ## + Fold01.Rep4: alpha=0.5759, lambda=1.247636 ## - Fold01.Rep4: alpha=0.5759, lambda=1.247636 ## + Fold01.Rep4: alpha=0.2560, lambda=0.003753 ## - Fold01.Rep4: alpha=0.2560, lambda=0.003753 ## + Fold01.Rep4: alpha=0.7879, lambda=0.195914 ## - Fold01.Rep4: alpha=0.7879, lambda=0.195914 ## + Fold01.Rep4: alpha=0.4018, lambda=0.086926 ## - Fold01.Rep4: alpha=0.4018, lambda=0.086926 ## + Fold01.Rep4: alpha=0.1517, lambda=1.159105 ## - Fold01.Rep4: alpha=0.1517, lambda=1.159105 ## + Fold01.Rep4: alpha=0.3660, lambda=0.692297 ## - Fold01.Rep4: alpha=0.3660, lambda=0.692297 ## + Fold01.Rep4: alpha=0.2080, lambda=0.003654 ## - Fold01.Rep4: alpha=0.2080, lambda=0.003654 ## + Fold02.Rep4: alpha=0.4004, lambda=0.002439 ## - Fold02.Rep4: alpha=0.4004, lambda=0.002439 ## + Fold02.Rep4: alpha=0.1857, lambda=1.109010 ## - Fold02.Rep4: alpha=0.1857, lambda=1.109010 ## + Fold02.Rep4: alpha=0.9054, lambda=0.120703 ## - Fold02.Rep4: alpha=0.9054, lambda=0.120703 ## + Fold02.Rep4: alpha=0.5759, lambda=1.247636 ## - Fold02.Rep4: alpha=0.5759, lambda=1.247636 ## + Fold02.Rep4: alpha=0.2560, lambda=0.003753 ## - Fold02.Rep4: alpha=0.2560, lambda=0.003753 ## + Fold02.Rep4: alpha=0.7879, lambda=0.195914 ## - Fold02.Rep4: alpha=0.7879, lambda=0.195914 ## + Fold02.Rep4: alpha=0.4018, lambda=0.086926 ## - Fold02.Rep4: alpha=0.4018, lambda=0.086926 ## + Fold02.Rep4: alpha=0.1517, lambda=1.159105 ## - Fold02.Rep4: alpha=0.1517, lambda=1.159105 ## + Fold02.Rep4: alpha=0.3660, lambda=0.692297 ## - Fold02.Rep4: alpha=0.3660, lambda=0.692297 ## + Fold02.Rep4: alpha=0.2080, lambda=0.003654 ## - Fold02.Rep4: alpha=0.2080, lambda=0.003654 ## + Fold03.Rep4: alpha=0.4004, lambda=0.002439 ## - Fold03.Rep4: alpha=0.4004, lambda=0.002439 ## + Fold03.Rep4: alpha=0.1857, lambda=1.109010 ## - Fold03.Rep4: alpha=0.1857, lambda=1.109010 ## + Fold03.Rep4: alpha=0.9054, lambda=0.120703 ## - Fold03.Rep4: alpha=0.9054, lambda=0.120703 ## + Fold03.Rep4: alpha=0.5759, lambda=1.247636 ## - Fold03.Rep4: alpha=0.5759, lambda=1.247636 ## + Fold03.Rep4: alpha=0.2560, lambda=0.003753 ## - Fold03.Rep4: alpha=0.2560, lambda=0.003753 ## + Fold03.Rep4: alpha=0.7879, lambda=0.195914 ## - Fold03.Rep4: alpha=0.7879, lambda=0.195914 ## + Fold03.Rep4: alpha=0.4018, lambda=0.086926 ## - Fold03.Rep4: alpha=0.4018, lambda=0.086926 ## + Fold03.Rep4: alpha=0.1517, lambda=1.159105 ## - Fold03.Rep4: alpha=0.1517, lambda=1.159105 ## + Fold03.Rep4: alpha=0.3660, lambda=0.692297 ## - Fold03.Rep4: alpha=0.3660, lambda=0.692297 ## + Fold03.Rep4: alpha=0.2080, lambda=0.003654 ## - Fold03.Rep4: alpha=0.2080, lambda=0.003654 ## + Fold04.Rep4: alpha=0.4004, lambda=0.002439 ## - Fold04.Rep4: alpha=0.4004, lambda=0.002439 ## + Fold04.Rep4: alpha=0.1857, lambda=1.109010 ## - Fold04.Rep4: alpha=0.1857, lambda=1.109010 ## + Fold04.Rep4: alpha=0.9054, lambda=0.120703 ## - Fold04.Rep4: alpha=0.9054, lambda=0.120703 ## + Fold04.Rep4: alpha=0.5759, lambda=1.247636 ## - Fold04.Rep4: alpha=0.5759, lambda=1.247636 ## + Fold04.Rep4: alpha=0.2560, lambda=0.003753 ## - Fold04.Rep4: alpha=0.2560, lambda=0.003753 ## + Fold04.Rep4: alpha=0.7879, lambda=0.195914 ## - Fold04.Rep4: alpha=0.7879, lambda=0.195914 ## + Fold04.Rep4: alpha=0.4018, lambda=0.086926 ## - Fold04.Rep4: alpha=0.4018, lambda=0.086926 ## + Fold04.Rep4: alpha=0.1517, lambda=1.159105 ## - Fold04.Rep4: alpha=0.1517, lambda=1.159105 ## + Fold04.Rep4: alpha=0.3660, lambda=0.692297 ## - Fold04.Rep4: alpha=0.3660, lambda=0.692297 ## + Fold04.Rep4: alpha=0.2080, lambda=0.003654 ## - Fold04.Rep4: alpha=0.2080, lambda=0.003654 ## + Fold05.Rep4: alpha=0.4004, lambda=0.002439 ## - Fold05.Rep4: alpha=0.4004, lambda=0.002439 ## + Fold05.Rep4: alpha=0.1857, lambda=1.109010 ## - Fold05.Rep4: alpha=0.1857, lambda=1.109010 ## + Fold05.Rep4: alpha=0.9054, lambda=0.120703 ## - Fold05.Rep4: alpha=0.9054, lambda=0.120703 ## + Fold05.Rep4: alpha=0.5759, lambda=1.247636 ## - Fold05.Rep4: alpha=0.5759, lambda=1.247636 ## + Fold05.Rep4: alpha=0.2560, lambda=0.003753 ## - Fold05.Rep4: alpha=0.2560, lambda=0.003753 ## + Fold05.Rep4: alpha=0.7879, lambda=0.195914 ## - Fold05.Rep4: alpha=0.7879, lambda=0.195914 ## + Fold05.Rep4: alpha=0.4018, lambda=0.086926 ## - Fold05.Rep4: alpha=0.4018, lambda=0.086926 ## + Fold05.Rep4: alpha=0.1517, lambda=1.159105 ## - Fold05.Rep4: alpha=0.1517, lambda=1.159105 ## + Fold05.Rep4: alpha=0.3660, lambda=0.692297 ## - Fold05.Rep4: alpha=0.3660, lambda=0.692297 ## + Fold05.Rep4: alpha=0.2080, lambda=0.003654 ## - Fold05.Rep4: alpha=0.2080, lambda=0.003654 ## + Fold06.Rep4: alpha=0.4004, lambda=0.002439 ## - Fold06.Rep4: alpha=0.4004, lambda=0.002439 ## + Fold06.Rep4: alpha=0.1857, lambda=1.109010 ## - Fold06.Rep4: alpha=0.1857, lambda=1.109010 ## + Fold06.Rep4: alpha=0.9054, lambda=0.120703 ## - Fold06.Rep4: alpha=0.9054, lambda=0.120703 ## + Fold06.Rep4: alpha=0.5759, lambda=1.247636 ## - Fold06.Rep4: alpha=0.5759, lambda=1.247636 ## + Fold06.Rep4: alpha=0.2560, lambda=0.003753 ## - Fold06.Rep4: alpha=0.2560, lambda=0.003753 ## + Fold06.Rep4: alpha=0.7879, lambda=0.195914 ## - Fold06.Rep4: alpha=0.7879, lambda=0.195914 ## + Fold06.Rep4: alpha=0.4018, lambda=0.086926 ## - Fold06.Rep4: alpha=0.4018, lambda=0.086926 ## + Fold06.Rep4: alpha=0.1517, lambda=1.159105 ## - Fold06.Rep4: alpha=0.1517, lambda=1.159105 ## + Fold06.Rep4: alpha=0.3660, lambda=0.692297 ## - Fold06.Rep4: alpha=0.3660, lambda=0.692297 ## + Fold06.Rep4: alpha=0.2080, lambda=0.003654 ## - Fold06.Rep4: alpha=0.2080, lambda=0.003654 ## + Fold07.Rep4: alpha=0.4004, lambda=0.002439 ## - Fold07.Rep4: alpha=0.4004, lambda=0.002439 ## + Fold07.Rep4: alpha=0.1857, lambda=1.109010 ## - Fold07.Rep4: alpha=0.1857, lambda=1.109010 ## + Fold07.Rep4: alpha=0.9054, lambda=0.120703 ## - Fold07.Rep4: alpha=0.9054, lambda=0.120703 ## + Fold07.Rep4: alpha=0.5759, lambda=1.247636 ## - Fold07.Rep4: alpha=0.5759, lambda=1.247636 ## + Fold07.Rep4: alpha=0.2560, lambda=0.003753 ## - Fold07.Rep4: alpha=0.2560, lambda=0.003753 ## + Fold07.Rep4: alpha=0.7879, lambda=0.195914 ## - Fold07.Rep4: alpha=0.7879, lambda=0.195914 ## + Fold07.Rep4: alpha=0.4018, lambda=0.086926 ## - Fold07.Rep4: alpha=0.4018, lambda=0.086926 ## + Fold07.Rep4: alpha=0.1517, lambda=1.159105 ## - Fold07.Rep4: alpha=0.1517, lambda=1.159105 ## + Fold07.Rep4: alpha=0.3660, lambda=0.692297 ## - Fold07.Rep4: alpha=0.3660, lambda=0.692297 ## + Fold07.Rep4: alpha=0.2080, lambda=0.003654 ## - Fold07.Rep4: alpha=0.2080, lambda=0.003654 ## + Fold08.Rep4: alpha=0.4004, lambda=0.002439 ## - Fold08.Rep4: alpha=0.4004, lambda=0.002439 ## + Fold08.Rep4: alpha=0.1857, lambda=1.109010 ## - Fold08.Rep4: alpha=0.1857, lambda=1.109010 ## + Fold08.Rep4: alpha=0.9054, lambda=0.120703 ## - Fold08.Rep4: alpha=0.9054, lambda=0.120703 ## + Fold08.Rep4: alpha=0.5759, lambda=1.247636 ## - Fold08.Rep4: alpha=0.5759, lambda=1.247636 ## + Fold08.Rep4: alpha=0.2560, lambda=0.003753 ## - Fold08.Rep4: alpha=0.2560, lambda=0.003753 ## + Fold08.Rep4: alpha=0.7879, lambda=0.195914 ## - Fold08.Rep4: alpha=0.7879, lambda=0.195914 ## + Fold08.Rep4: alpha=0.4018, lambda=0.086926 ## - Fold08.Rep4: alpha=0.4018, lambda=0.086926 ## + Fold08.Rep4: alpha=0.1517, lambda=1.159105 ## - Fold08.Rep4: alpha=0.1517, lambda=1.159105 ## + Fold08.Rep4: alpha=0.3660, lambda=0.692297 ## - Fold08.Rep4: alpha=0.3660, lambda=0.692297 ## + Fold08.Rep4: alpha=0.2080, lambda=0.003654 ## - Fold08.Rep4: alpha=0.2080, lambda=0.003654 ## + Fold09.Rep4: alpha=0.4004, lambda=0.002439 ## - Fold09.Rep4: alpha=0.4004, lambda=0.002439 ## + Fold09.Rep4: alpha=0.1857, lambda=1.109010 ## - Fold09.Rep4: alpha=0.1857, lambda=1.109010 ## + Fold09.Rep4: alpha=0.9054, lambda=0.120703 ## - Fold09.Rep4: alpha=0.9054, lambda=0.120703 ## + Fold09.Rep4: alpha=0.5759, lambda=1.247636 ## - Fold09.Rep4: alpha=0.5759, lambda=1.247636 ## + Fold09.Rep4: alpha=0.2560, lambda=0.003753 ## - Fold09.Rep4: alpha=0.2560, lambda=0.003753 ## + Fold09.Rep4: alpha=0.7879, lambda=0.195914 ## - Fold09.Rep4: alpha=0.7879, lambda=0.195914 ## + Fold09.Rep4: alpha=0.4018, lambda=0.086926 ## - Fold09.Rep4: alpha=0.4018, lambda=0.086926 ## + Fold09.Rep4: alpha=0.1517, lambda=1.159105 ## - Fold09.Rep4: alpha=0.1517, lambda=1.159105 ## + Fold09.Rep4: alpha=0.3660, lambda=0.692297 ## - Fold09.Rep4: alpha=0.3660, lambda=0.692297 ## + Fold09.Rep4: alpha=0.2080, lambda=0.003654 ## - Fold09.Rep4: alpha=0.2080, lambda=0.003654 ## + Fold10.Rep4: alpha=0.4004, lambda=0.002439 ## - Fold10.Rep4: alpha=0.4004, lambda=0.002439 ## + Fold10.Rep4: alpha=0.1857, lambda=1.109010 ## - Fold10.Rep4: alpha=0.1857, lambda=1.109010 ## + Fold10.Rep4: alpha=0.9054, lambda=0.120703 ## - Fold10.Rep4: alpha=0.9054, lambda=0.120703 ## + Fold10.Rep4: alpha=0.5759, lambda=1.247636 ## - Fold10.Rep4: alpha=0.5759, lambda=1.247636 ## + Fold10.Rep4: alpha=0.2560, lambda=0.003753 ## - Fold10.Rep4: alpha=0.2560, lambda=0.003753 ## + Fold10.Rep4: alpha=0.7879, lambda=0.195914 ## - Fold10.Rep4: alpha=0.7879, lambda=0.195914 ## + Fold10.Rep4: alpha=0.4018, lambda=0.086926 ## - Fold10.Rep4: alpha=0.4018, lambda=0.086926 ## + Fold10.Rep4: alpha=0.1517, lambda=1.159105 ## - Fold10.Rep4: alpha=0.1517, lambda=1.159105 ## + Fold10.Rep4: alpha=0.3660, lambda=0.692297 ## - Fold10.Rep4: alpha=0.3660, lambda=0.692297 ## + Fold10.Rep4: alpha=0.2080, lambda=0.003654 ## - Fold10.Rep4: alpha=0.2080, lambda=0.003654 ## + Fold01.Rep5: alpha=0.4004, lambda=0.002439 ## - Fold01.Rep5: alpha=0.4004, lambda=0.002439 ## + Fold01.Rep5: alpha=0.1857, lambda=1.109010 ## - Fold01.Rep5: alpha=0.1857, lambda=1.109010 ## + Fold01.Rep5: alpha=0.9054, lambda=0.120703 ## - Fold01.Rep5: alpha=0.9054, lambda=0.120703 ## + Fold01.Rep5: alpha=0.5759, lambda=1.247636 ## - Fold01.Rep5: alpha=0.5759, lambda=1.247636 ## + Fold01.Rep5: alpha=0.2560, lambda=0.003753 ## - Fold01.Rep5: alpha=0.2560, lambda=0.003753 ## + Fold01.Rep5: alpha=0.7879, lambda=0.195914 ## - Fold01.Rep5: alpha=0.7879, lambda=0.195914 ## + Fold01.Rep5: alpha=0.4018, lambda=0.086926 ## - Fold01.Rep5: alpha=0.4018, lambda=0.086926 ## + Fold01.Rep5: alpha=0.1517, lambda=1.159105 ## - Fold01.Rep5: alpha=0.1517, lambda=1.159105 ## + Fold01.Rep5: alpha=0.3660, lambda=0.692297 ## - Fold01.Rep5: alpha=0.3660, lambda=0.692297 ## + Fold01.Rep5: alpha=0.2080, lambda=0.003654 ## - Fold01.Rep5: alpha=0.2080, lambda=0.003654 ## + Fold02.Rep5: alpha=0.4004, lambda=0.002439 ## - Fold02.Rep5: alpha=0.4004, lambda=0.002439 ## + Fold02.Rep5: alpha=0.1857, lambda=1.109010 ## - Fold02.Rep5: alpha=0.1857, lambda=1.109010 ## + Fold02.Rep5: alpha=0.9054, lambda=0.120703 ## - Fold02.Rep5: alpha=0.9054, lambda=0.120703 ## + Fold02.Rep5: alpha=0.5759, lambda=1.247636 ## - Fold02.Rep5: alpha=0.5759, lambda=1.247636 ## + Fold02.Rep5: alpha=0.2560, lambda=0.003753 ## - Fold02.Rep5: alpha=0.2560, lambda=0.003753 ## + Fold02.Rep5: alpha=0.7879, lambda=0.195914 ## - Fold02.Rep5: alpha=0.7879, lambda=0.195914 ## + Fold02.Rep5: alpha=0.4018, lambda=0.086926 ## - Fold02.Rep5: alpha=0.4018, lambda=0.086926 ## + Fold02.Rep5: alpha=0.1517, lambda=1.159105 ## - Fold02.Rep5: alpha=0.1517, lambda=1.159105 ## + Fold02.Rep5: alpha=0.3660, lambda=0.692297 ## - Fold02.Rep5: alpha=0.3660, lambda=0.692297 ## + Fold02.Rep5: alpha=0.2080, lambda=0.003654 ## - Fold02.Rep5: alpha=0.2080, lambda=0.003654 ## + Fold03.Rep5: alpha=0.4004, lambda=0.002439 ## - Fold03.Rep5: alpha=0.4004, lambda=0.002439 ## + Fold03.Rep5: alpha=0.1857, lambda=1.109010 ## - Fold03.Rep5: alpha=0.1857, lambda=1.109010 ## + Fold03.Rep5: alpha=0.9054, lambda=0.120703 ## - Fold03.Rep5: alpha=0.9054, lambda=0.120703 ## + Fold03.Rep5: alpha=0.5759, lambda=1.247636 ## - Fold03.Rep5: alpha=0.5759, lambda=1.247636 ## + Fold03.Rep5: alpha=0.2560, lambda=0.003753 ## - Fold03.Rep5: alpha=0.2560, lambda=0.003753 ## + Fold03.Rep5: alpha=0.7879, lambda=0.195914 ## - Fold03.Rep5: alpha=0.7879, lambda=0.195914 ## + Fold03.Rep5: alpha=0.4018, lambda=0.086926 ## - Fold03.Rep5: alpha=0.4018, lambda=0.086926 ## + Fold03.Rep5: alpha=0.1517, lambda=1.159105 ## - Fold03.Rep5: alpha=0.1517, lambda=1.159105 ## + Fold03.Rep5: alpha=0.3660, lambda=0.692297 ## - Fold03.Rep5: alpha=0.3660, lambda=0.692297 ## + Fold03.Rep5: alpha=0.2080, lambda=0.003654 ## - Fold03.Rep5: alpha=0.2080, lambda=0.003654 ## + Fold04.Rep5: alpha=0.4004, lambda=0.002439 ## - Fold04.Rep5: alpha=0.4004, lambda=0.002439 ## + Fold04.Rep5: alpha=0.1857, lambda=1.109010 ## - Fold04.Rep5: alpha=0.1857, lambda=1.109010 ## + Fold04.Rep5: alpha=0.9054, lambda=0.120703 ## - Fold04.Rep5: alpha=0.9054, lambda=0.120703 ## + Fold04.Rep5: alpha=0.5759, lambda=1.247636 ## - Fold04.Rep5: alpha=0.5759, lambda=1.247636 ## + Fold04.Rep5: alpha=0.2560, lambda=0.003753 ## - Fold04.Rep5: alpha=0.2560, lambda=0.003753 ## + Fold04.Rep5: alpha=0.7879, lambda=0.195914 ## - Fold04.Rep5: alpha=0.7879, lambda=0.195914 ## + Fold04.Rep5: alpha=0.4018, lambda=0.086926 ## - Fold04.Rep5: alpha=0.4018, lambda=0.086926 ## + Fold04.Rep5: alpha=0.1517, lambda=1.159105 ## - Fold04.Rep5: alpha=0.1517, lambda=1.159105 ## + Fold04.Rep5: alpha=0.3660, lambda=0.692297 ## - Fold04.Rep5: alpha=0.3660, lambda=0.692297 ## + Fold04.Rep5: alpha=0.2080, lambda=0.003654 ## - Fold04.Rep5: alpha=0.2080, lambda=0.003654 ## + Fold05.Rep5: alpha=0.4004, lambda=0.002439 ## - Fold05.Rep5: alpha=0.4004, lambda=0.002439 ## + Fold05.Rep5: alpha=0.1857, lambda=1.109010 ## - Fold05.Rep5: alpha=0.1857, lambda=1.109010 ## + Fold05.Rep5: alpha=0.9054, lambda=0.120703 ## - Fold05.Rep5: alpha=0.9054, lambda=0.120703 ## + Fold05.Rep5: alpha=0.5759, lambda=1.247636 ## - Fold05.Rep5: alpha=0.5759, lambda=1.247636 ## + Fold05.Rep5: alpha=0.2560, lambda=0.003753 ## - Fold05.Rep5: alpha=0.2560, lambda=0.003753 ## + Fold05.Rep5: alpha=0.7879, lambda=0.195914 ## - Fold05.Rep5: alpha=0.7879, lambda=0.195914 ## + Fold05.Rep5: alpha=0.4018, lambda=0.086926 ## - Fold05.Rep5: alpha=0.4018, lambda=0.086926 ## + Fold05.Rep5: alpha=0.1517, lambda=1.159105 ## - Fold05.Rep5: alpha=0.1517, lambda=1.159105 ## + Fold05.Rep5: alpha=0.3660, lambda=0.692297 ## - Fold05.Rep5: alpha=0.3660, lambda=0.692297 ## + Fold05.Rep5: alpha=0.2080, lambda=0.003654 ## - Fold05.Rep5: alpha=0.2080, lambda=0.003654 ## + Fold06.Rep5: alpha=0.4004, lambda=0.002439 ## - Fold06.Rep5: alpha=0.4004, lambda=0.002439 ## + Fold06.Rep5: alpha=0.1857, lambda=1.109010 ## - Fold06.Rep5: alpha=0.1857, lambda=1.109010 ## + Fold06.Rep5: alpha=0.9054, lambda=0.120703 ## - Fold06.Rep5: alpha=0.9054, lambda=0.120703 ## + Fold06.Rep5: alpha=0.5759, lambda=1.247636 ## - Fold06.Rep5: alpha=0.5759, lambda=1.247636 ## + Fold06.Rep5: alpha=0.2560, lambda=0.003753 ## - Fold06.Rep5: alpha=0.2560, lambda=0.003753 ## + Fold06.Rep5: alpha=0.7879, lambda=0.195914 ## - Fold06.Rep5: alpha=0.7879, lambda=0.195914 ## + Fold06.Rep5: alpha=0.4018, lambda=0.086926 ## - Fold06.Rep5: alpha=0.4018, lambda=0.086926 ## + Fold06.Rep5: alpha=0.1517, lambda=1.159105 ## - Fold06.Rep5: alpha=0.1517, lambda=1.159105 ## + Fold06.Rep5: alpha=0.3660, lambda=0.692297 ## - Fold06.Rep5: alpha=0.3660, lambda=0.692297 ## + Fold06.Rep5: alpha=0.2080, lambda=0.003654 ## - Fold06.Rep5: alpha=0.2080, lambda=0.003654 ## + Fold07.Rep5: alpha=0.4004, lambda=0.002439 ## - Fold07.Rep5: alpha=0.4004, lambda=0.002439 ## + Fold07.Rep5: alpha=0.1857, lambda=1.109010 ## - Fold07.Rep5: alpha=0.1857, lambda=1.109010 ## + Fold07.Rep5: alpha=0.9054, lambda=0.120703 ## - Fold07.Rep5: alpha=0.9054, lambda=0.120703 ## + Fold07.Rep5: alpha=0.5759, lambda=1.247636 ## - Fold07.Rep5: alpha=0.5759, lambda=1.247636 ## + Fold07.Rep5: alpha=0.2560, lambda=0.003753 ## - Fold07.Rep5: alpha=0.2560, lambda=0.003753 ## + Fold07.Rep5: alpha=0.7879, lambda=0.195914 ## - Fold07.Rep5: alpha=0.7879, lambda=0.195914 ## + Fold07.Rep5: alpha=0.4018, lambda=0.086926 ## - Fold07.Rep5: alpha=0.4018, lambda=0.086926 ## + Fold07.Rep5: alpha=0.1517, lambda=1.159105 ## - Fold07.Rep5: alpha=0.1517, lambda=1.159105 ## + Fold07.Rep5: alpha=0.3660, lambda=0.692297 ## - Fold07.Rep5: alpha=0.3660, lambda=0.692297 ## + Fold07.Rep5: alpha=0.2080, lambda=0.003654 ## - Fold07.Rep5: alpha=0.2080, lambda=0.003654 ## + Fold08.Rep5: alpha=0.4004, lambda=0.002439 ## - Fold08.Rep5: alpha=0.4004, lambda=0.002439 ## + Fold08.Rep5: alpha=0.1857, lambda=1.109010 ## - Fold08.Rep5: alpha=0.1857, lambda=1.109010 ## + Fold08.Rep5: alpha=0.9054, lambda=0.120703 ## - Fold08.Rep5: alpha=0.9054, lambda=0.120703 ## + Fold08.Rep5: alpha=0.5759, lambda=1.247636 ## - Fold08.Rep5: alpha=0.5759, lambda=1.247636 ## + Fold08.Rep5: alpha=0.2560, lambda=0.003753 ## - Fold08.Rep5: alpha=0.2560, lambda=0.003753 ## + Fold08.Rep5: alpha=0.7879, lambda=0.195914 ## - Fold08.Rep5: alpha=0.7879, lambda=0.195914 ## + Fold08.Rep5: alpha=0.4018, lambda=0.086926 ## - Fold08.Rep5: alpha=0.4018, lambda=0.086926 ## + Fold08.Rep5: alpha=0.1517, lambda=1.159105 ## - Fold08.Rep5: alpha=0.1517, lambda=1.159105 ## + Fold08.Rep5: alpha=0.3660, lambda=0.692297 ## - Fold08.Rep5: alpha=0.3660, lambda=0.692297 ## + Fold08.Rep5: alpha=0.2080, lambda=0.003654 ## - Fold08.Rep5: alpha=0.2080, lambda=0.003654 ## + Fold09.Rep5: alpha=0.4004, lambda=0.002439 ## - Fold09.Rep5: alpha=0.4004, lambda=0.002439 ## + Fold09.Rep5: alpha=0.1857, lambda=1.109010 ## - Fold09.Rep5: alpha=0.1857, lambda=1.109010 ## + Fold09.Rep5: alpha=0.9054, lambda=0.120703 ## - Fold09.Rep5: alpha=0.9054, lambda=0.120703 ## + Fold09.Rep5: alpha=0.5759, lambda=1.247636 ## - Fold09.Rep5: alpha=0.5759, lambda=1.247636 ## + Fold09.Rep5: alpha=0.2560, lambda=0.003753 ## - Fold09.Rep5: alpha=0.2560, lambda=0.003753 ## + Fold09.Rep5: alpha=0.7879, lambda=0.195914 ## - Fold09.Rep5: alpha=0.7879, lambda=0.195914 ## + Fold09.Rep5: alpha=0.4018, lambda=0.086926 ## - Fold09.Rep5: alpha=0.4018, lambda=0.086926 ## + Fold09.Rep5: alpha=0.1517, lambda=1.159105 ## - Fold09.Rep5: alpha=0.1517, lambda=1.159105 ## + Fold09.Rep5: alpha=0.3660, lambda=0.692297 ## - Fold09.Rep5: alpha=0.3660, lambda=0.692297 ## + Fold09.Rep5: alpha=0.2080, lambda=0.003654 ## - Fold09.Rep5: alpha=0.2080, lambda=0.003654 ## + Fold10.Rep5: alpha=0.4004, lambda=0.002439 ## - Fold10.Rep5: alpha=0.4004, lambda=0.002439 ## + Fold10.Rep5: alpha=0.1857, lambda=1.109010 ## - Fold10.Rep5: alpha=0.1857, lambda=1.109010 ## + Fold10.Rep5: alpha=0.9054, lambda=0.120703 ## - Fold10.Rep5: alpha=0.9054, lambda=0.120703 ## + Fold10.Rep5: alpha=0.5759, lambda=1.247636 ## - Fold10.Rep5: alpha=0.5759, lambda=1.247636 ## + Fold10.Rep5: alpha=0.2560, lambda=0.003753 ## - Fold10.Rep5: alpha=0.2560, lambda=0.003753 ## + Fold10.Rep5: alpha=0.7879, lambda=0.195914 ## - Fold10.Rep5: alpha=0.7879, lambda=0.195914 ## + Fold10.Rep5: alpha=0.4018, lambda=0.086926 ## - Fold10.Rep5: alpha=0.4018, lambda=0.086926 ## + Fold10.Rep5: alpha=0.1517, lambda=1.159105 ## - Fold10.Rep5: alpha=0.1517, lambda=1.159105 ## + Fold10.Rep5: alpha=0.3660, lambda=0.692297 ## - Fold10.Rep5: alpha=0.3660, lambda=0.692297 ## + Fold10.Rep5: alpha=0.2080, lambda=0.003654 ## - Fold10.Rep5: alpha=0.2080, lambda=0.003654 ## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : There were missing values in resampled performance measures. ## Aggregating results ## Selecting tuning parameters ## Fitting alpha = 0.208, lambda = 0.00365 on full training set # Best tuning parameter elastic_reg$bestTune ## alpha lambda ## 3 0.2079599 0.003654144 # Make predictions on training set predictions_train &lt;- predict(elastic_reg, x) eval_results(y_train, predictions_train, train) ## RMSE Rsquare ## 1 0.7897507 0.375736 # Make predictions on test set predictions_test &lt;- predict(elastic_reg, x_test) eval_results(y_test, predictions_test, test) ## Warning in mean.default(true): argument is not numeric or logical: returning NA ## RMSE Rsquare ## 1 0 NaN "],["references.html", "Chapter 7 References", " Chapter 7 References P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physiochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009. Puckette, Madeline. What Is Wine Exactly? Wine Folly, 5 Oct. 2015, winefolly.com/deep-dive/what-is-wine. Descriptive Statistics. Wikipedia, en.wikipedia.org/wiki/Descriptive_statistics. Accessed 29 May 2021. "]]
