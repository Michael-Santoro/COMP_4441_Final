[["introduction.html", "COMP 4441 Final Project Chapter 1 Introduction", " COMP 4441 Final Project Emma Bright and Michael Santoro 2021-05-30 Chapter 1 Introduction Purpose Wine is an alcoholic beverage made from fermented grape juice. (Puckette) Generally, a wines quality is determined by taste, smell, and visual tests performed by wine experts, sommeliers. These tests grade the wine on subjective measures: Acidity, Sweetness, Alcohol, Tannin and Aroma. Though these tests are subjective, there is overlap between the observation of these characteristics and a wines physiochemical properties. For example, a wines perceived acidity or tartness is based upon the pH of the wine and a wines perceived sweetness is based upon the residual sugar of the wine. The goal of our research is to determine: Can we predict the subjective quality rating of wine based solely on its physiochemical properties? An accurate predictive model for determining an objective quality of wine would enable small wineries and vineyards, especially those from less established wine regions, to more accurately price their inventory and ensure a better return on investment. Statistical or Analytical Method We will be using the the following statistical methods to create predictive models for quality of wine: LASSO Regression, Random Forest, and K-Nearest Neighbor. "],["descriptive-statistics.html", "Chapter 2 Descriptive Statistics 2.1 Dataset 2.2 Exploratory Analysis", " Chapter 2 Descriptive Statistics 2.1 Dataset The dataset utilized for our analysis was collected from the UCI Machine Learning Repository (see References). The dataset consists of 12 elements, 11 physiochemical properties and 1 subjective quality measurement, for both red Vinho Verde wine samples from Portugal. Read in Data redWine&lt;-read.table(&quot;data/winequality-red.csv&quot;,stringsAsFactors = FALSE, sep=&quot;,&quot;,header = TRUE) whiteWine&lt;-read.table(&quot;data/winequality-white.csv&quot;,stringsAsFactors = FALSE, sep=&quot;;&quot;,header = TRUE) redWine$type=&#39;red&#39; whiteWine$type=&#39;white&#39; # create a field that shows whether a wine is red or white based on initial # datasets wine &lt;- rbind(redWine, whiteWine) str(wine) ## &#39;data.frame&#39;: 6497 obs. of 13 variables: ## $ fixed.acidity : num 7.4 7.8 7.8 11.2 7.4 7.4 7.9 7.3 7.8 7.5 ... ## $ volatile.acidity : num 0.7 0.88 0.76 0.28 0.7 0.66 0.6 0.65 0.58 0.5 ... ## $ citric.acid : num 0 0 0.04 0.56 0 0 0.06 0 0.02 0.36 ... ## $ residual.sugar : num 1.9 2.6 2.3 1.9 1.9 1.8 1.6 1.2 2 6.1 ... ## $ chlorides : num 0.076 0.098 0.092 0.075 0.076 0.075 0.069 0.065 0.073 0.071 ... ## $ free.sulfur.dioxide : num 11 25 15 17 11 13 15 15 9 17 ... ## $ total.sulfur.dioxide: num 34 67 54 60 34 40 59 21 18 102 ... ## $ density : num 0.998 0.997 0.997 0.998 0.998 ... ## $ pH : num 3.51 3.2 3.26 3.16 3.51 3.51 3.3 3.39 3.36 3.35 ... ## $ sulphates : num 0.56 0.68 0.65 0.58 0.56 0.56 0.46 0.47 0.57 0.8 ... ## $ alcohol : num 9.4 9.8 9.8 9.8 9.4 9.4 9.4 10 9.5 10.5 ... ## $ quality : int 5 5 5 6 5 5 5 7 7 5 ... ## $ type : chr &quot;red&quot; &quot;red&quot; &quot;red&quot; &quot;red&quot; ... 2.2 Exploratory Analysis 2.2.1 Descriptive Statistics In order to better understand our dataset we performed some initial descriptive statistics. A descriptive statistic is a summary level statistic, such as mean or median, that describes a variable or feature of a dataset. Descriptive statistics is the processing of analyzing the descriptive statistic taken from your dataset. (Descriptive Statistics) The table below shows descriptive statistic measurements for each of the variables in our dataset. stat.desc(wine) ## fixed.acidity volatile.acidity citric.acid residual.sugar chlorides free.sulfur.dioxide total.sulfur.dioxide density ## nbr.val 6.497000e+03 6.497000e+03 6.497000e+03 6.497000e+03 6.497000e+03 6.497000e+03 6.497000e+03 6.497000e+03 ## nbr.null 0.000000e+00 0.000000e+00 1.510000e+02 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 ## nbr.na 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 ## min 3.800000e+00 8.000000e-02 0.000000e+00 6.000000e-01 9.000000e-03 1.000000e+00 6.000000e+00 9.871100e-01 ## max 1.590000e+01 1.580000e+00 1.660000e+00 6.580000e+01 6.110000e-01 2.890000e+02 4.400000e+02 1.038980e+00 ## range 1.210000e+01 1.500000e+00 1.660000e+00 6.520000e+01 6.020000e-01 2.880000e+02 4.340000e+02 5.187000e-02 ## sum 4.687785e+04 2.206810e+03 2.070160e+03 3.536470e+04 3.640520e+02 1.983230e+05 7.519925e+05 6.462544e+03 ## median 7.000000e+00 2.900000e-01 3.100000e-01 3.000000e+00 4.700000e-02 2.900000e+01 1.180000e+02 9.948900e-01 ## mean 7.215307e+00 3.396660e-01 3.186332e-01 5.443235e+00 5.603386e-02 3.052532e+01 1.157446e+02 9.946966e-01 ## SE.mean 1.608399e-02 2.042536e-03 1.802862e-03 5.902692e-02 4.346387e-04 2.202050e-01 7.012292e-01 3.720255e-05 ## CI.mean.0.95 3.152992e-02 4.004042e-03 3.534204e-03 1.157122e-01 8.520349e-04 4.316744e-01 1.374640e+00 7.292924e-05 ## var 1.680740e+00 2.710517e-02 2.111728e-02 2.263670e+01 1.227353e-03 3.150412e+02 3.194720e+03 8.992040e-06 ## std.dev 1.296434e+00 1.646365e-01 1.453179e-01 4.757804e+00 3.503360e-02 1.774940e+01 5.652185e+01 2.998673e-03 ## coef.var 1.796783e-01 4.847011e-01 4.560663e-01 8.740764e-01 6.252220e-01 5.814648e-01 4.883326e-01 3.014661e-03 ## pH sulphates alcohol quality type ## nbr.val 6.497000e+03 6.497000e+03 6.497000e+03 6.497000e+03 NA ## nbr.null 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 NA ## nbr.na 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 NA ## min 2.720000e+00 2.200000e-01 8.000000e+00 3.000000e+00 NA ## max 4.010000e+00 2.000000e+00 1.490000e+01 9.000000e+00 NA ## range 1.290000e+00 1.780000e+00 6.900000e+00 6.000000e+00 NA ## sum 2.091060e+04 3.451650e+03 6.816523e+04 3.780200e+04 NA ## median 3.210000e+00 5.100000e-01 1.030000e+01 6.000000e+00 NA ## mean 3.218501e+00 5.312683e-01 1.049180e+01 5.818378e+00 NA ## SE.mean 1.994780e-03 1.846136e-03 1.479718e-02 1.083390e-02 NA ## CI.mean.0.95 3.910426e-03 3.619034e-03 2.900735e-02 2.123801e-02 NA ## var 2.585252e-02 2.214319e-02 1.422561e+00 7.625748e-01 NA ## std.dev 1.607872e-01 1.488059e-01 1.192712e+00 8.732553e-01 NA ## coef.var 4.995717e-02 2.800955e-01 1.136804e-01 1.500857e-01 NA Univariate Exploration We can use univariate exploration to explore the distribution of a single variable or desriptive statistic. The variables for the three plots below were selected judgmentally from the dataset. mu &lt;- ddply(wine, &quot;type&quot;, summarise, grp.mean=mean(alcohol)) p&lt;-ggplot(wine, aes(x=alcohol, fill=type, color=type)) + geom_histogram(position=&quot;dodge&quot;)+ theme(legend.position=&quot;top&quot;) + geom_vline(data=mu, aes(xintercept=grp.mean, color=type), linetype=&quot;dashed&quot;)+ theme(legend.position=&quot;top&quot;) p ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. mu &lt;- ddply(wine, &quot;type&quot;, summarise, grp.mean=mean(volatile.acidity)) p&lt;-ggplot(wine, aes(x=volatile.acidity, fill=type, color=type)) + geom_histogram(position=&quot;dodge&quot;)+ theme(legend.position=&quot;top&quot;) + geom_vline(data=mu, aes(xintercept=grp.mean, color=type), linetype=&quot;dashed&quot;)+ theme(legend.position=&quot;top&quot;) p ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. mu &lt;- ddply(wine, &quot;type&quot;, summarise, grp.mean=mean(density)) p&lt;-ggplot(wine, aes(x=density, fill=type, color=type)) + geom_histogram(position=&quot;dodge&quot;)+ theme(legend.position=&quot;top&quot;) + geom_vline(data=mu, aes(xintercept=grp.mean, color=type), linetype=&quot;dashed&quot;)+ theme(legend.position=&quot;top&quot;) p ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Bivariate Exploration Bivariate exploration allows us to explore the relationship between two descriptive statistics in a dataset. To better determine which variable relationships we should explore, we ran correlation analysis. # ++++++++++++++++++++++++++++ # flattenCorrMatrix # ++++++++++++++++++++++++++++ # cormat : matrix of the correlation coefficients # pmat : matrix of the correlation p-values flattenCorrMatrix &lt;- function(cormat, pmat) { ut &lt;- upper.tri(cormat) data.frame( row = rownames(cormat)[row(cormat)[ut]], column = rownames(cormat)[col(cormat)[ut]], cor =(cormat)[ut], p = pmat[ut] ) } # exclude the type of wine from the correlation variables res2 &lt;- rcorr(as.matrix(wine[,1:12])) cor.m &lt;- flattenCorrMatrix(res2$r, res2$P) cor.m[order(cor.m[,3],decreasing=TRUE),] ## row column cor p ## 21 free.sulfur.dioxide total.sulfur.dioxide 0.720934081 0.000000e+00 ## 25 residual.sugar density 0.552516950 0.000000e+00 ## 19 residual.sugar total.sulfur.dioxide 0.495481587 0.000000e+00 ## 22 fixed.acidity density 0.458909982 0.000000e+00 ## 66 alcohol quality 0.444318520 0.000000e+00 ## 14 residual.sugar free.sulfur.dioxide 0.402870640 0.000000e+00 ## 41 chlorides sulphates 0.395593307 0.000000e+00 ## 8 volatile.acidity chlorides 0.377124276 0.000000e+00 ## 26 chlorides density 0.362614656 0.000000e+00 ## 2 fixed.acidity citric.acid 0.324435725 0.000000e+00 ## 37 fixed.acidity sulphates 0.299567744 0.000000e+00 ## 7 fixed.acidity chlorides 0.298194772 0.000000e+00 ## 23 volatile.acidity density 0.271295648 0.000000e+00 ## 30 volatile.acidity pH 0.261454403 0.000000e+00 ## 44 density sulphates 0.259478495 0.000000e+00 ## 38 volatile.acidity sulphates 0.225983680 0.000000e+00 ## 1 fixed.acidity volatile.acidity 0.219008256 0.000000e+00 ## 18 citric.acid total.sulfur.dioxide 0.195241976 0.000000e+00 ## 45 pH sulphates 0.192123407 0.000000e+00 ## 6 citric.acid residual.sugar 0.142451226 0.000000e+00 ## 13 citric.acid free.sulfur.dioxide 0.133125810 0.000000e+00 ## 54 pH alcohol 0.121248467 0.000000e+00 ## 24 citric.acid density 0.096153929 7.993606e-15 ## 58 citric.acid quality 0.085531717 5.001777e-12 ## 39 citric.acid sulphates 0.056197300 5.830741e-06 ## 61 free.sulfur.dioxide quality 0.055463059 7.708445e-06 ## 33 chlorides pH 0.044707980 3.124540e-04 ## 9 citric.acid chlorides 0.038998014 1.666635e-03 ## 65 sulphates quality 0.038485446 1.918079e-03 ## 28 total.sulfur.dioxide density 0.032394512 9.019631e-03 ## 27 free.sulfur.dioxide density 0.025716842 3.818871e-02 ## 64 pH quality 0.019505704 1.159310e-01 ## 36 density pH 0.011686081 3.462974e-01 ## 55 sulphates alcohol -0.003029195 8.071389e-01 ## 48 citric.acid alcohol -0.010493492 3.977326e-01 ## 59 residual.sugar quality -0.036980485 2.871025e-03 ## 47 volatile.acidity alcohol -0.037640386 2.409699e-03 ## 62 total.sulfur.dioxide quality -0.041385454 8.480397e-04 ## 56 fixed.acidity quality -0.076743208 5.874849e-10 ## 46 fixed.acidity alcohol -0.095451523 1.243450e-14 ## 4 fixed.acidity residual.sugar -0.111981281 0.000000e+00 ## 10 residual.sugar chlorides -0.128940500 0.000000e+00 ## 34 free.sulfur.dioxide pH -0.145853896 0.000000e+00 ## 51 free.sulfur.dioxide alcohol -0.179838435 0.000000e+00 ## 40 residual.sugar sulphates -0.185927405 0.000000e+00 ## 42 free.sulfur.dioxide sulphates -0.188457249 0.000000e+00 ## 15 chlorides free.sulfur.dioxide -0.195044785 0.000000e+00 ## 5 volatile.acidity residual.sugar -0.196011174 0.000000e+00 ## 60 chlorides quality -0.200665500 0.000000e+00 ## 35 total.sulfur.dioxide pH -0.238413103 0.000000e+00 ## 29 fixed.acidity pH -0.252700468 0.000000e+00 ## 50 chlorides alcohol -0.256915580 0.000000e+00 ## 57 volatile.acidity quality -0.265699478 0.000000e+00 ## 52 total.sulfur.dioxide alcohol -0.265739639 0.000000e+00 ## 32 residual.sugar pH -0.267319837 0.000000e+00 ## 43 total.sulfur.dioxide sulphates -0.275726820 0.000000e+00 ## 20 chlorides total.sulfur.dioxide -0.279630447 0.000000e+00 ## 11 fixed.acidity free.sulfur.dioxide -0.282735428 0.000000e+00 ## 63 density quality -0.305857906 0.000000e+00 ## 16 fixed.acidity total.sulfur.dioxide -0.329053901 0.000000e+00 ## 31 citric.acid pH -0.329808191 0.000000e+00 ## 12 volatile.acidity free.sulfur.dioxide -0.352557306 0.000000e+00 ## 49 residual.sugar alcohol -0.359414771 0.000000e+00 ## 3 volatile.acidity citric.acid -0.377981317 0.000000e+00 ## 17 volatile.acidity total.sulfur.dioxide -0.414476195 0.000000e+00 ## 53 density alcohol -0.686745421 0.000000e+00 Visualize Correlations # Insignificant correlations are leaved blank corrplot(res2$r, type=&quot;upper&quot;, order=&quot;hclust&quot;, p.mat = res2$P, sig.level = 0.01, insig = &quot;blank&quot;) Based on the correlation matrix and the correlation plots it appears that there is a strong negative correlation between alcohol and density and a strong positive correlation between residual sugar and density. These are plotted below. ggplot(wine, aes(x = alcohol, y = density, color = type)) + geom_point(size = 3, alpha = .05) + labs(title = &quot;Wine Composition by alcohol, density, and type&quot;) ggplot(wine, aes(x = residual.sugar, y = density, color = type)) + geom_point(size = 3, alpha = .05) + labs(title = &quot;Wine Composition by residual.sugar, density, and type&quot;) "],["data-cleaning-and-partitioning.html", "Chapter 3 Data Cleaning and Partitioning 3.1 Data Normalization 3.2 Data Partitioning 3.3 Comparing Results", " Chapter 3 Data Cleaning and Partitioning 3.1 Data Normalization Our dataset already contains only predictive values and output, so we do not need to remove any descriptive columns. We must normalize the values within the dataset to avoid any bias and remove the output variable (quality) since its the prediction. set.seed(123) # do not include wine type in partitioned data. wine &lt;-wine[,1:12] #Normalization normalize &lt;- function(x) { return ((x - min(x)) / (max(x) - min(x))) } #De-Normalization de.normalize &lt;- function(val){ x &lt;- wine[,1:12] return (val*(max(x) - min(x))+min(x)) } #only normalize variables wine.normal &lt;- as.data.frame(lapply(wine[,1:11], normalize)) # add quality back wine.normal$quality = wine$quality head(wine.normal) ## fixed.acidity volatile.acidity citric.acid residual.sugar chlorides free.sulfur.dioxide total.sulfur.dioxide density pH sulphates ## 1 0.2975207 0.4133333 0.00000000 0.01993865 0.1112957 0.03472222 0.06451613 0.2060922 0.6124031 0.1910112 ## 2 0.3305785 0.5333333 0.00000000 0.03067485 0.1478405 0.08333333 0.14055300 0.1868132 0.3720930 0.2584270 ## 3 0.3305785 0.4533333 0.02409639 0.02607362 0.1378738 0.04861111 0.11059908 0.1906690 0.4186047 0.2415730 ## 4 0.6115702 0.1333333 0.33734940 0.01993865 0.1096346 0.05555556 0.12442396 0.2099479 0.3410853 0.2022472 ## 5 0.2975207 0.4133333 0.00000000 0.01993865 0.1112957 0.03472222 0.06451613 0.2060922 0.6124031 0.1910112 ## 6 0.2975207 0.3866667 0.00000000 0.01840491 0.1096346 0.04166667 0.07834101 0.2060922 0.6124031 0.1910112 ## alcohol quality ## 1 0.2028986 5 ## 2 0.2608696 5 ## 3 0.2608696 5 ## 4 0.2608696 6 ## 5 0.2028986 5 ## 6 0.2028986 5 3.2 Data Partitioning In order to determine whether our models are effective we must partition our dataset into a training and test set. ind &lt;- sample(2, nrow(wine.normal), replace=TRUE, prob=c(0.7, 0.3)) train &lt;- wine.normal[ind==1, ] test &lt;- wine.normal[ind==2,] 3.3 Comparing Results We will be comparing accuracy of each of the methods so we will set up an accuracy data frame to compare the effectiveness of each of the methods. accuracy &lt;- data.frame(Accuracy=c(rep(NA,times=3)),row.names = c(&quot;K-Nearest&quot;,&quot;Random Forest&quot;,&quot;Regression&quot;)) accuracy ## Accuracy ## K-Nearest NA ## Random Forest NA ## Regression NA "],["random-forest.html", "Chapter 4 Random Forest 4.1 Create Random Forest Model 4.2 Predicition and Confusion Matrix - Training Data 4.3 Predicition and Confusion Matrix - Test Data 4.4 Tuning our Model 4.5 Recreate our Random Forest Model 4.6 Rerun Predicition and Confusion Matrix - Training Data 4.7 Rerun Predicition and Confusion Matrix - Test Data 4.8 Variable Importance 4.9 Accuracy", " Chapter 4 Random Forest Decision trees can suffer from from the problem of overfitting. In order to help solve this problem, the Random Forest algorithm was devised. It works by building multiple decision trees and then merging them together to form a prediction. Random forest can be used to solve both classification and regression problems, however, for our analysis we will be utilizing it solve our classification problem. \"Each tree within the Random Forest is grown as follows: If the number of cases in the training set is N, sample N cases at random - but with replacement, from the original data. This sample will be the training set for growing the tree. If there are M input variables, a number m&lt;&lt;M is specified such that at each node, m variables are selected at random out of the M and the best split on these m is used to split the node. The value of m is held constant during the forest growing. Each tree is grown to the largest extent possible. There is no pruning.\" (Breiman and Cutler) 4.1 Create Random Forest Model We will start by running the randomForest function utilizing the training dataset to train the model. set.seed(444) rf.train &lt;- train rf.train$quality &lt;-as.factor(rf.train$quality) rf.test &lt;- test rf.test$quality &lt;-as.factor(rf.test$quality) # quality is a function of all other variables rf &lt;- randomForest(quality~., data=rf.train) print(rf) ## ## Call: ## randomForest(formula = quality ~ ., data = rf.train) ## Type of random forest: classification ## Number of trees: 500 ## No. of variables tried at each split: 3 ## ## OOB estimate of error rate: 32.91% ## Confusion matrix: ## 3 4 5 6 7 8 9 class.error ## 3 0 0 11 8 0 0 0 1.0000000 ## 4 0 24 82 43 2 0 0 0.8410596 ## 5 0 4 1086 421 7 0 0 0.2845850 ## 6 0 1 335 1526 112 1 0 0.2273418 ## 7 0 0 21 354 379 6 0 0.5013158 ## 8 0 0 3 52 34 47 0 0.6544118 ## 9 0 0 0 1 4 0 0 1.0000000 The default number of trees was utilized to run our model (500) and the number of variables tried at each split in the tree was 3 variables. The table above shows the confusion matrix for the random forest model created with the training dataset. The out-of-bag (OOB) error is shown to be 32.91%. According to the class.error results, the model was most inaccurate when predicting wines with a quality of 3 or 9 with a 100% error rate. It was most accurate when predicting wines of value 6 with only a 22% error rate. 4.2 Predicition and Confusion Matrix - Training Data The following code runs a prediction of the quality values for the training set using the random forest model created above. As you can see, all 6 predictions in the predicted values vector are 100% accurate. This makes sense as the random forest model was built using the training data, so it has already seen these values. p1 &lt;- predict(rf, rf.train) p1&lt;- droplevels(p1) #drop any unused levels head(p1) # predicted values ## 1 3 6 7 9 10 ## 5 5 5 5 7 5 ## Levels: 3 4 5 6 7 8 9 head(rf.train$quality) # actual values ## [1] 5 5 5 5 7 5 ## Levels: 3 4 5 6 7 8 9 #install.packages(&#39;caret&#39;, dependencies = TRUE) confusionMatrix(p1, rf.train$quality ) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 3 4 5 6 7 8 9 ## 3 19 0 0 0 0 0 0 ## 4 0 151 0 0 0 0 0 ## 5 0 0 1518 0 0 0 0 ## 6 0 0 0 1975 0 0 0 ## 7 0 0 0 0 760 0 0 ## 8 0 0 0 0 0 136 0 ## 9 0 0 0 0 0 0 5 ## ## Overall Statistics ## ## Accuracy : 1 ## 95% CI : (0.9992, 1) ## No Information Rate : 0.4327 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 1 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: 3 Class: 4 Class: 5 Class: 6 Class: 7 Class: 8 Class: 9 ## Sensitivity 1.000000 1.00000 1.0000 1.0000 1.0000 1.0000 1.000000 ## Specificity 1.000000 1.00000 1.0000 1.0000 1.0000 1.0000 1.000000 ## Pos Pred Value 1.000000 1.00000 1.0000 1.0000 1.0000 1.0000 1.000000 ## Neg Pred Value 1.000000 1.00000 1.0000 1.0000 1.0000 1.0000 1.000000 ## Prevalence 0.004163 0.03309 0.3326 0.4327 0.1665 0.0298 0.001096 ## Detection Rate 0.004163 0.03309 0.3326 0.4327 0.1665 0.0298 0.001096 ## Detection Prevalence 0.004163 0.03309 0.3326 0.4327 0.1665 0.0298 0.001096 ## Balanced Accuracy 1.000000 1.00000 1.0000 1.0000 1.0000 1.0000 1.000000 Using the confusion matrix function from the caret library, we can show the accuracy of our model when predicting the quality values for our training dataset. The accuracy is 100% when utilizing the training dataset with the random forest model trained by the training dataset. 4.3 Predicition and Confusion Matrix - Test Data The following code runs a prediction of the quality values for the test dataset using the random forest model created with our training dataset from above. This model was able to predict 4 out of 6 of the first values accurately. p2 &lt;- predict(rf, rf.test) p2&lt;-droplevels(p2) # drop unused levels head(p2) # predicted values ## 2 4 5 8 11 16 ## 5 5 5 5 5 5 ## Levels: 3 4 5 6 7 8 head(rf.test$quality) # actual values ## [1] 5 6 5 7 5 5 ## Levels: 3 4 5 6 7 8 confusionMatrix(p2, rf.test$quality ) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 3 4 5 6 7 8 ## 3 0 1 0 0 0 0 ## 4 0 8 1 1 0 0 ## 5 3 31 448 136 9 0 ## 6 7 24 169 680 128 24 ## 7 1 1 2 44 178 15 ## 8 0 0 0 0 4 18 ## ## Overall Statistics ## ## Accuracy : 0.6891 ## 95% CI : (0.6679, 0.7097) ## No Information Rate : 0.4454 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.512 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: 3 Class: 4 Class: 5 Class: 6 Class: 7 Class: 8 ## Sensitivity 0.0000000 0.123077 0.7226 0.7898 0.55799 0.315789 ## Specificity 0.9994797 0.998929 0.8637 0.6716 0.96097 0.997868 ## Pos Pred Value 0.0000000 0.800000 0.7145 0.6589 0.73859 0.818182 ## Neg Pred Value 0.9943064 0.970359 0.8683 0.7991 0.91667 0.979592 ## Prevalence 0.0056906 0.033626 0.3207 0.4454 0.16503 0.029488 ## Detection Rate 0.0000000 0.004139 0.2318 0.3518 0.09208 0.009312 ## Detection Prevalence 0.0005173 0.005173 0.3244 0.5339 0.12468 0.011381 ## Balanced Accuracy 0.4997399 0.561003 0.7931 0.7307 0.75948 0.656829 Using the confusion matrix function from the caret library, we can show the accuracy of our model in predicting the quality values for our test dataset. The accuracy is roughly 69% when utilizing the test dataset with the random forest model trained by the training dataset. 4.4 Tuning our Model Now, that we have run our initial model, we can focus on fine tuning the random forest parameters in order to create a more accurate predictive model. 4.4.1 Plotting the Error Rate Plotting the error rate to the number of trees can show us where our model has more or less the same level of effectiveness and we can choose a more accurate tree number. plot(rf) The model appears to have a drop off after about 300 trees and then is more or less constant, therefore, we can adjust our tree number in the model to be 300. 4.4.2 Tuning mTry The mTry value reflects the number of variables tested at each node split. The tuneRF function can be used to determine what our mTry value should be. set.seed(2222) t &lt;- tuneRF(rf.train[,-12], rf.train[,12], stepFactor = 0.5, plot=TRUE, ntreeTry = 100, trace=TRUE, improve=0.05) ## mtry = 3 OOB error = 33.26% ## Searching left ... ## mtry = 6 OOB error = 34.36% ## -0.03293808 0.05 ## Searching right ... ## mtry = 1 OOB error = 33.46% ## -0.005928854 0.05 This output indicates that our model hits its lowest error rate when the mTry value is 3, so we can adjust our model to reflect this new value. 4.5 Recreate our Random Forest Model The following code reruns our model utilizing the new tuning parameters found above. set.seed(444) # quality is a function of all other variables rf &lt;- randomForest(quality~., data=rf.train, ntree=300, mTry=3, importance=TRUE, proximity=TRUE) print(rf) ## ## Call: ## randomForest(formula = quality ~ ., data = rf.train, ntree = 300, mTry = 3, importance = TRUE, proximity = TRUE) ## Type of random forest: classification ## Number of trees: 300 ## No. of variables tried at each split: 3 ## ## OOB estimate of error rate: 32.23% ## Confusion matrix: ## 3 4 5 6 7 8 9 class.error ## 3 0 1 10 8 0 0 0 1.0000000 ## 4 0 21 85 45 0 0 0 0.8609272 ## 5 0 5 1100 407 6 0 0 0.2753623 ## 6 0 1 321 1537 115 1 0 0.2217722 ## 7 0 0 22 343 388 7 0 0.4894737 ## 8 0 0 4 55 30 47 0 0.6544118 ## 9 0 0 0 1 4 0 0 1.0000000 Our original OOB error rate was 32.91% and utilizing our new tuned parameters, our OOB error rate was 32.23%, so it was improved roughly 0.7%. 4.6 Rerun Predicition and Confusion Matrix - Training Data p1 &lt;- predict(rf, rf.train) p1&lt;- droplevels(p1) # drop any unused levels head(p1) # predicted values ## 1 3 6 7 9 10 ## 5 5 5 5 7 5 ## Levels: 3 4 5 6 7 8 9 head(rf.train$quality) # actual values ## [1] 5 5 5 5 7 5 ## Levels: 3 4 5 6 7 8 9 confusionMatrix(p1, rf.train$quality ) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 3 4 5 6 7 8 9 ## 3 19 0 0 0 0 0 0 ## 4 0 151 0 0 0 0 0 ## 5 0 0 1518 0 0 0 0 ## 6 0 0 0 1975 0 0 0 ## 7 0 0 0 0 760 0 0 ## 8 0 0 0 0 0 136 0 ## 9 0 0 0 0 0 0 5 ## ## Overall Statistics ## ## Accuracy : 1 ## 95% CI : (0.9992, 1) ## No Information Rate : 0.4327 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 1 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: 3 Class: 4 Class: 5 Class: 6 Class: 7 Class: 8 Class: 9 ## Sensitivity 1.000000 1.00000 1.0000 1.0000 1.0000 1.0000 1.000000 ## Specificity 1.000000 1.00000 1.0000 1.0000 1.0000 1.0000 1.000000 ## Pos Pred Value 1.000000 1.00000 1.0000 1.0000 1.0000 1.0000 1.000000 ## Neg Pred Value 1.000000 1.00000 1.0000 1.0000 1.0000 1.0000 1.000000 ## Prevalence 0.004163 0.03309 0.3326 0.4327 0.1665 0.0298 0.001096 ## Detection Rate 0.004163 0.03309 0.3326 0.4327 0.1665 0.0298 0.001096 ## Detection Prevalence 0.004163 0.03309 0.3326 0.4327 0.1665 0.0298 0.001096 ## Balanced Accuracy 1.000000 1.00000 1.0000 1.0000 1.0000 1.0000 1.000000 Again, all of our predictions were 100% accurate as the training data had already been seen by the model. 4.7 Rerun Predicition and Confusion Matrix - Test Data p2 &lt;- predict(rf, rf.test) p2 &lt;- droplevels(p2) # drop any unuused levels head(p2) # predicted values ## 2 4 5 8 11 16 ## 5 5 5 5 5 5 ## Levels: 3 4 5 6 7 8 head(rf.test$quality) # actual values ## [1] 5 6 5 7 5 5 ## Levels: 3 4 5 6 7 8 confusionMatrix(p2, rf.test$quality ) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 3 4 5 6 7 8 ## 3 0 1 0 0 0 0 ## 4 0 8 1 2 0 0 ## 5 3 32 447 139 10 0 ## 6 7 23 170 675 130 25 ## 7 1 1 2 45 175 14 ## 8 0 0 0 0 4 18 ## ## Overall Statistics ## ## Accuracy : 0.6844 ## 95% CI : (0.6632, 0.7051) ## No Information Rate : 0.4454 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.5047 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: 3 Class: 4 Class: 5 Class: 6 Class: 7 Class: 8 ## Sensitivity 0.0000000 0.123077 0.7210 0.7840 0.54859 0.315789 ## Specificity 0.9994797 0.998394 0.8599 0.6688 0.96097 0.997868 ## Pos Pred Value 0.0000000 0.727273 0.7084 0.6553 0.73529 0.818182 ## Neg Pred Value 0.9943064 0.970343 0.8671 0.7940 0.91504 0.979592 ## Prevalence 0.0056906 0.033626 0.3207 0.4454 0.16503 0.029488 ## Detection Rate 0.0000000 0.004139 0.2312 0.3492 0.09053 0.009312 ## Detection Prevalence 0.0005173 0.005691 0.3264 0.5329 0.12312 0.011381 ## Balanced Accuracy 0.4997399 0.560735 0.7904 0.7264 0.75478 0.656829 The accuracy is roughly 68.24% when utilizing the test dataset with the new random forest model. This is a .7% improvement from the previous results of 68.91%. 4.8 Variable Importance We are able to see the variables that had the highest level of importance in our model by running the varImpPlot function. varImpPlot(rf, main=&quot;Variable Importance&quot;) This tells us that alcohol has the greatest importance in our model. Removing this variable would result in a 75% mean decrease in accuracy. 4.9 Accuracy In the code below we are adding the percentage accuracy to the accuracy dataframe for later model comparison. accuracy[2,1] &lt;- confusionMatrix(p2, rf.test$quality )[[&quot;overall&quot;]][[&quot;Accuracy&quot;]] "],["k-nearest-neighbor.html", "Chapter 5 K-Nearest Neighbor 5.1 Introduction 5.2 Load Data 5.3 Clean and Normalize the data. 5.4 Data Splice 5.5 Model Evaluation 5.6 Optimization", " Chapter 5 K-Nearest Neighbor 5.1 Introduction Just planning to use this file to walk through some tutorials of k-means clustering I found. 5.2 Load Data We have taken a data-set from Kaggle: https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009 This data-set inlcudes the measurements from the wine along with its quality rating. wine&lt;-read.table(&quot;data/winequality-red.csv&quot;,stringsAsFactors = FALSE, sep=&quot;,&quot;,header = TRUE) head(wine) ## fixed.acidity volatile.acidity citric.acid residual.sugar chlorides free.sulfur.dioxide total.sulfur.dioxide density pH sulphates alcohol ## 1 7.4 0.70 0.00 1.9 0.076 11 34 0.9978 3.51 0.56 9.4 ## 2 7.8 0.88 0.00 2.6 0.098 25 67 0.9968 3.20 0.68 9.8 ## 3 7.8 0.76 0.04 2.3 0.092 15 54 0.9970 3.26 0.65 9.8 ## 4 11.2 0.28 0.56 1.9 0.075 17 60 0.9980 3.16 0.58 9.8 ## 5 7.4 0.70 0.00 1.9 0.076 11 34 0.9978 3.51 0.56 9.4 ## 6 7.4 0.66 0.00 1.8 0.075 13 40 0.9978 3.51 0.56 9.4 ## quality ## 1 5 ## 2 5 ## 3 5 ## 4 6 ## 5 5 ## 6 5 5.3 Clean and Normalize the data. Our dataset already contains only predictive values and output, so we do not need to remove any descriptive columns. We must normalize the values within the dataset to avoid any bias and remove the output variable (quality) since its the prediction. #Normalization normalize &lt;- function(x) { return ((x - min(x)) / (max(x) - min(x))) } wine.normal &lt;- as.data.frame(lapply(wine[,1:11], normalize)) head(wine.normal) ## fixed.acidity volatile.acidity citric.acid residual.sugar chlorides free.sulfur.dioxide total.sulfur.dioxide density pH sulphates ## 1 0.2477876 0.3972603 0.00 0.06849315 0.1068447 0.1408451 0.09893993 0.5675477 0.6062992 0.1377246 ## 2 0.2831858 0.5205479 0.00 0.11643836 0.1435726 0.3380282 0.21554770 0.4941263 0.3622047 0.2095808 ## 3 0.2831858 0.4383562 0.04 0.09589041 0.1335559 0.1971831 0.16961131 0.5088106 0.4094488 0.1916168 ## 4 0.5840708 0.1095890 0.56 0.06849315 0.1051753 0.2253521 0.19081272 0.5822320 0.3307087 0.1497006 ## 5 0.2477876 0.3972603 0.00 0.06849315 0.1068447 0.1408451 0.09893993 0.5675477 0.6062992 0.1377246 ## 6 0.2477876 0.3698630 0.00 0.06164384 0.1051753 0.1690141 0.12014134 0.5675477 0.6062992 0.1377246 ## alcohol ## 1 0.1538462 ## 2 0.2153846 ## 3 0.2153846 ## 4 0.2153846 ## 5 0.1538462 ## 6 0.1538462 5.4 Data Splice Since our data-set our research question involves prediction will will randomly select a portion of data to use for overall effectiveness measurement. We plan to save about \\(5\\%\\) of the data for testing which ends up being \\(80\\) values. set.seed(123) dat.d &lt;- sample(1:nrow(wine.normal),size=nrow(wine.normal)*0.8,replace = FALSE) #random selection of 90% data. train.wine &lt;- wine.normal[dat.d,] # 90% training data test.wine &lt;- wine.normal[-dat.d,] # remaining 10% test data #Creating seperate dataframe for &#39;Quality&#39; feature which is our target. train.quality_label &lt;- wine[dat.d,12] test.quality_label &lt;-wine[-dat.d,12] Next, were going to calculate the number of observations in the training data set. The reason were doing this is that we want to initialize the value of K in the KNN model. One of the ways to find the optimal K value is to calculate the square root of the total number of observations in the data set. This square root will give you the K value. NROW(train.quality_label) ## [1] 1279 sqrt(NROW(train.quality_label) ) ## [1] 35.76311 The square root of 1493 is around 35.7 well create a model with a K value as 36. knn.36 &lt;- knn(train=train.wine, test=test.wine, cl=train.quality_label, k=36) 5.5 Model Evaluation #Calculate the proportion of correct classification for k =37 ACC.36 &lt;- 100 * sum(test.quality_label == knn.36)/NROW(test.quality_label) ACC.36 ## [1] 58.4375 As shown above, the accuracy for K = 36 is 58.435 5.6 Optimization i=1 k.optm=1 for (i in 1:37){ knn.mod &lt;- knn(train=train.wine, test=test.wine, cl=train.quality_label, k=i) k.optm[i] &lt;- 100 * sum(test.quality_label == knn.mod)/NROW(test.quality_label) k=i cat(k,&#39;=&#39;,k.optm[i],&#39;&#39;) } ## 1 = 61.875 2 = 56.875 3 = 56.875 4 = 61.25 5 = 59.0625 6 = 58.125 7 = 55.625 8 = 57.1875 9 = 57.5 10 = 56.875 11 = 59.0625 12 = 59.375 13 = 58.75 14 = 59.375 15 = 60 16 = 60.3125 17 = 61.25 18 = 60.9375 19 = 60 20 = 59.0625 21 = 59.375 22 = 59.375 23 = 59.6875 24 = 59.0625 25 = 58.125 26 = 58.125 27 = 59.0625 28 = 59.0625 29 = 58.4375 30 = 58.4375 31 = 58.75 32 = 59.375 33 = 60 34 = 60.3125 35 = 59.6875 36 = 59.0625 37 = 59.375 #Accuracy plot plot(k.optm, type=&quot;b&quot;, xlab=&quot;K- Value&quot;,ylab=&quot;Accuracy level&quot;) "],["lasso-regression.html", "Chapter 6 Lasso Regression 6.1 Linear Regression 6.2 Ridge Regression 6.3 Lasso Regression", " Chapter 6 Lasso Regression 6.1 Linear Regression 6.1.1 What is it? The simplest form of regression is linear regression, which assumes that the predictors have a linear relationship with the target variable. ### Assumptions * Input is assumed to have a Normal distribution and are not correlated with each other. We saw in the Descriptive Stats section that visually this factor had a strong correlation with quality. With these assumptions being true we can model quality with the following equation. \\[ q = a_1x_1 + a_2x_2 + a_3x_3 + \\dots + a_2nx_n + b + \\epsilon\\] Where \\(a_1, a_2, \\dots, a_n\\) are coefficients from the model. \\(x_1, x_2, \\dots, x_n\\) are the input variables to the model. \\(b\\) is a factor of the model representing the y-intercept and \\(q\\) is equal to the quality output. Finally, \\(\\epsilon\\) is the error. The method that we use to optimize this model is Ordinary Least Squares (OLS). 6.1.2 Single Variable In the code-block below we output the summary of just the measurement alcohol into the linear model. lr.1 &lt;- lm(quality~alcohol, data = train) summary(lr.1) ## ## Call: ## lm(formula = quality ~ alcohol, data = train) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4417 -0.5005 -0.0522 0.4995 3.2074 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.01358 0.02706 185.29 &lt;2e-16 *** ## alcohol 2.23962 0.06800 32.94 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7863 on 4562 degrees of freedom ## Multiple R-squared: 0.1921, Adjusted R-squared: 0.1919 ## F-statistic: 1085 on 1 and 4562 DF, p-value: &lt; 2.2e-16 Focusing on the p-value above we can tell that it is unlikely that by chance we will observe a relationship between alcohol and quality. Which fits our standard threshold of \\(5\\%\\). We will use the Root Mean Square Error (RMSE) to compare the results of each of the models. (Glenn) The Root Mean Square Error (RMSE) is the standard deviation of the residuals (prediction errors). Residuals are a measure of how far from the regression line data points are; RMSE is a measure of how spread out these residuals are. In other words, it tells you how concentrated the data is around the line of best fit. We are looking for a value very close to zero here as the formula is as follows: \\[RMSE = \\sqrt{\\frac{\\sum_{i=1}^{N}(predicted_i - actual_i)^2}{N}}\\] p.1 &lt;- predict(lr.1, newdata = test) RMSE.1 &lt;- RMSE(p.1,test$quality) RMSE.1 ## [1] 0.77316 Since we will be comparing results to other discrette statistical methods we now calculate an accuracy measure. rp.1&lt;-round(p.1) acc.1 &lt;- mean(rp.1==test$quality) 6.1.3 Multiple Variables In the code block below we will input all of the variables and examine the output. lr.2 &lt;- lm(quality~., data = train) summary(lr.2) ## ## Call: ## lm(formula = quality ~ ., data = train) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.6204 -0.4638 -0.0460 0.4627 2.9628 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.21981 0.09286 56.213 &lt; 2e-16 *** ## fixed.acidity 0.93217 0.22387 4.164 3.19e-05 *** ## volatile.acidity -2.06695 0.13890 -14.880 &lt; 2e-16 *** ## citric.acid -0.29888 0.15704 -1.903 0.0571 . ## residual.sugar 3.12413 0.39967 7.817 6.68e-15 *** ## chlorides -0.37223 0.22612 -1.646 0.0998 . ## free.sulfur.dioxide 1.58600 0.25778 6.153 8.28e-10 *** ## total.sulfur.dioxide -1.11244 0.14508 -7.668 2.12e-14 *** ## density -3.12231 0.73831 -4.229 2.39e-05 *** ## pH 0.58725 0.13904 4.224 2.45e-05 *** ## sulphates 1.44233 0.16082 8.968 &lt; 2e-16 *** ## alcohol 1.79685 0.13661 13.153 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7371 on 4552 degrees of freedom ## Multiple R-squared: 0.2915, Adjusted R-squared: 0.2898 ## F-statistic: 170.3 on 11 and 4552 DF, p-value: &lt; 2.2e-16 Reviewing the output we find from the output that the variables that fall under our standard \\(5\\%\\) null-hypothesis threshold are: volatile.acidity, citric.acid, chlorides, sulphates, and alcohol. One of the issues with so many variables is there a risk of including variables that do not affect the outcome. p.2 &lt;- predict(lr.2, newdata = test) RMSE.2 &lt;- RMSE(p.2,test$quality) RMSE.2 ## [1] 0.7317968 rp.2&lt;-round(p.2) mean(rp.2==test$quality) ## [1] 0.5390585 6.1.4 Less Variables In the code block below we will input all of the variables and examine the output. lr.3 &lt;- lm(quality~ volatile.acidity + citric.acid + chlorides + free.sulfur.dioxide + sulphates + alcohol, data = train) summary(lr.3) ## ## Call: ## lm(formula = quality ~ volatile.acidity + citric.acid + chlorides + ## free.sulfur.dioxide + sulphates + alcohol, data = train) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.7639 -0.4727 -0.0507 0.4813 3.1745 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.16190 0.05906 87.404 &lt; 2e-16 *** ## volatile.acidity -2.06506 0.12577 -16.420 &lt; 2e-16 *** ## citric.acid -0.26893 0.14025 -1.918 0.0552 . ## chlorides -0.51031 0.22066 -2.313 0.0208 * ## free.sulfur.dioxide 0.81147 0.19723 4.114 3.95e-05 *** ## sulphates 1.33698 0.14654 9.124 &lt; 2e-16 *** ## alcohol 2.19591 0.06904 31.807 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7474 on 4557 degrees of freedom ## Multiple R-squared: 0.2709, Adjusted R-squared: 0.2699 ## F-statistic: 282.2 on 6 and 4557 DF, p-value: &lt; 2.2e-16 p.3 &lt;- predict(lr.3, newdata = test) RMSE.3 &lt;- RMSE(p.3,test$quality) RMSE.3 ## [1] 0.7376155 rp.3&lt;-round(p.3) mean(rp.3==test$quality) ## [1] 0.5437144 6.2 Ridge Regression Ridge regression is an extension of linear regression where the loss function is modified to minimize the complexity of the model. This modification is done by adding a penalty parameter that is equivalent to the square of the magnitude of the coefficients. If we re-write the OLS in Matrix for below: \\[X_tX\\beta = X_tY\\] To solbe for the \\(\\beta\\) terms to obtain the estimation model, we obtain the following. \\[\\beta = (X^{&#39;}X)^{-1}X^{&#39;}Y\\] Ridge regression modifies the above by adding a small value of \\(\\lambda\\), to the diagonal elements of the correlation matrix. \\[\\beta = (R+\\lambda I)^{-1}X^{&#39;}Y\\] We need to find an optimal value for the \\(lambda\\) factor. The glmnet feature makes this easy for us. It does not accept a data frame though so we need to make a matrix of the the input variables. The other thing that the chunk below is doing is creating a vector of \\(\\lambda\\) values that we want to try. Finally, we plot the qual &lt;- train$quality cols &lt;-colnames(train) cols &lt;- cols[1:length(cols)-1] train.mat &lt;- train %&gt;% select(cols) %&gt;% data.matrix() ## Note: Using an external vector in selections is ambiguous. ## i Use `all_of(cols)` instead of `cols` to silence this message. ## i See &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;. ## This message is displayed once per session. lambdas &lt;- 10^seq(3, -2, by = -.1) ridge_reg &lt;- cv.glmnet(train.mat, qual, alpha = 0, lambda = lambdas) plot(ridge_reg) The lowest point in the curve indicates the optimal lambda: the log value of lambda that best minimized the error in cross-validation. This can be pulled out of the glmnet output. opt_lambda &lt;- ridge_reg$lambda.min opt_lambda ## [1] 0.01 ridge_model &lt;- ridge_reg$glmnet.fit summary(ridge_model) ## Length Class Mode ## a0 51 -none- numeric ## beta 561 dgCMatrix S4 ## df 51 -none- numeric ## dim 2 -none- numeric ## lambda 51 -none- numeric ## dev.ratio 51 -none- numeric ## nulldev 1 -none- numeric ## npasses 1 -none- numeric ## jerr 1 -none- numeric ## offset 1 -none- logical ## call 5 -none- call ## nobs 1 -none- numeric Next, we prepare our test data for prediction. test.mat &lt;- test %&gt;% select(cols) %&gt;% data.matrix() Next, we find the RMSE of the ridge regression. p.4 &lt;- predict(ridge_model, s = opt_lambda, newx = test.mat) RMSE.4 &lt;- RMSE(p.4,test$quality) RMSE.4 ## [1] 0.7314353 rp.4&lt;-round(p.4) mean(rp.4==test$quality) ## [1] 0.5390585 6.3 Lasso Regression The method for LASSO regression is much similar to the above. lambdas &lt;- 10^seq(2, -3, by = -.1) # Setting alpha = 1 implements lasso regression lasso_reg &lt;- cv.glmnet(train.mat, qual, alpha = 1, lambda = lambdas) plot(lasso_reg) # Best lambda_best &lt;- lasso_reg$lambda.min lambda_best ## [1] 0.001 p.5 &lt;- predict(lasso_reg, s = lambda_best, newx = test.mat) RMSE.5 &lt;- RMSE(p.5,test$quality) RMSE.5 ## [1] 0.7315071 rp.5&lt;-round(p.5) mean(rp.5==test$quality) ## [1] 0.5395758 6.3.1 Features of the glmnet Package \\(\\lambda\\) is defined once and \\(\\alpha\\) where lasso is scaled by \\(\\alpha\\) and ridge penalty is scaled by \\((1-\\alpha\\)). 6.3.2 Elastic Net Regression Elastic net regression combines the properties of ridge and lasso regression. So, we need a technique to cycle through find the most optimal limiting factors. ## + Fold01.Rep1: alpha=0.26250, lambda=0.261806 ## - Fold01.Rep1: alpha=0.26250, lambda=0.261806 ## + Fold01.Rep1: alpha=0.54570, lambda=0.002641 ## - Fold01.Rep1: alpha=0.54570, lambda=0.002641 ## + Fold01.Rep1: alpha=0.11957, lambda=0.123919 ## - Fold01.Rep1: alpha=0.11957, lambda=0.123919 ## + Fold01.Rep1: alpha=0.30234, lambda=0.027277 ## - Fold01.Rep1: alpha=0.30234, lambda=0.027277 ## + Fold01.Rep1: alpha=0.95329, lambda=0.004732 ## - Fold01.Rep1: alpha=0.95329, lambda=0.004732 ## + Fold01.Rep1: alpha=0.09178, lambda=0.186019 ## - Fold01.Rep1: alpha=0.09178, lambda=0.186019 ## + Fold01.Rep1: alpha=0.73123, lambda=0.160383 ## - Fold01.Rep1: alpha=0.73123, lambda=0.160383 ## + Fold01.Rep1: alpha=0.17634, lambda=0.003602 ## - Fold01.Rep1: alpha=0.17634, lambda=0.003602 ## + Fold01.Rep1: alpha=0.85495, lambda=0.687550 ## - Fold01.Rep1: alpha=0.85495, lambda=0.687550 ## + Fold01.Rep1: alpha=0.17667, lambda=2.997956 ## - Fold01.Rep1: alpha=0.17667, lambda=2.997956 ## + Fold02.Rep1: alpha=0.26250, lambda=0.261806 ## - Fold02.Rep1: alpha=0.26250, lambda=0.261806 ## + Fold02.Rep1: alpha=0.54570, lambda=0.002641 ## - Fold02.Rep1: alpha=0.54570, lambda=0.002641 ## + Fold02.Rep1: alpha=0.11957, lambda=0.123919 ## - Fold02.Rep1: alpha=0.11957, lambda=0.123919 ## + Fold02.Rep1: alpha=0.30234, lambda=0.027277 ## - Fold02.Rep1: alpha=0.30234, lambda=0.027277 ## + Fold02.Rep1: alpha=0.95329, lambda=0.004732 ## - Fold02.Rep1: alpha=0.95329, lambda=0.004732 ## + Fold02.Rep1: alpha=0.09178, lambda=0.186019 ## - Fold02.Rep1: alpha=0.09178, lambda=0.186019 ## + Fold02.Rep1: alpha=0.73123, lambda=0.160383 ## - Fold02.Rep1: alpha=0.73123, lambda=0.160383 ## + Fold02.Rep1: alpha=0.17634, lambda=0.003602 ## - Fold02.Rep1: alpha=0.17634, lambda=0.003602 ## + Fold02.Rep1: alpha=0.85495, lambda=0.687550 ## - Fold02.Rep1: alpha=0.85495, lambda=0.687550 ## + Fold02.Rep1: alpha=0.17667, lambda=2.997956 ## - Fold02.Rep1: alpha=0.17667, lambda=2.997956 ## + Fold03.Rep1: alpha=0.26250, lambda=0.261806 ## - Fold03.Rep1: alpha=0.26250, lambda=0.261806 ## + Fold03.Rep1: alpha=0.54570, lambda=0.002641 ## - Fold03.Rep1: alpha=0.54570, lambda=0.002641 ## + Fold03.Rep1: alpha=0.11957, lambda=0.123919 ## - Fold03.Rep1: alpha=0.11957, lambda=0.123919 ## + Fold03.Rep1: alpha=0.30234, lambda=0.027277 ## - Fold03.Rep1: alpha=0.30234, lambda=0.027277 ## + Fold03.Rep1: alpha=0.95329, lambda=0.004732 ## - Fold03.Rep1: alpha=0.95329, lambda=0.004732 ## + Fold03.Rep1: alpha=0.09178, lambda=0.186019 ## - Fold03.Rep1: alpha=0.09178, lambda=0.186019 ## + Fold03.Rep1: alpha=0.73123, lambda=0.160383 ## - Fold03.Rep1: alpha=0.73123, lambda=0.160383 ## + Fold03.Rep1: alpha=0.17634, lambda=0.003602 ## - Fold03.Rep1: alpha=0.17634, lambda=0.003602 ## + Fold03.Rep1: alpha=0.85495, lambda=0.687550 ## - Fold03.Rep1: alpha=0.85495, lambda=0.687550 ## + Fold03.Rep1: alpha=0.17667, lambda=2.997956 ## - Fold03.Rep1: alpha=0.17667, lambda=2.997956 ## + Fold04.Rep1: alpha=0.26250, lambda=0.261806 ## - Fold04.Rep1: alpha=0.26250, lambda=0.261806 ## + Fold04.Rep1: alpha=0.54570, lambda=0.002641 ## - Fold04.Rep1: alpha=0.54570, lambda=0.002641 ## + Fold04.Rep1: alpha=0.11957, lambda=0.123919 ## - Fold04.Rep1: alpha=0.11957, lambda=0.123919 ## + Fold04.Rep1: alpha=0.30234, lambda=0.027277 ## - Fold04.Rep1: alpha=0.30234, lambda=0.027277 ## + Fold04.Rep1: alpha=0.95329, lambda=0.004732 ## - Fold04.Rep1: alpha=0.95329, lambda=0.004732 ## + Fold04.Rep1: alpha=0.09178, lambda=0.186019 ## - Fold04.Rep1: alpha=0.09178, lambda=0.186019 ## + Fold04.Rep1: alpha=0.73123, lambda=0.160383 ## - Fold04.Rep1: alpha=0.73123, lambda=0.160383 ## + Fold04.Rep1: alpha=0.17634, lambda=0.003602 ## - Fold04.Rep1: alpha=0.17634, lambda=0.003602 ## + Fold04.Rep1: alpha=0.85495, lambda=0.687550 ## - Fold04.Rep1: alpha=0.85495, lambda=0.687550 ## + Fold04.Rep1: alpha=0.17667, lambda=2.997956 ## - Fold04.Rep1: alpha=0.17667, lambda=2.997956 ## + Fold05.Rep1: alpha=0.26250, lambda=0.261806 ## - Fold05.Rep1: alpha=0.26250, lambda=0.261806 ## + Fold05.Rep1: alpha=0.54570, lambda=0.002641 ## - Fold05.Rep1: alpha=0.54570, lambda=0.002641 ## + Fold05.Rep1: alpha=0.11957, lambda=0.123919 ## - Fold05.Rep1: alpha=0.11957, lambda=0.123919 ## + Fold05.Rep1: alpha=0.30234, lambda=0.027277 ## - Fold05.Rep1: alpha=0.30234, lambda=0.027277 ## + Fold05.Rep1: alpha=0.95329, lambda=0.004732 ## - Fold05.Rep1: alpha=0.95329, lambda=0.004732 ## + Fold05.Rep1: alpha=0.09178, lambda=0.186019 ## - Fold05.Rep1: alpha=0.09178, lambda=0.186019 ## + Fold05.Rep1: alpha=0.73123, lambda=0.160383 ## - Fold05.Rep1: alpha=0.73123, lambda=0.160383 ## + Fold05.Rep1: alpha=0.17634, lambda=0.003602 ## - Fold05.Rep1: alpha=0.17634, lambda=0.003602 ## + Fold05.Rep1: alpha=0.85495, lambda=0.687550 ## - Fold05.Rep1: alpha=0.85495, lambda=0.687550 ## + Fold05.Rep1: alpha=0.17667, lambda=2.997956 ## - Fold05.Rep1: alpha=0.17667, lambda=2.997956 ## + Fold06.Rep1: alpha=0.26250, lambda=0.261806 ## - Fold06.Rep1: alpha=0.26250, lambda=0.261806 ## + Fold06.Rep1: alpha=0.54570, lambda=0.002641 ## - Fold06.Rep1: alpha=0.54570, lambda=0.002641 ## + Fold06.Rep1: alpha=0.11957, lambda=0.123919 ## - Fold06.Rep1: alpha=0.11957, lambda=0.123919 ## + Fold06.Rep1: alpha=0.30234, lambda=0.027277 ## - Fold06.Rep1: alpha=0.30234, lambda=0.027277 ## + Fold06.Rep1: alpha=0.95329, lambda=0.004732 ## - Fold06.Rep1: alpha=0.95329, lambda=0.004732 ## + Fold06.Rep1: alpha=0.09178, lambda=0.186019 ## - Fold06.Rep1: alpha=0.09178, lambda=0.186019 ## + Fold06.Rep1: alpha=0.73123, lambda=0.160383 ## - Fold06.Rep1: alpha=0.73123, lambda=0.160383 ## + Fold06.Rep1: alpha=0.17634, lambda=0.003602 ## - Fold06.Rep1: alpha=0.17634, lambda=0.003602 ## + Fold06.Rep1: alpha=0.85495, lambda=0.687550 ## - Fold06.Rep1: alpha=0.85495, lambda=0.687550 ## + Fold06.Rep1: alpha=0.17667, lambda=2.997956 ## - Fold06.Rep1: alpha=0.17667, lambda=2.997956 ## + Fold07.Rep1: alpha=0.26250, lambda=0.261806 ## - Fold07.Rep1: alpha=0.26250, lambda=0.261806 ## + Fold07.Rep1: alpha=0.54570, lambda=0.002641 ## - Fold07.Rep1: alpha=0.54570, lambda=0.002641 ## + Fold07.Rep1: alpha=0.11957, lambda=0.123919 ## - Fold07.Rep1: alpha=0.11957, lambda=0.123919 ## + Fold07.Rep1: alpha=0.30234, lambda=0.027277 ## - Fold07.Rep1: alpha=0.30234, lambda=0.027277 ## + Fold07.Rep1: alpha=0.95329, lambda=0.004732 ## - Fold07.Rep1: alpha=0.95329, lambda=0.004732 ## + Fold07.Rep1: alpha=0.09178, lambda=0.186019 ## - Fold07.Rep1: alpha=0.09178, lambda=0.186019 ## + Fold07.Rep1: alpha=0.73123, lambda=0.160383 ## - Fold07.Rep1: alpha=0.73123, lambda=0.160383 ## + Fold07.Rep1: alpha=0.17634, lambda=0.003602 ## - Fold07.Rep1: alpha=0.17634, lambda=0.003602 ## + Fold07.Rep1: alpha=0.85495, lambda=0.687550 ## - Fold07.Rep1: alpha=0.85495, lambda=0.687550 ## + Fold07.Rep1: alpha=0.17667, lambda=2.997956 ## - Fold07.Rep1: alpha=0.17667, lambda=2.997956 ## + Fold08.Rep1: alpha=0.26250, lambda=0.261806 ## - Fold08.Rep1: alpha=0.26250, lambda=0.261806 ## + Fold08.Rep1: alpha=0.54570, lambda=0.002641 ## - Fold08.Rep1: alpha=0.54570, lambda=0.002641 ## + Fold08.Rep1: alpha=0.11957, lambda=0.123919 ## - Fold08.Rep1: alpha=0.11957, lambda=0.123919 ## + Fold08.Rep1: alpha=0.30234, lambda=0.027277 ## - Fold08.Rep1: alpha=0.30234, lambda=0.027277 ## + Fold08.Rep1: alpha=0.95329, lambda=0.004732 ## - Fold08.Rep1: alpha=0.95329, lambda=0.004732 ## + Fold08.Rep1: alpha=0.09178, lambda=0.186019 ## - Fold08.Rep1: alpha=0.09178, lambda=0.186019 ## + Fold08.Rep1: alpha=0.73123, lambda=0.160383 ## - Fold08.Rep1: alpha=0.73123, lambda=0.160383 ## + Fold08.Rep1: alpha=0.17634, lambda=0.003602 ## - Fold08.Rep1: alpha=0.17634, lambda=0.003602 ## + Fold08.Rep1: alpha=0.85495, lambda=0.687550 ## - Fold08.Rep1: alpha=0.85495, lambda=0.687550 ## + Fold08.Rep1: alpha=0.17667, lambda=2.997956 ## - Fold08.Rep1: alpha=0.17667, lambda=2.997956 ## + Fold09.Rep1: alpha=0.26250, lambda=0.261806 ## - Fold09.Rep1: alpha=0.26250, lambda=0.261806 ## + Fold09.Rep1: alpha=0.54570, lambda=0.002641 ## - Fold09.Rep1: alpha=0.54570, lambda=0.002641 ## + Fold09.Rep1: alpha=0.11957, lambda=0.123919 ## - Fold09.Rep1: alpha=0.11957, lambda=0.123919 ## + Fold09.Rep1: alpha=0.30234, lambda=0.027277 ## - Fold09.Rep1: alpha=0.30234, lambda=0.027277 ## + Fold09.Rep1: alpha=0.95329, lambda=0.004732 ## - Fold09.Rep1: alpha=0.95329, lambda=0.004732 ## + Fold09.Rep1: alpha=0.09178, lambda=0.186019 ## - Fold09.Rep1: alpha=0.09178, lambda=0.186019 ## + Fold09.Rep1: alpha=0.73123, lambda=0.160383 ## - Fold09.Rep1: alpha=0.73123, lambda=0.160383 ## + Fold09.Rep1: alpha=0.17634, lambda=0.003602 ## - Fold09.Rep1: alpha=0.17634, lambda=0.003602 ## + Fold09.Rep1: alpha=0.85495, lambda=0.687550 ## - Fold09.Rep1: alpha=0.85495, lambda=0.687550 ## + Fold09.Rep1: alpha=0.17667, lambda=2.997956 ## - Fold09.Rep1: alpha=0.17667, lambda=2.997956 ## + Fold10.Rep1: alpha=0.26250, lambda=0.261806 ## - Fold10.Rep1: alpha=0.26250, lambda=0.261806 ## + Fold10.Rep1: alpha=0.54570, lambda=0.002641 ## - Fold10.Rep1: alpha=0.54570, lambda=0.002641 ## + Fold10.Rep1: alpha=0.11957, lambda=0.123919 ## - Fold10.Rep1: alpha=0.11957, lambda=0.123919 ## + Fold10.Rep1: alpha=0.30234, lambda=0.027277 ## - Fold10.Rep1: alpha=0.30234, lambda=0.027277 ## + Fold10.Rep1: alpha=0.95329, lambda=0.004732 ## - Fold10.Rep1: alpha=0.95329, lambda=0.004732 ## + Fold10.Rep1: alpha=0.09178, lambda=0.186019 ## - Fold10.Rep1: alpha=0.09178, lambda=0.186019 ## + Fold10.Rep1: alpha=0.73123, lambda=0.160383 ## - Fold10.Rep1: alpha=0.73123, lambda=0.160383 ## + Fold10.Rep1: alpha=0.17634, lambda=0.003602 ## - Fold10.Rep1: alpha=0.17634, lambda=0.003602 ## + Fold10.Rep1: alpha=0.85495, lambda=0.687550 ## - Fold10.Rep1: alpha=0.85495, lambda=0.687550 ## + Fold10.Rep1: alpha=0.17667, lambda=2.997956 ## - Fold10.Rep1: alpha=0.17667, lambda=2.997956 ## + Fold01.Rep2: alpha=0.26250, lambda=0.261806 ## - Fold01.Rep2: alpha=0.26250, lambda=0.261806 ## + Fold01.Rep2: alpha=0.54570, lambda=0.002641 ## - Fold01.Rep2: alpha=0.54570, lambda=0.002641 ## + Fold01.Rep2: alpha=0.11957, lambda=0.123919 ## - Fold01.Rep2: alpha=0.11957, lambda=0.123919 ## + Fold01.Rep2: alpha=0.30234, lambda=0.027277 ## - Fold01.Rep2: alpha=0.30234, lambda=0.027277 ## + Fold01.Rep2: alpha=0.95329, lambda=0.004732 ## - Fold01.Rep2: alpha=0.95329, lambda=0.004732 ## + Fold01.Rep2: alpha=0.09178, lambda=0.186019 ## - Fold01.Rep2: alpha=0.09178, lambda=0.186019 ## + Fold01.Rep2: alpha=0.73123, lambda=0.160383 ## - Fold01.Rep2: alpha=0.73123, lambda=0.160383 ## + Fold01.Rep2: alpha=0.17634, lambda=0.003602 ## - Fold01.Rep2: alpha=0.17634, lambda=0.003602 ## + Fold01.Rep2: alpha=0.85495, lambda=0.687550 ## - Fold01.Rep2: alpha=0.85495, lambda=0.687550 ## + Fold01.Rep2: alpha=0.17667, lambda=2.997956 ## - Fold01.Rep2: alpha=0.17667, lambda=2.997956 ## + Fold02.Rep2: alpha=0.26250, lambda=0.261806 ## - Fold02.Rep2: alpha=0.26250, lambda=0.261806 ## + Fold02.Rep2: alpha=0.54570, lambda=0.002641 ## - Fold02.Rep2: alpha=0.54570, lambda=0.002641 ## + Fold02.Rep2: alpha=0.11957, lambda=0.123919 ## - Fold02.Rep2: alpha=0.11957, lambda=0.123919 ## + Fold02.Rep2: alpha=0.30234, lambda=0.027277 ## - Fold02.Rep2: alpha=0.30234, lambda=0.027277 ## + Fold02.Rep2: alpha=0.95329, lambda=0.004732 ## - Fold02.Rep2: alpha=0.95329, lambda=0.004732 ## + Fold02.Rep2: alpha=0.09178, lambda=0.186019 ## - Fold02.Rep2: alpha=0.09178, lambda=0.186019 ## + Fold02.Rep2: alpha=0.73123, lambda=0.160383 ## - Fold02.Rep2: alpha=0.73123, lambda=0.160383 ## + Fold02.Rep2: alpha=0.17634, lambda=0.003602 ## - Fold02.Rep2: alpha=0.17634, lambda=0.003602 ## + Fold02.Rep2: alpha=0.85495, lambda=0.687550 ## - Fold02.Rep2: alpha=0.85495, lambda=0.687550 ## + Fold02.Rep2: alpha=0.17667, lambda=2.997956 ## - Fold02.Rep2: alpha=0.17667, lambda=2.997956 ## + Fold03.Rep2: alpha=0.26250, lambda=0.261806 ## - Fold03.Rep2: alpha=0.26250, lambda=0.261806 ## + Fold03.Rep2: alpha=0.54570, lambda=0.002641 ## - Fold03.Rep2: alpha=0.54570, lambda=0.002641 ## + Fold03.Rep2: alpha=0.11957, lambda=0.123919 ## - Fold03.Rep2: alpha=0.11957, lambda=0.123919 ## + Fold03.Rep2: alpha=0.30234, lambda=0.027277 ## - Fold03.Rep2: alpha=0.30234, lambda=0.027277 ## + Fold03.Rep2: alpha=0.95329, lambda=0.004732 ## - Fold03.Rep2: alpha=0.95329, lambda=0.004732 ## + Fold03.Rep2: alpha=0.09178, lambda=0.186019 ## - Fold03.Rep2: alpha=0.09178, lambda=0.186019 ## + Fold03.Rep2: alpha=0.73123, lambda=0.160383 ## - Fold03.Rep2: alpha=0.73123, lambda=0.160383 ## + Fold03.Rep2: alpha=0.17634, lambda=0.003602 ## - Fold03.Rep2: alpha=0.17634, lambda=0.003602 ## + Fold03.Rep2: alpha=0.85495, lambda=0.687550 ## - Fold03.Rep2: alpha=0.85495, lambda=0.687550 ## + Fold03.Rep2: alpha=0.17667, lambda=2.997956 ## - Fold03.Rep2: alpha=0.17667, lambda=2.997956 ## + Fold04.Rep2: alpha=0.26250, lambda=0.261806 ## - Fold04.Rep2: alpha=0.26250, lambda=0.261806 ## + Fold04.Rep2: alpha=0.54570, lambda=0.002641 ## - Fold04.Rep2: alpha=0.54570, lambda=0.002641 ## + Fold04.Rep2: alpha=0.11957, lambda=0.123919 ## - Fold04.Rep2: alpha=0.11957, lambda=0.123919 ## + Fold04.Rep2: alpha=0.30234, lambda=0.027277 ## - Fold04.Rep2: alpha=0.30234, lambda=0.027277 ## + Fold04.Rep2: alpha=0.95329, lambda=0.004732 ## - Fold04.Rep2: alpha=0.95329, lambda=0.004732 ## + Fold04.Rep2: alpha=0.09178, lambda=0.186019 ## - Fold04.Rep2: alpha=0.09178, lambda=0.186019 ## + Fold04.Rep2: alpha=0.73123, lambda=0.160383 ## - Fold04.Rep2: alpha=0.73123, lambda=0.160383 ## + Fold04.Rep2: alpha=0.17634, lambda=0.003602 ## - Fold04.Rep2: alpha=0.17634, lambda=0.003602 ## + Fold04.Rep2: alpha=0.85495, lambda=0.687550 ## - Fold04.Rep2: alpha=0.85495, lambda=0.687550 ## + Fold04.Rep2: alpha=0.17667, lambda=2.997956 ## - Fold04.Rep2: alpha=0.17667, lambda=2.997956 ## + Fold05.Rep2: alpha=0.26250, lambda=0.261806 ## - Fold05.Rep2: alpha=0.26250, lambda=0.261806 ## + Fold05.Rep2: alpha=0.54570, lambda=0.002641 ## - Fold05.Rep2: alpha=0.54570, lambda=0.002641 ## + Fold05.Rep2: alpha=0.11957, lambda=0.123919 ## - Fold05.Rep2: alpha=0.11957, lambda=0.123919 ## + Fold05.Rep2: alpha=0.30234, lambda=0.027277 ## - Fold05.Rep2: alpha=0.30234, lambda=0.027277 ## + Fold05.Rep2: alpha=0.95329, lambda=0.004732 ## - Fold05.Rep2: alpha=0.95329, lambda=0.004732 ## + Fold05.Rep2: alpha=0.09178, lambda=0.186019 ## - Fold05.Rep2: alpha=0.09178, lambda=0.186019 ## + Fold05.Rep2: alpha=0.73123, lambda=0.160383 ## - Fold05.Rep2: alpha=0.73123, lambda=0.160383 ## + Fold05.Rep2: alpha=0.17634, lambda=0.003602 ## - Fold05.Rep2: alpha=0.17634, lambda=0.003602 ## + Fold05.Rep2: alpha=0.85495, lambda=0.687550 ## - Fold05.Rep2: alpha=0.85495, lambda=0.687550 ## + Fold05.Rep2: alpha=0.17667, lambda=2.997956 ## - Fold05.Rep2: alpha=0.17667, lambda=2.997956 ## + Fold06.Rep2: alpha=0.26250, lambda=0.261806 ## - Fold06.Rep2: alpha=0.26250, lambda=0.261806 ## + Fold06.Rep2: alpha=0.54570, lambda=0.002641 ## - Fold06.Rep2: alpha=0.54570, lambda=0.002641 ## + Fold06.Rep2: alpha=0.11957, lambda=0.123919 ## - Fold06.Rep2: alpha=0.11957, lambda=0.123919 ## + Fold06.Rep2: alpha=0.30234, lambda=0.027277 ## - Fold06.Rep2: alpha=0.30234, lambda=0.027277 ## + Fold06.Rep2: alpha=0.95329, lambda=0.004732 ## - Fold06.Rep2: alpha=0.95329, lambda=0.004732 ## + Fold06.Rep2: alpha=0.09178, lambda=0.186019 ## - Fold06.Rep2: alpha=0.09178, lambda=0.186019 ## + Fold06.Rep2: alpha=0.73123, lambda=0.160383 ## - Fold06.Rep2: alpha=0.73123, lambda=0.160383 ## + Fold06.Rep2: alpha=0.17634, lambda=0.003602 ## - Fold06.Rep2: alpha=0.17634, lambda=0.003602 ## + Fold06.Rep2: alpha=0.85495, lambda=0.687550 ## - Fold06.Rep2: alpha=0.85495, lambda=0.687550 ## + Fold06.Rep2: alpha=0.17667, lambda=2.997956 ## - Fold06.Rep2: alpha=0.17667, lambda=2.997956 ## + Fold07.Rep2: alpha=0.26250, lambda=0.261806 ## - Fold07.Rep2: alpha=0.26250, lambda=0.261806 ## + Fold07.Rep2: alpha=0.54570, lambda=0.002641 ## - Fold07.Rep2: alpha=0.54570, lambda=0.002641 ## + Fold07.Rep2: alpha=0.11957, lambda=0.123919 ## - Fold07.Rep2: alpha=0.11957, lambda=0.123919 ## + Fold07.Rep2: alpha=0.30234, lambda=0.027277 ## - Fold07.Rep2: alpha=0.30234, lambda=0.027277 ## + Fold07.Rep2: alpha=0.95329, lambda=0.004732 ## - Fold07.Rep2: alpha=0.95329, lambda=0.004732 ## + Fold07.Rep2: alpha=0.09178, lambda=0.186019 ## - Fold07.Rep2: alpha=0.09178, lambda=0.186019 ## + Fold07.Rep2: alpha=0.73123, lambda=0.160383 ## - Fold07.Rep2: alpha=0.73123, lambda=0.160383 ## + Fold07.Rep2: alpha=0.17634, lambda=0.003602 ## - Fold07.Rep2: alpha=0.17634, lambda=0.003602 ## + Fold07.Rep2: alpha=0.85495, lambda=0.687550 ## - Fold07.Rep2: alpha=0.85495, lambda=0.687550 ## + Fold07.Rep2: alpha=0.17667, lambda=2.997956 ## - Fold07.Rep2: alpha=0.17667, lambda=2.997956 ## + Fold08.Rep2: alpha=0.26250, lambda=0.261806 ## - Fold08.Rep2: alpha=0.26250, lambda=0.261806 ## + Fold08.Rep2: alpha=0.54570, lambda=0.002641 ## - Fold08.Rep2: alpha=0.54570, lambda=0.002641 ## + Fold08.Rep2: alpha=0.11957, lambda=0.123919 ## - Fold08.Rep2: alpha=0.11957, lambda=0.123919 ## + Fold08.Rep2: alpha=0.30234, lambda=0.027277 ## - Fold08.Rep2: alpha=0.30234, lambda=0.027277 ## + Fold08.Rep2: alpha=0.95329, lambda=0.004732 ## - Fold08.Rep2: alpha=0.95329, lambda=0.004732 ## + Fold08.Rep2: alpha=0.09178, lambda=0.186019 ## - Fold08.Rep2: alpha=0.09178, lambda=0.186019 ## + Fold08.Rep2: alpha=0.73123, lambda=0.160383 ## - Fold08.Rep2: alpha=0.73123, lambda=0.160383 ## + Fold08.Rep2: alpha=0.17634, lambda=0.003602 ## - Fold08.Rep2: alpha=0.17634, lambda=0.003602 ## + Fold08.Rep2: alpha=0.85495, lambda=0.687550 ## - Fold08.Rep2: alpha=0.85495, lambda=0.687550 ## + Fold08.Rep2: alpha=0.17667, lambda=2.997956 ## - Fold08.Rep2: alpha=0.17667, lambda=2.997956 ## + Fold09.Rep2: alpha=0.26250, lambda=0.261806 ## - Fold09.Rep2: alpha=0.26250, lambda=0.261806 ## + Fold09.Rep2: alpha=0.54570, lambda=0.002641 ## - Fold09.Rep2: alpha=0.54570, lambda=0.002641 ## + Fold09.Rep2: alpha=0.11957, lambda=0.123919 ## - Fold09.Rep2: alpha=0.11957, lambda=0.123919 ## + Fold09.Rep2: alpha=0.30234, lambda=0.027277 ## - Fold09.Rep2: alpha=0.30234, lambda=0.027277 ## + Fold09.Rep2: alpha=0.95329, lambda=0.004732 ## - Fold09.Rep2: alpha=0.95329, lambda=0.004732 ## + Fold09.Rep2: alpha=0.09178, lambda=0.186019 ## - Fold09.Rep2: alpha=0.09178, lambda=0.186019 ## + Fold09.Rep2: alpha=0.73123, lambda=0.160383 ## - Fold09.Rep2: alpha=0.73123, lambda=0.160383 ## + Fold09.Rep2: alpha=0.17634, lambda=0.003602 ## - Fold09.Rep2: alpha=0.17634, lambda=0.003602 ## + Fold09.Rep2: alpha=0.85495, lambda=0.687550 ## - Fold09.Rep2: alpha=0.85495, lambda=0.687550 ## + Fold09.Rep2: alpha=0.17667, lambda=2.997956 ## - Fold09.Rep2: alpha=0.17667, lambda=2.997956 ## + Fold10.Rep2: alpha=0.26250, lambda=0.261806 ## - Fold10.Rep2: alpha=0.26250, lambda=0.261806 ## + Fold10.Rep2: alpha=0.54570, lambda=0.002641 ## - Fold10.Rep2: alpha=0.54570, lambda=0.002641 ## + Fold10.Rep2: alpha=0.11957, lambda=0.123919 ## - Fold10.Rep2: alpha=0.11957, lambda=0.123919 ## + Fold10.Rep2: alpha=0.30234, lambda=0.027277 ## - Fold10.Rep2: alpha=0.30234, lambda=0.027277 ## + Fold10.Rep2: alpha=0.95329, lambda=0.004732 ## - Fold10.Rep2: alpha=0.95329, lambda=0.004732 ## + Fold10.Rep2: alpha=0.09178, lambda=0.186019 ## - Fold10.Rep2: alpha=0.09178, lambda=0.186019 ## + Fold10.Rep2: alpha=0.73123, lambda=0.160383 ## - Fold10.Rep2: alpha=0.73123, lambda=0.160383 ## + Fold10.Rep2: alpha=0.17634, lambda=0.003602 ## - Fold10.Rep2: alpha=0.17634, lambda=0.003602 ## + Fold10.Rep2: alpha=0.85495, lambda=0.687550 ## - Fold10.Rep2: alpha=0.85495, lambda=0.687550 ## + Fold10.Rep2: alpha=0.17667, lambda=2.997956 ## - Fold10.Rep2: alpha=0.17667, lambda=2.997956 ## + Fold01.Rep3: alpha=0.26250, lambda=0.261806 ## - Fold01.Rep3: alpha=0.26250, lambda=0.261806 ## + Fold01.Rep3: alpha=0.54570, lambda=0.002641 ## - Fold01.Rep3: alpha=0.54570, lambda=0.002641 ## + Fold01.Rep3: alpha=0.11957, lambda=0.123919 ## - Fold01.Rep3: alpha=0.11957, lambda=0.123919 ## + Fold01.Rep3: alpha=0.30234, lambda=0.027277 ## - Fold01.Rep3: alpha=0.30234, lambda=0.027277 ## + Fold01.Rep3: alpha=0.95329, lambda=0.004732 ## - Fold01.Rep3: alpha=0.95329, lambda=0.004732 ## + Fold01.Rep3: alpha=0.09178, lambda=0.186019 ## - Fold01.Rep3: alpha=0.09178, lambda=0.186019 ## + Fold01.Rep3: alpha=0.73123, lambda=0.160383 ## - Fold01.Rep3: alpha=0.73123, lambda=0.160383 ## + Fold01.Rep3: alpha=0.17634, lambda=0.003602 ## - Fold01.Rep3: alpha=0.17634, lambda=0.003602 ## + Fold01.Rep3: alpha=0.85495, lambda=0.687550 ## - Fold01.Rep3: alpha=0.85495, lambda=0.687550 ## + Fold01.Rep3: alpha=0.17667, lambda=2.997956 ## - Fold01.Rep3: alpha=0.17667, lambda=2.997956 ## + Fold02.Rep3: alpha=0.26250, lambda=0.261806 ## - Fold02.Rep3: alpha=0.26250, lambda=0.261806 ## + Fold02.Rep3: alpha=0.54570, lambda=0.002641 ## - Fold02.Rep3: alpha=0.54570, lambda=0.002641 ## + Fold02.Rep3: alpha=0.11957, lambda=0.123919 ## - Fold02.Rep3: alpha=0.11957, lambda=0.123919 ## + Fold02.Rep3: alpha=0.30234, lambda=0.027277 ## - Fold02.Rep3: alpha=0.30234, lambda=0.027277 ## + Fold02.Rep3: alpha=0.95329, lambda=0.004732 ## - Fold02.Rep3: alpha=0.95329, lambda=0.004732 ## + Fold02.Rep3: alpha=0.09178, lambda=0.186019 ## - Fold02.Rep3: alpha=0.09178, lambda=0.186019 ## + Fold02.Rep3: alpha=0.73123, lambda=0.160383 ## - Fold02.Rep3: alpha=0.73123, lambda=0.160383 ## + Fold02.Rep3: alpha=0.17634, lambda=0.003602 ## - Fold02.Rep3: alpha=0.17634, lambda=0.003602 ## + Fold02.Rep3: alpha=0.85495, lambda=0.687550 ## - Fold02.Rep3: alpha=0.85495, lambda=0.687550 ## + Fold02.Rep3: alpha=0.17667, lambda=2.997956 ## - Fold02.Rep3: alpha=0.17667, lambda=2.997956 ## + Fold03.Rep3: alpha=0.26250, lambda=0.261806 ## - Fold03.Rep3: alpha=0.26250, lambda=0.261806 ## + Fold03.Rep3: alpha=0.54570, lambda=0.002641 ## - Fold03.Rep3: alpha=0.54570, lambda=0.002641 ## + Fold03.Rep3: alpha=0.11957, lambda=0.123919 ## - Fold03.Rep3: alpha=0.11957, lambda=0.123919 ## + Fold03.Rep3: alpha=0.30234, lambda=0.027277 ## - Fold03.Rep3: alpha=0.30234, lambda=0.027277 ## + Fold03.Rep3: alpha=0.95329, lambda=0.004732 ## - Fold03.Rep3: alpha=0.95329, lambda=0.004732 ## + Fold03.Rep3: alpha=0.09178, lambda=0.186019 ## - Fold03.Rep3: alpha=0.09178, lambda=0.186019 ## + Fold03.Rep3: alpha=0.73123, lambda=0.160383 ## - Fold03.Rep3: alpha=0.73123, lambda=0.160383 ## + Fold03.Rep3: alpha=0.17634, lambda=0.003602 ## - Fold03.Rep3: alpha=0.17634, lambda=0.003602 ## + Fold03.Rep3: alpha=0.85495, lambda=0.687550 ## - Fold03.Rep3: alpha=0.85495, lambda=0.687550 ## + Fold03.Rep3: alpha=0.17667, lambda=2.997956 ## - Fold03.Rep3: alpha=0.17667, lambda=2.997956 ## + Fold04.Rep3: alpha=0.26250, lambda=0.261806 ## - Fold04.Rep3: alpha=0.26250, lambda=0.261806 ## + Fold04.Rep3: alpha=0.54570, lambda=0.002641 ## - Fold04.Rep3: alpha=0.54570, lambda=0.002641 ## + Fold04.Rep3: alpha=0.11957, lambda=0.123919 ## - Fold04.Rep3: alpha=0.11957, lambda=0.123919 ## + Fold04.Rep3: alpha=0.30234, lambda=0.027277 ## - Fold04.Rep3: alpha=0.30234, lambda=0.027277 ## + Fold04.Rep3: alpha=0.95329, lambda=0.004732 ## - Fold04.Rep3: alpha=0.95329, lambda=0.004732 ## + Fold04.Rep3: alpha=0.09178, lambda=0.186019 ## - Fold04.Rep3: alpha=0.09178, lambda=0.186019 ## + Fold04.Rep3: alpha=0.73123, lambda=0.160383 ## - Fold04.Rep3: alpha=0.73123, lambda=0.160383 ## + Fold04.Rep3: alpha=0.17634, lambda=0.003602 ## - Fold04.Rep3: alpha=0.17634, lambda=0.003602 ## + Fold04.Rep3: alpha=0.85495, lambda=0.687550 ## - Fold04.Rep3: alpha=0.85495, lambda=0.687550 ## + Fold04.Rep3: alpha=0.17667, lambda=2.997956 ## - Fold04.Rep3: alpha=0.17667, lambda=2.997956 ## + Fold05.Rep3: alpha=0.26250, lambda=0.261806 ## - Fold05.Rep3: alpha=0.26250, lambda=0.261806 ## + Fold05.Rep3: alpha=0.54570, lambda=0.002641 ## - Fold05.Rep3: alpha=0.54570, lambda=0.002641 ## + Fold05.Rep3: alpha=0.11957, lambda=0.123919 ## - Fold05.Rep3: alpha=0.11957, lambda=0.123919 ## + Fold05.Rep3: alpha=0.30234, lambda=0.027277 ## - Fold05.Rep3: alpha=0.30234, lambda=0.027277 ## + Fold05.Rep3: alpha=0.95329, lambda=0.004732 ## - Fold05.Rep3: alpha=0.95329, lambda=0.004732 ## + Fold05.Rep3: alpha=0.09178, lambda=0.186019 ## - Fold05.Rep3: alpha=0.09178, lambda=0.186019 ## + Fold05.Rep3: alpha=0.73123, lambda=0.160383 ## - Fold05.Rep3: alpha=0.73123, lambda=0.160383 ## + Fold05.Rep3: alpha=0.17634, lambda=0.003602 ## - Fold05.Rep3: alpha=0.17634, lambda=0.003602 ## + Fold05.Rep3: alpha=0.85495, lambda=0.687550 ## - Fold05.Rep3: alpha=0.85495, lambda=0.687550 ## + Fold05.Rep3: alpha=0.17667, lambda=2.997956 ## - Fold05.Rep3: alpha=0.17667, lambda=2.997956 ## + Fold06.Rep3: alpha=0.26250, lambda=0.261806 ## - Fold06.Rep3: alpha=0.26250, lambda=0.261806 ## + Fold06.Rep3: alpha=0.54570, lambda=0.002641 ## - Fold06.Rep3: alpha=0.54570, lambda=0.002641 ## + Fold06.Rep3: alpha=0.11957, lambda=0.123919 ## - Fold06.Rep3: alpha=0.11957, lambda=0.123919 ## + Fold06.Rep3: alpha=0.30234, lambda=0.027277 ## - Fold06.Rep3: alpha=0.30234, lambda=0.027277 ## + Fold06.Rep3: alpha=0.95329, lambda=0.004732 ## - Fold06.Rep3: alpha=0.95329, lambda=0.004732 ## + Fold06.Rep3: alpha=0.09178, lambda=0.186019 ## - Fold06.Rep3: alpha=0.09178, lambda=0.186019 ## + Fold06.Rep3: alpha=0.73123, lambda=0.160383 ## - Fold06.Rep3: alpha=0.73123, lambda=0.160383 ## + Fold06.Rep3: alpha=0.17634, lambda=0.003602 ## - Fold06.Rep3: alpha=0.17634, lambda=0.003602 ## + Fold06.Rep3: alpha=0.85495, lambda=0.687550 ## - Fold06.Rep3: alpha=0.85495, lambda=0.687550 ## + Fold06.Rep3: alpha=0.17667, lambda=2.997956 ## - Fold06.Rep3: alpha=0.17667, lambda=2.997956 ## + Fold07.Rep3: alpha=0.26250, lambda=0.261806 ## - Fold07.Rep3: alpha=0.26250, lambda=0.261806 ## + Fold07.Rep3: alpha=0.54570, lambda=0.002641 ## - Fold07.Rep3: alpha=0.54570, lambda=0.002641 ## + Fold07.Rep3: alpha=0.11957, lambda=0.123919 ## - Fold07.Rep3: alpha=0.11957, lambda=0.123919 ## + Fold07.Rep3: alpha=0.30234, lambda=0.027277 ## - Fold07.Rep3: alpha=0.30234, lambda=0.027277 ## + Fold07.Rep3: alpha=0.95329, lambda=0.004732 ## - Fold07.Rep3: alpha=0.95329, lambda=0.004732 ## + Fold07.Rep3: alpha=0.09178, lambda=0.186019 ## - Fold07.Rep3: alpha=0.09178, lambda=0.186019 ## + Fold07.Rep3: alpha=0.73123, lambda=0.160383 ## - Fold07.Rep3: alpha=0.73123, lambda=0.160383 ## + Fold07.Rep3: alpha=0.17634, lambda=0.003602 ## - Fold07.Rep3: alpha=0.17634, lambda=0.003602 ## + Fold07.Rep3: alpha=0.85495, lambda=0.687550 ## - Fold07.Rep3: alpha=0.85495, lambda=0.687550 ## + Fold07.Rep3: alpha=0.17667, lambda=2.997956 ## - Fold07.Rep3: alpha=0.17667, lambda=2.997956 ## + Fold08.Rep3: alpha=0.26250, lambda=0.261806 ## - Fold08.Rep3: alpha=0.26250, lambda=0.261806 ## + Fold08.Rep3: alpha=0.54570, lambda=0.002641 ## - Fold08.Rep3: alpha=0.54570, lambda=0.002641 ## + Fold08.Rep3: alpha=0.11957, lambda=0.123919 ## - Fold08.Rep3: alpha=0.11957, lambda=0.123919 ## + Fold08.Rep3: alpha=0.30234, lambda=0.027277 ## - Fold08.Rep3: alpha=0.30234, lambda=0.027277 ## + Fold08.Rep3: alpha=0.95329, lambda=0.004732 ## - Fold08.Rep3: alpha=0.95329, lambda=0.004732 ## + Fold08.Rep3: alpha=0.09178, lambda=0.186019 ## - Fold08.Rep3: alpha=0.09178, lambda=0.186019 ## + Fold08.Rep3: alpha=0.73123, lambda=0.160383 ## - Fold08.Rep3: alpha=0.73123, lambda=0.160383 ## + Fold08.Rep3: alpha=0.17634, lambda=0.003602 ## - Fold08.Rep3: alpha=0.17634, lambda=0.003602 ## + Fold08.Rep3: alpha=0.85495, lambda=0.687550 ## - Fold08.Rep3: alpha=0.85495, lambda=0.687550 ## + Fold08.Rep3: alpha=0.17667, lambda=2.997956 ## - Fold08.Rep3: alpha=0.17667, lambda=2.997956 ## + Fold09.Rep3: alpha=0.26250, lambda=0.261806 ## - Fold09.Rep3: alpha=0.26250, lambda=0.261806 ## + Fold09.Rep3: alpha=0.54570, lambda=0.002641 ## - Fold09.Rep3: alpha=0.54570, lambda=0.002641 ## + Fold09.Rep3: alpha=0.11957, lambda=0.123919 ## - Fold09.Rep3: alpha=0.11957, lambda=0.123919 ## + Fold09.Rep3: alpha=0.30234, lambda=0.027277 ## - Fold09.Rep3: alpha=0.30234, lambda=0.027277 ## + Fold09.Rep3: alpha=0.95329, lambda=0.004732 ## - Fold09.Rep3: alpha=0.95329, lambda=0.004732 ## + Fold09.Rep3: alpha=0.09178, lambda=0.186019 ## - Fold09.Rep3: alpha=0.09178, lambda=0.186019 ## + Fold09.Rep3: alpha=0.73123, lambda=0.160383 ## - Fold09.Rep3: alpha=0.73123, lambda=0.160383 ## + Fold09.Rep3: alpha=0.17634, lambda=0.003602 ## - Fold09.Rep3: alpha=0.17634, lambda=0.003602 ## + Fold09.Rep3: alpha=0.85495, lambda=0.687550 ## - Fold09.Rep3: alpha=0.85495, lambda=0.687550 ## + Fold09.Rep3: alpha=0.17667, lambda=2.997956 ## - Fold09.Rep3: alpha=0.17667, lambda=2.997956 ## + Fold10.Rep3: alpha=0.26250, lambda=0.261806 ## - Fold10.Rep3: alpha=0.26250, lambda=0.261806 ## + Fold10.Rep3: alpha=0.54570, lambda=0.002641 ## - Fold10.Rep3: alpha=0.54570, lambda=0.002641 ## + Fold10.Rep3: alpha=0.11957, lambda=0.123919 ## - Fold10.Rep3: alpha=0.11957, lambda=0.123919 ## + Fold10.Rep3: alpha=0.30234, lambda=0.027277 ## - Fold10.Rep3: alpha=0.30234, lambda=0.027277 ## + Fold10.Rep3: alpha=0.95329, lambda=0.004732 ## - Fold10.Rep3: alpha=0.95329, lambda=0.004732 ## + Fold10.Rep3: alpha=0.09178, lambda=0.186019 ## - Fold10.Rep3: alpha=0.09178, lambda=0.186019 ## + Fold10.Rep3: alpha=0.73123, lambda=0.160383 ## - Fold10.Rep3: alpha=0.73123, lambda=0.160383 ## + Fold10.Rep3: alpha=0.17634, lambda=0.003602 ## - Fold10.Rep3: alpha=0.17634, lambda=0.003602 ## + Fold10.Rep3: alpha=0.85495, lambda=0.687550 ## - Fold10.Rep3: alpha=0.85495, lambda=0.687550 ## + Fold10.Rep3: alpha=0.17667, lambda=2.997956 ## - Fold10.Rep3: alpha=0.17667, lambda=2.997956 ## + Fold01.Rep4: alpha=0.26250, lambda=0.261806 ## - Fold01.Rep4: alpha=0.26250, lambda=0.261806 ## + Fold01.Rep4: alpha=0.54570, lambda=0.002641 ## - Fold01.Rep4: alpha=0.54570, lambda=0.002641 ## + Fold01.Rep4: alpha=0.11957, lambda=0.123919 ## - Fold01.Rep4: alpha=0.11957, lambda=0.123919 ## + Fold01.Rep4: alpha=0.30234, lambda=0.027277 ## - Fold01.Rep4: alpha=0.30234, lambda=0.027277 ## + Fold01.Rep4: alpha=0.95329, lambda=0.004732 ## - Fold01.Rep4: alpha=0.95329, lambda=0.004732 ## + Fold01.Rep4: alpha=0.09178, lambda=0.186019 ## - Fold01.Rep4: alpha=0.09178, lambda=0.186019 ## + Fold01.Rep4: alpha=0.73123, lambda=0.160383 ## - Fold01.Rep4: alpha=0.73123, lambda=0.160383 ## + Fold01.Rep4: alpha=0.17634, lambda=0.003602 ## - Fold01.Rep4: alpha=0.17634, lambda=0.003602 ## + Fold01.Rep4: alpha=0.85495, lambda=0.687550 ## - Fold01.Rep4: alpha=0.85495, lambda=0.687550 ## + Fold01.Rep4: alpha=0.17667, lambda=2.997956 ## - Fold01.Rep4: alpha=0.17667, lambda=2.997956 ## + Fold02.Rep4: alpha=0.26250, lambda=0.261806 ## - Fold02.Rep4: alpha=0.26250, lambda=0.261806 ## + Fold02.Rep4: alpha=0.54570, lambda=0.002641 ## - Fold02.Rep4: alpha=0.54570, lambda=0.002641 ## + Fold02.Rep4: alpha=0.11957, lambda=0.123919 ## - Fold02.Rep4: alpha=0.11957, lambda=0.123919 ## + Fold02.Rep4: alpha=0.30234, lambda=0.027277 ## - Fold02.Rep4: alpha=0.30234, lambda=0.027277 ## + Fold02.Rep4: alpha=0.95329, lambda=0.004732 ## - Fold02.Rep4: alpha=0.95329, lambda=0.004732 ## + Fold02.Rep4: alpha=0.09178, lambda=0.186019 ## - Fold02.Rep4: alpha=0.09178, lambda=0.186019 ## + Fold02.Rep4: alpha=0.73123, lambda=0.160383 ## - Fold02.Rep4: alpha=0.73123, lambda=0.160383 ## + Fold02.Rep4: alpha=0.17634, lambda=0.003602 ## - Fold02.Rep4: alpha=0.17634, lambda=0.003602 ## + Fold02.Rep4: alpha=0.85495, lambda=0.687550 ## - Fold02.Rep4: alpha=0.85495, lambda=0.687550 ## + Fold02.Rep4: alpha=0.17667, lambda=2.997956 ## - Fold02.Rep4: alpha=0.17667, lambda=2.997956 ## + Fold03.Rep4: alpha=0.26250, lambda=0.261806 ## - Fold03.Rep4: alpha=0.26250, lambda=0.261806 ## + Fold03.Rep4: alpha=0.54570, lambda=0.002641 ## - Fold03.Rep4: alpha=0.54570, lambda=0.002641 ## + Fold03.Rep4: alpha=0.11957, lambda=0.123919 ## - Fold03.Rep4: alpha=0.11957, lambda=0.123919 ## + Fold03.Rep4: alpha=0.30234, lambda=0.027277 ## - Fold03.Rep4: alpha=0.30234, lambda=0.027277 ## + Fold03.Rep4: alpha=0.95329, lambda=0.004732 ## - Fold03.Rep4: alpha=0.95329, lambda=0.004732 ## + Fold03.Rep4: alpha=0.09178, lambda=0.186019 ## - Fold03.Rep4: alpha=0.09178, lambda=0.186019 ## + Fold03.Rep4: alpha=0.73123, lambda=0.160383 ## - Fold03.Rep4: alpha=0.73123, lambda=0.160383 ## + Fold03.Rep4: alpha=0.17634, lambda=0.003602 ## - Fold03.Rep4: alpha=0.17634, lambda=0.003602 ## + Fold03.Rep4: alpha=0.85495, lambda=0.687550 ## - Fold03.Rep4: alpha=0.85495, lambda=0.687550 ## + Fold03.Rep4: alpha=0.17667, lambda=2.997956 ## - Fold03.Rep4: alpha=0.17667, lambda=2.997956 ## + Fold04.Rep4: alpha=0.26250, lambda=0.261806 ## - Fold04.Rep4: alpha=0.26250, lambda=0.261806 ## + Fold04.Rep4: alpha=0.54570, lambda=0.002641 ## - Fold04.Rep4: alpha=0.54570, lambda=0.002641 ## + Fold04.Rep4: alpha=0.11957, lambda=0.123919 ## - Fold04.Rep4: alpha=0.11957, lambda=0.123919 ## + Fold04.Rep4: alpha=0.30234, lambda=0.027277 ## - Fold04.Rep4: alpha=0.30234, lambda=0.027277 ## + Fold04.Rep4: alpha=0.95329, lambda=0.004732 ## - Fold04.Rep4: alpha=0.95329, lambda=0.004732 ## + Fold04.Rep4: alpha=0.09178, lambda=0.186019 ## - Fold04.Rep4: alpha=0.09178, lambda=0.186019 ## + Fold04.Rep4: alpha=0.73123, lambda=0.160383 ## - Fold04.Rep4: alpha=0.73123, lambda=0.160383 ## + Fold04.Rep4: alpha=0.17634, lambda=0.003602 ## - Fold04.Rep4: alpha=0.17634, lambda=0.003602 ## + Fold04.Rep4: alpha=0.85495, lambda=0.687550 ## - Fold04.Rep4: alpha=0.85495, lambda=0.687550 ## + Fold04.Rep4: alpha=0.17667, lambda=2.997956 ## - Fold04.Rep4: alpha=0.17667, lambda=2.997956 ## + Fold05.Rep4: alpha=0.26250, lambda=0.261806 ## - Fold05.Rep4: alpha=0.26250, lambda=0.261806 ## + Fold05.Rep4: alpha=0.54570, lambda=0.002641 ## - Fold05.Rep4: alpha=0.54570, lambda=0.002641 ## + Fold05.Rep4: alpha=0.11957, lambda=0.123919 ## - Fold05.Rep4: alpha=0.11957, lambda=0.123919 ## + Fold05.Rep4: alpha=0.30234, lambda=0.027277 ## - Fold05.Rep4: alpha=0.30234, lambda=0.027277 ## + Fold05.Rep4: alpha=0.95329, lambda=0.004732 ## - Fold05.Rep4: alpha=0.95329, lambda=0.004732 ## + Fold05.Rep4: alpha=0.09178, lambda=0.186019 ## - Fold05.Rep4: alpha=0.09178, lambda=0.186019 ## + Fold05.Rep4: alpha=0.73123, lambda=0.160383 ## - Fold05.Rep4: alpha=0.73123, lambda=0.160383 ## + Fold05.Rep4: alpha=0.17634, lambda=0.003602 ## - Fold05.Rep4: alpha=0.17634, lambda=0.003602 ## + Fold05.Rep4: alpha=0.85495, lambda=0.687550 ## - Fold05.Rep4: alpha=0.85495, lambda=0.687550 ## + Fold05.Rep4: alpha=0.17667, lambda=2.997956 ## - Fold05.Rep4: alpha=0.17667, lambda=2.997956 ## + Fold06.Rep4: alpha=0.26250, lambda=0.261806 ## - Fold06.Rep4: alpha=0.26250, lambda=0.261806 ## + Fold06.Rep4: alpha=0.54570, lambda=0.002641 ## - Fold06.Rep4: alpha=0.54570, lambda=0.002641 ## + Fold06.Rep4: alpha=0.11957, lambda=0.123919 ## - Fold06.Rep4: alpha=0.11957, lambda=0.123919 ## + Fold06.Rep4: alpha=0.30234, lambda=0.027277 ## - Fold06.Rep4: alpha=0.30234, lambda=0.027277 ## + Fold06.Rep4: alpha=0.95329, lambda=0.004732 ## - Fold06.Rep4: alpha=0.95329, lambda=0.004732 ## + Fold06.Rep4: alpha=0.09178, lambda=0.186019 ## - Fold06.Rep4: alpha=0.09178, lambda=0.186019 ## + Fold06.Rep4: alpha=0.73123, lambda=0.160383 ## - Fold06.Rep4: alpha=0.73123, lambda=0.160383 ## + Fold06.Rep4: alpha=0.17634, lambda=0.003602 ## - Fold06.Rep4: alpha=0.17634, lambda=0.003602 ## + Fold06.Rep4: alpha=0.85495, lambda=0.687550 ## - Fold06.Rep4: alpha=0.85495, lambda=0.687550 ## + Fold06.Rep4: alpha=0.17667, lambda=2.997956 ## - Fold06.Rep4: alpha=0.17667, lambda=2.997956 ## + Fold07.Rep4: alpha=0.26250, lambda=0.261806 ## - Fold07.Rep4: alpha=0.26250, lambda=0.261806 ## + Fold07.Rep4: alpha=0.54570, lambda=0.002641 ## - Fold07.Rep4: alpha=0.54570, lambda=0.002641 ## + Fold07.Rep4: alpha=0.11957, lambda=0.123919 ## - Fold07.Rep4: alpha=0.11957, lambda=0.123919 ## + Fold07.Rep4: alpha=0.30234, lambda=0.027277 ## - Fold07.Rep4: alpha=0.30234, lambda=0.027277 ## + Fold07.Rep4: alpha=0.95329, lambda=0.004732 ## - Fold07.Rep4: alpha=0.95329, lambda=0.004732 ## + Fold07.Rep4: alpha=0.09178, lambda=0.186019 ## - Fold07.Rep4: alpha=0.09178, lambda=0.186019 ## + Fold07.Rep4: alpha=0.73123, lambda=0.160383 ## - Fold07.Rep4: alpha=0.73123, lambda=0.160383 ## + Fold07.Rep4: alpha=0.17634, lambda=0.003602 ## - Fold07.Rep4: alpha=0.17634, lambda=0.003602 ## + Fold07.Rep4: alpha=0.85495, lambda=0.687550 ## - Fold07.Rep4: alpha=0.85495, lambda=0.687550 ## + Fold07.Rep4: alpha=0.17667, lambda=2.997956 ## - Fold07.Rep4: alpha=0.17667, lambda=2.997956 ## + Fold08.Rep4: alpha=0.26250, lambda=0.261806 ## - Fold08.Rep4: alpha=0.26250, lambda=0.261806 ## + Fold08.Rep4: alpha=0.54570, lambda=0.002641 ## - Fold08.Rep4: alpha=0.54570, lambda=0.002641 ## + Fold08.Rep4: alpha=0.11957, lambda=0.123919 ## - Fold08.Rep4: alpha=0.11957, lambda=0.123919 ## + Fold08.Rep4: alpha=0.30234, lambda=0.027277 ## - Fold08.Rep4: alpha=0.30234, lambda=0.027277 ## + Fold08.Rep4: alpha=0.95329, lambda=0.004732 ## - Fold08.Rep4: alpha=0.95329, lambda=0.004732 ## + Fold08.Rep4: alpha=0.09178, lambda=0.186019 ## - Fold08.Rep4: alpha=0.09178, lambda=0.186019 ## + Fold08.Rep4: alpha=0.73123, lambda=0.160383 ## - Fold08.Rep4: alpha=0.73123, lambda=0.160383 ## + Fold08.Rep4: alpha=0.17634, lambda=0.003602 ## - Fold08.Rep4: alpha=0.17634, lambda=0.003602 ## + Fold08.Rep4: alpha=0.85495, lambda=0.687550 ## - Fold08.Rep4: alpha=0.85495, lambda=0.687550 ## + Fold08.Rep4: alpha=0.17667, lambda=2.997956 ## - Fold08.Rep4: alpha=0.17667, lambda=2.997956 ## + Fold09.Rep4: alpha=0.26250, lambda=0.261806 ## - Fold09.Rep4: alpha=0.26250, lambda=0.261806 ## + Fold09.Rep4: alpha=0.54570, lambda=0.002641 ## - Fold09.Rep4: alpha=0.54570, lambda=0.002641 ## + Fold09.Rep4: alpha=0.11957, lambda=0.123919 ## - Fold09.Rep4: alpha=0.11957, lambda=0.123919 ## + Fold09.Rep4: alpha=0.30234, lambda=0.027277 ## - Fold09.Rep4: alpha=0.30234, lambda=0.027277 ## + Fold09.Rep4: alpha=0.95329, lambda=0.004732 ## - Fold09.Rep4: alpha=0.95329, lambda=0.004732 ## + Fold09.Rep4: alpha=0.09178, lambda=0.186019 ## - Fold09.Rep4: alpha=0.09178, lambda=0.186019 ## + Fold09.Rep4: alpha=0.73123, lambda=0.160383 ## - Fold09.Rep4: alpha=0.73123, lambda=0.160383 ## + Fold09.Rep4: alpha=0.17634, lambda=0.003602 ## - Fold09.Rep4: alpha=0.17634, lambda=0.003602 ## + Fold09.Rep4: alpha=0.85495, lambda=0.687550 ## - Fold09.Rep4: alpha=0.85495, lambda=0.687550 ## + Fold09.Rep4: alpha=0.17667, lambda=2.997956 ## - Fold09.Rep4: alpha=0.17667, lambda=2.997956 ## + Fold10.Rep4: alpha=0.26250, lambda=0.261806 ## - Fold10.Rep4: alpha=0.26250, lambda=0.261806 ## + Fold10.Rep4: alpha=0.54570, lambda=0.002641 ## - Fold10.Rep4: alpha=0.54570, lambda=0.002641 ## + Fold10.Rep4: alpha=0.11957, lambda=0.123919 ## - Fold10.Rep4: alpha=0.11957, lambda=0.123919 ## + Fold10.Rep4: alpha=0.30234, lambda=0.027277 ## - Fold10.Rep4: alpha=0.30234, lambda=0.027277 ## + Fold10.Rep4: alpha=0.95329, lambda=0.004732 ## - Fold10.Rep4: alpha=0.95329, lambda=0.004732 ## + Fold10.Rep4: alpha=0.09178, lambda=0.186019 ## - Fold10.Rep4: alpha=0.09178, lambda=0.186019 ## + Fold10.Rep4: alpha=0.73123, lambda=0.160383 ## - Fold10.Rep4: alpha=0.73123, lambda=0.160383 ## + Fold10.Rep4: alpha=0.17634, lambda=0.003602 ## - Fold10.Rep4: alpha=0.17634, lambda=0.003602 ## + Fold10.Rep4: alpha=0.85495, lambda=0.687550 ## - Fold10.Rep4: alpha=0.85495, lambda=0.687550 ## + Fold10.Rep4: alpha=0.17667, lambda=2.997956 ## - Fold10.Rep4: alpha=0.17667, lambda=2.997956 ## + Fold01.Rep5: alpha=0.26250, lambda=0.261806 ## - Fold01.Rep5: alpha=0.26250, lambda=0.261806 ## + Fold01.Rep5: alpha=0.54570, lambda=0.002641 ## - Fold01.Rep5: alpha=0.54570, lambda=0.002641 ## + Fold01.Rep5: alpha=0.11957, lambda=0.123919 ## - Fold01.Rep5: alpha=0.11957, lambda=0.123919 ## + Fold01.Rep5: alpha=0.30234, lambda=0.027277 ## - Fold01.Rep5: alpha=0.30234, lambda=0.027277 ## + Fold01.Rep5: alpha=0.95329, lambda=0.004732 ## - Fold01.Rep5: alpha=0.95329, lambda=0.004732 ## + Fold01.Rep5: alpha=0.09178, lambda=0.186019 ## - Fold01.Rep5: alpha=0.09178, lambda=0.186019 ## + Fold01.Rep5: alpha=0.73123, lambda=0.160383 ## - Fold01.Rep5: alpha=0.73123, lambda=0.160383 ## + Fold01.Rep5: alpha=0.17634, lambda=0.003602 ## - Fold01.Rep5: alpha=0.17634, lambda=0.003602 ## + Fold01.Rep5: alpha=0.85495, lambda=0.687550 ## - Fold01.Rep5: alpha=0.85495, lambda=0.687550 ## + Fold01.Rep5: alpha=0.17667, lambda=2.997956 ## - Fold01.Rep5: alpha=0.17667, lambda=2.997956 ## + Fold02.Rep5: alpha=0.26250, lambda=0.261806 ## - Fold02.Rep5: alpha=0.26250, lambda=0.261806 ## + Fold02.Rep5: alpha=0.54570, lambda=0.002641 ## - Fold02.Rep5: alpha=0.54570, lambda=0.002641 ## + Fold02.Rep5: alpha=0.11957, lambda=0.123919 ## - Fold02.Rep5: alpha=0.11957, lambda=0.123919 ## + Fold02.Rep5: alpha=0.30234, lambda=0.027277 ## - Fold02.Rep5: alpha=0.30234, lambda=0.027277 ## + Fold02.Rep5: alpha=0.95329, lambda=0.004732 ## - Fold02.Rep5: alpha=0.95329, lambda=0.004732 ## + Fold02.Rep5: alpha=0.09178, lambda=0.186019 ## - Fold02.Rep5: alpha=0.09178, lambda=0.186019 ## + Fold02.Rep5: alpha=0.73123, lambda=0.160383 ## - Fold02.Rep5: alpha=0.73123, lambda=0.160383 ## + Fold02.Rep5: alpha=0.17634, lambda=0.003602 ## - Fold02.Rep5: alpha=0.17634, lambda=0.003602 ## + Fold02.Rep5: alpha=0.85495, lambda=0.687550 ## - Fold02.Rep5: alpha=0.85495, lambda=0.687550 ## + Fold02.Rep5: alpha=0.17667, lambda=2.997956 ## - Fold02.Rep5: alpha=0.17667, lambda=2.997956 ## + Fold03.Rep5: alpha=0.26250, lambda=0.261806 ## - Fold03.Rep5: alpha=0.26250, lambda=0.261806 ## + Fold03.Rep5: alpha=0.54570, lambda=0.002641 ## - Fold03.Rep5: alpha=0.54570, lambda=0.002641 ## + Fold03.Rep5: alpha=0.11957, lambda=0.123919 ## - Fold03.Rep5: alpha=0.11957, lambda=0.123919 ## + Fold03.Rep5: alpha=0.30234, lambda=0.027277 ## - Fold03.Rep5: alpha=0.30234, lambda=0.027277 ## + Fold03.Rep5: alpha=0.95329, lambda=0.004732 ## - Fold03.Rep5: alpha=0.95329, lambda=0.004732 ## + Fold03.Rep5: alpha=0.09178, lambda=0.186019 ## - Fold03.Rep5: alpha=0.09178, lambda=0.186019 ## + Fold03.Rep5: alpha=0.73123, lambda=0.160383 ## - Fold03.Rep5: alpha=0.73123, lambda=0.160383 ## + Fold03.Rep5: alpha=0.17634, lambda=0.003602 ## - Fold03.Rep5: alpha=0.17634, lambda=0.003602 ## + Fold03.Rep5: alpha=0.85495, lambda=0.687550 ## - Fold03.Rep5: alpha=0.85495, lambda=0.687550 ## + Fold03.Rep5: alpha=0.17667, lambda=2.997956 ## - Fold03.Rep5: alpha=0.17667, lambda=2.997956 ## + Fold04.Rep5: alpha=0.26250, lambda=0.261806 ## - Fold04.Rep5: alpha=0.26250, lambda=0.261806 ## + Fold04.Rep5: alpha=0.54570, lambda=0.002641 ## - Fold04.Rep5: alpha=0.54570, lambda=0.002641 ## + Fold04.Rep5: alpha=0.11957, lambda=0.123919 ## - Fold04.Rep5: alpha=0.11957, lambda=0.123919 ## + Fold04.Rep5: alpha=0.30234, lambda=0.027277 ## - Fold04.Rep5: alpha=0.30234, lambda=0.027277 ## + Fold04.Rep5: alpha=0.95329, lambda=0.004732 ## - Fold04.Rep5: alpha=0.95329, lambda=0.004732 ## + Fold04.Rep5: alpha=0.09178, lambda=0.186019 ## - Fold04.Rep5: alpha=0.09178, lambda=0.186019 ## + Fold04.Rep5: alpha=0.73123, lambda=0.160383 ## - Fold04.Rep5: alpha=0.73123, lambda=0.160383 ## + Fold04.Rep5: alpha=0.17634, lambda=0.003602 ## - Fold04.Rep5: alpha=0.17634, lambda=0.003602 ## + Fold04.Rep5: alpha=0.85495, lambda=0.687550 ## - Fold04.Rep5: alpha=0.85495, lambda=0.687550 ## + Fold04.Rep5: alpha=0.17667, lambda=2.997956 ## - Fold04.Rep5: alpha=0.17667, lambda=2.997956 ## + Fold05.Rep5: alpha=0.26250, lambda=0.261806 ## - Fold05.Rep5: alpha=0.26250, lambda=0.261806 ## + Fold05.Rep5: alpha=0.54570, lambda=0.002641 ## - Fold05.Rep5: alpha=0.54570, lambda=0.002641 ## + Fold05.Rep5: alpha=0.11957, lambda=0.123919 ## - Fold05.Rep5: alpha=0.11957, lambda=0.123919 ## + Fold05.Rep5: alpha=0.30234, lambda=0.027277 ## - Fold05.Rep5: alpha=0.30234, lambda=0.027277 ## + Fold05.Rep5: alpha=0.95329, lambda=0.004732 ## - Fold05.Rep5: alpha=0.95329, lambda=0.004732 ## + Fold05.Rep5: alpha=0.09178, lambda=0.186019 ## - Fold05.Rep5: alpha=0.09178, lambda=0.186019 ## + Fold05.Rep5: alpha=0.73123, lambda=0.160383 ## - Fold05.Rep5: alpha=0.73123, lambda=0.160383 ## + Fold05.Rep5: alpha=0.17634, lambda=0.003602 ## - Fold05.Rep5: alpha=0.17634, lambda=0.003602 ## + Fold05.Rep5: alpha=0.85495, lambda=0.687550 ## - Fold05.Rep5: alpha=0.85495, lambda=0.687550 ## + Fold05.Rep5: alpha=0.17667, lambda=2.997956 ## - Fold05.Rep5: alpha=0.17667, lambda=2.997956 ## + Fold06.Rep5: alpha=0.26250, lambda=0.261806 ## - Fold06.Rep5: alpha=0.26250, lambda=0.261806 ## + Fold06.Rep5: alpha=0.54570, lambda=0.002641 ## - Fold06.Rep5: alpha=0.54570, lambda=0.002641 ## + Fold06.Rep5: alpha=0.11957, lambda=0.123919 ## - Fold06.Rep5: alpha=0.11957, lambda=0.123919 ## + Fold06.Rep5: alpha=0.30234, lambda=0.027277 ## - Fold06.Rep5: alpha=0.30234, lambda=0.027277 ## + Fold06.Rep5: alpha=0.95329, lambda=0.004732 ## - Fold06.Rep5: alpha=0.95329, lambda=0.004732 ## + Fold06.Rep5: alpha=0.09178, lambda=0.186019 ## - Fold06.Rep5: alpha=0.09178, lambda=0.186019 ## + Fold06.Rep5: alpha=0.73123, lambda=0.160383 ## - Fold06.Rep5: alpha=0.73123, lambda=0.160383 ## + Fold06.Rep5: alpha=0.17634, lambda=0.003602 ## - Fold06.Rep5: alpha=0.17634, lambda=0.003602 ## + Fold06.Rep5: alpha=0.85495, lambda=0.687550 ## - Fold06.Rep5: alpha=0.85495, lambda=0.687550 ## + Fold06.Rep5: alpha=0.17667, lambda=2.997956 ## - Fold06.Rep5: alpha=0.17667, lambda=2.997956 ## + Fold07.Rep5: alpha=0.26250, lambda=0.261806 ## - Fold07.Rep5: alpha=0.26250, lambda=0.261806 ## + Fold07.Rep5: alpha=0.54570, lambda=0.002641 ## - Fold07.Rep5: alpha=0.54570, lambda=0.002641 ## + Fold07.Rep5: alpha=0.11957, lambda=0.123919 ## - Fold07.Rep5: alpha=0.11957, lambda=0.123919 ## + Fold07.Rep5: alpha=0.30234, lambda=0.027277 ## - Fold07.Rep5: alpha=0.30234, lambda=0.027277 ## + Fold07.Rep5: alpha=0.95329, lambda=0.004732 ## - Fold07.Rep5: alpha=0.95329, lambda=0.004732 ## + Fold07.Rep5: alpha=0.09178, lambda=0.186019 ## - Fold07.Rep5: alpha=0.09178, lambda=0.186019 ## + Fold07.Rep5: alpha=0.73123, lambda=0.160383 ## - Fold07.Rep5: alpha=0.73123, lambda=0.160383 ## + Fold07.Rep5: alpha=0.17634, lambda=0.003602 ## - Fold07.Rep5: alpha=0.17634, lambda=0.003602 ## + Fold07.Rep5: alpha=0.85495, lambda=0.687550 ## - Fold07.Rep5: alpha=0.85495, lambda=0.687550 ## + Fold07.Rep5: alpha=0.17667, lambda=2.997956 ## - Fold07.Rep5: alpha=0.17667, lambda=2.997956 ## + Fold08.Rep5: alpha=0.26250, lambda=0.261806 ## - Fold08.Rep5: alpha=0.26250, lambda=0.261806 ## + Fold08.Rep5: alpha=0.54570, lambda=0.002641 ## - Fold08.Rep5: alpha=0.54570, lambda=0.002641 ## + Fold08.Rep5: alpha=0.11957, lambda=0.123919 ## - Fold08.Rep5: alpha=0.11957, lambda=0.123919 ## + Fold08.Rep5: alpha=0.30234, lambda=0.027277 ## - Fold08.Rep5: alpha=0.30234, lambda=0.027277 ## + Fold08.Rep5: alpha=0.95329, lambda=0.004732 ## - Fold08.Rep5: alpha=0.95329, lambda=0.004732 ## + Fold08.Rep5: alpha=0.09178, lambda=0.186019 ## - Fold08.Rep5: alpha=0.09178, lambda=0.186019 ## + Fold08.Rep5: alpha=0.73123, lambda=0.160383 ## - Fold08.Rep5: alpha=0.73123, lambda=0.160383 ## + Fold08.Rep5: alpha=0.17634, lambda=0.003602 ## - Fold08.Rep5: alpha=0.17634, lambda=0.003602 ## + Fold08.Rep5: alpha=0.85495, lambda=0.687550 ## - Fold08.Rep5: alpha=0.85495, lambda=0.687550 ## + Fold08.Rep5: alpha=0.17667, lambda=2.997956 ## - Fold08.Rep5: alpha=0.17667, lambda=2.997956 ## + Fold09.Rep5: alpha=0.26250, lambda=0.261806 ## - Fold09.Rep5: alpha=0.26250, lambda=0.261806 ## + Fold09.Rep5: alpha=0.54570, lambda=0.002641 ## - Fold09.Rep5: alpha=0.54570, lambda=0.002641 ## + Fold09.Rep5: alpha=0.11957, lambda=0.123919 ## - Fold09.Rep5: alpha=0.11957, lambda=0.123919 ## + Fold09.Rep5: alpha=0.30234, lambda=0.027277 ## - Fold09.Rep5: alpha=0.30234, lambda=0.027277 ## + Fold09.Rep5: alpha=0.95329, lambda=0.004732 ## - Fold09.Rep5: alpha=0.95329, lambda=0.004732 ## + Fold09.Rep5: alpha=0.09178, lambda=0.186019 ## - Fold09.Rep5: alpha=0.09178, lambda=0.186019 ## + Fold09.Rep5: alpha=0.73123, lambda=0.160383 ## - Fold09.Rep5: alpha=0.73123, lambda=0.160383 ## + Fold09.Rep5: alpha=0.17634, lambda=0.003602 ## - Fold09.Rep5: alpha=0.17634, lambda=0.003602 ## + Fold09.Rep5: alpha=0.85495, lambda=0.687550 ## - Fold09.Rep5: alpha=0.85495, lambda=0.687550 ## + Fold09.Rep5: alpha=0.17667, lambda=2.997956 ## - Fold09.Rep5: alpha=0.17667, lambda=2.997956 ## + Fold10.Rep5: alpha=0.26250, lambda=0.261806 ## - Fold10.Rep5: alpha=0.26250, lambda=0.261806 ## + Fold10.Rep5: alpha=0.54570, lambda=0.002641 ## - Fold10.Rep5: alpha=0.54570, lambda=0.002641 ## + Fold10.Rep5: alpha=0.11957, lambda=0.123919 ## - Fold10.Rep5: alpha=0.11957, lambda=0.123919 ## + Fold10.Rep5: alpha=0.30234, lambda=0.027277 ## - Fold10.Rep5: alpha=0.30234, lambda=0.027277 ## + Fold10.Rep5: alpha=0.95329, lambda=0.004732 ## - Fold10.Rep5: alpha=0.95329, lambda=0.004732 ## + Fold10.Rep5: alpha=0.09178, lambda=0.186019 ## - Fold10.Rep5: alpha=0.09178, lambda=0.186019 ## + Fold10.Rep5: alpha=0.73123, lambda=0.160383 ## - Fold10.Rep5: alpha=0.73123, lambda=0.160383 ## + Fold10.Rep5: alpha=0.17634, lambda=0.003602 ## - Fold10.Rep5: alpha=0.17634, lambda=0.003602 ## + Fold10.Rep5: alpha=0.85495, lambda=0.687550 ## - Fold10.Rep5: alpha=0.85495, lambda=0.687550 ## + Fold10.Rep5: alpha=0.17667, lambda=2.997956 ## - Fold10.Rep5: alpha=0.17667, lambda=2.997956 ## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : There were missing values in resampled performance measures. ## Aggregating results ## Selecting tuning parameters ## Fitting alpha = 0.176, lambda = 0.0036 on full training set # Best tuning parameter elastic_reg$bestTune ## alpha lambda ## 3 0.1763378 0.003602272 elastic_reg &lt;- glmnet(train.mat , qual, alpha = elastic_reg$bestTune[1,1], lambda = elastic_reg$bestTune[1,2]) p.6 &lt;- predict(elastic_reg, s = elastic_reg$bestTune, newx = test.mat) RMSE.6 &lt;- RMSE(p.6,test$quality) RMSE.6 ## [1] 0.7314961 rp.6&lt;-round(p.6) acc.6 &lt;- mean(rp.6==test$quality) accuracy[3,1] &lt;- acc.6 "],["results.html", "Chapter 7 Results", " Chapter 7 Results accuracy ## Accuracy ## K-Nearest NA ## Random Forest 0.6844283 ## Regression 0.5385411 "],["references.html", "Chapter 8 References", " Chapter 8 References P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physiochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009. Puckette, Madeline. What Is Wine Exactly? Wine Folly, 5 Oct. 2015, winefolly.com/deep-dive/what-is-wine. Descriptive Statistics. Wikipedia, en.wikipedia.org/wiki/Descriptive_statistics. Accessed 29 May 2021. Linear, Lasso, and Ridge Regression with R Deepika Singh, 12 Nov. 2019, https://www.pluralsight.com/guides/linear-lasso-and-ridge-regression-with-r Ridge Regression, LASSO, and Elastic Nets Derek Kane, 23, Feb. 2015, https://www.slideshare.net/DerekKane/data-science-part-xii-ridge-regression-lasso-and-elastic-nets; https://www.youtube.com/watch?v=ipb2MhSRGdw Stephanie Glen. RMSE: Root Mean Square Error From StatisticsHowTo.com: Elementary Statistics for the rest of us! https://www.statisticshowto.com/probability-and-statistics/regression-analysis/rmse-root-mean-square-error/ Breiman, Leo, and Adele Cutler. Random Forests. Berkeley Statistics, www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm. Accessed 30 May 2021. "]]
