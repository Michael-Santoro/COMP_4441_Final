# Random Forest 

The Random Forest modeling technique is a supervised machine learning algorithm that works by utilizing the outcome of multiple decision trees to make a prediction for a data point. It can be used to solve both classification and regression problems, however, for our analysis we utilized it to help solve our research question of classifying the quality of wine samples from 1-10. The algorithm can be tuned by a user by inputting a particular number of trees and variables to be tested at each split in the tree, in order to make it more precise.

"Each tree within the Random Forest is grown as follows:

1. If the number of cases in the training set is N, sample N cases at random - but with replacement, from the original data. This sample will be the training set for growing the tree.
2. If there are M input variables, a number m<<M is specified such that at each node, m variables are selected at random out of the M and the best split on these m is used to split the node. The value of m is held constant during the forest growing.
3. Each tree is grown to the largest extent possible. There is no pruning." @random_forests


## Create Random Forest Model

We will start by running the randomForest function with our training dataset used to train the model. 

```{r}
set.seed(444)

rf.train <- train
rf.train$quality <-as.factor(rf.train$quality)

rf.test <- test
rf.test$quality <-as.factor(rf.test$quality)

# quality is a function of all other variables 
rf <- randomForest(quality~., data=rf.train)
print(rf)

```

The default number of trees was utilized to run our model (500) and the number of variables tried at each split in the tree was $3$ variables.

The table above shows the confusion matrix for the random forest model created with the training dataset. The out-of-bag (OOB) error is shown to be $32.91\%$. According to the class.error results, the model was most inaccurate when predicting wines with a quality of $3$ or $9$ with a $100\%$ error rate.  It was most accurate when predicting wines of value 6 with only a $22\%$ error rate. 


## Predicition and Confusion Matrix - Training Data

The following code runs a prediction of the quality values for the training set using the random forest model created above.  

As you can see, all 6 predictions in the predicted values vector are 100% accurate. This makes sense as the random forest model was built using the training data, so it has already "seen" these values. 

```{r}

p1 <- predict(rf, rf.train)
p1<- droplevels(p1) #drop any unused levels
head(p1) # predicted values
head(rf.train$quality) # actual values
```


```{r}
#install.packages('caret', dependencies = TRUE)

confusionMatrix(p1, rf.train$quality )
```

Using the confusion matrix function from the caret library, we can show the accuracy of our model when predicting the quality values for our training dataset. The accuracy is 100% when utilizing the training dataset with the random forest model trained by the training dataset. 

## Predicition and Confusion Matrix - Test Data

The following code runs a prediction of the quality values for the test dataset using the random forest model created with our training dataset from above.   

This model was able to predict 4 out of 6 of the first values accurately. 

```{r}

p2 <- predict(rf, rf.test)
p2<-droplevels(p2) # drop unused levels
head(p2) # predicted values
head(rf.test$quality) # actual values
```

```{r}
confusionMatrix(p2, rf.test$quality )
```

Using the confusion matrix function from the caret library, we can show the accuracy of our model in predicting the quality values for our test dataset. The accuracy is roughly 69% when utilizing the test dataset with the random forest model trained by the training dataset. 

## Tuning our Model

Now, that we have run our initial model, we can focus on fine tuning the random forest parameters in order to create a more accurate predictive model. 

### Plotting the Error Rate

Plotting the error rate to the number of trees can show us where our model has more or less the same level of effectiveness and we can choose a more accurate tree number. 

```{r}
plot(rf)
```
The model appears to have a drop off after about 300 trees and then is more or less constant, therefore, we can adjust our tree number in the model to be 300. 

### Tuning mTry

The mTry value reflects the number of variables tested at each node split. The tuneRF function can be used to determine what our mTry value should be. 

```{r}

set.seed(2222)

t <- tuneRF(rf.train[,-12], rf.train[,12], 
       stepFactor = 0.5, 
       plot=TRUE,
       ntreeTry = 100,
       trace=TRUE,
       improve=0.05)

```
This output indicates that our model hits its lowest error rate when the mTry value is 3, so we can adjust our model to reflect this new  value. 

## Recreate our Random Forest Model

The following code reruns our model utilizing the new tuning parameters found above.

```{r}
set.seed(444)

# quality is a function of all other variables 
rf <- randomForest(quality~., data=rf.train,
                   ntree=300,
                   mTry=3, 
                   importance=TRUE,
                   proximity=TRUE)
print(rf)

```

Our original OOB error rate was 32.91% and utilizing our new tuned parameters, our OOB error rate was 32.23%, so it was improved roughly 0.7%. 

## Rerun Predicition and Confusion Matrix - Training Data
```{r}

p1 <- predict(rf, rf.train)
p1<- droplevels(p1) # drop any unused levels
head(p1) # predicted values
head(rf.train$quality) # actual values
```

```{r}

confusionMatrix(p1, rf.train$quality )
```

Again, all of our predictions were 100% accurate as the training data had already been "seen" by the model. 

## Rerun Predicition and Confusion Matrix - Test Data
```{r}

p2 <- predict(rf, rf.test)
p2 <- droplevels(p2) # drop any unuused levels 
head(p2) # predicted values
head(rf.test$quality) # actual values
```

```{r}
confusionMatrix(p2, rf.test$quality )
```
The accuracy is roughly $68.24\%$ when utilizing the test dataset with the new random forest model. This is a $0.7\%$ improvement from the previous results of $68.91\%$.

## Variable Importance 

We are able to see the variables that had the highest level of importance in our model by running the varImpPlot function. 

```{r}
varImpPlot(rf, 
           main="Variable Importance")
```
This tells us that alcohol has the greatest importance in our model. Removing this variable would result in a 75% mean decrease in accuracy. 

## Accuracy

In the code below we are adding the percentage accuracy to the accuracy dataframe for later model comparison. 


```{r}

accuracy[2,1] <- confusionMatrix(p2, rf.test$quality )[["overall"]][["Accuracy"]]
```




