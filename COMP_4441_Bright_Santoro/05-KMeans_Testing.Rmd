# K-Nearest Neighbor

K-Nearest Neighbor (KNN) is another supervised machine learning algorithm that can classify a new data point by comparing the data points variable or factors to the variables or factors of its "nearest neighbors". KNN is also able to solve both classification and regression problems, however, we utilized it to solve our research question of classifying the quality of wine samples from 1-10. The algorithm for KNN determines a data points "nearest neighbors" by computing the Euclidean distance between the points. Then, similarly to Random Forest, KNN uses the consensus of those neighbors to predict the outcome variable for the new data point. @knn


## Data Partitioning
Our data has already been randomly partitioned into training and test data sets, however, the KNN algorithm we are utilizing within the caret library requires that we make our quality variables a "factor" data type. 

```{r}
set.seed(444)

knn.train <- train
knn.train$quality <-as.factor(knn.train$quality)

knn.test <- test
knn.test$quality <-as.factor(knn.test$quality)

str(knn.test)
str(knn.train)

```

Because our data was randomly partitioned you can see there are 7 levels within quality factor in the train data, while there are only 6 levels within the quality factor in the test data. This will have to be managed later using the "droplevels" function, to find the accuracy using the confusionmatrix.


## Determine the k-value

The k-value in the KNN algorithm determines how many of the nearest neighbors we will use to calculate our predicted quality.  There are many different methods for computing the value of K, however, caret provides a built in library function "train" to help determine what the k-value we should utilize is. 

"train" utilizes k-fold cross validation to determine the accuracy of each k-value value.  For our model, we used the repeated cross-validation method which indicates using cross-validation and repeating it a number of times to ensure accuracy.  For our parameters, we chose to repeat 5 times and left the default number of folds to 10, as that has been previously found to result in a low-bias model. 

Within the train function, we define the method for training our model to be "knn" for k-nearest neighbor and our tune length to be 40.  The tuneLength parameter tells the algorithm to try 40 different values of k. 

```{r}
set.seed(400)
tc <- trainControl(method="repeatedcv",repeats = 5) #,classProbs=TRUE,summaryFunction = twoClassSummary)
k.value <- train(quality ~ ., data = knn.train, method = "knn", trControl = tc, tuneLength=40)

k.value

```
The plot below shows the accuracy of the repeated cross-validation models to the k-values from train.  We can see that the highest level of accuracy was hit at k=37 and then appears to decrease in accuracy as k increases. 

```{r}
plot(k.value)
```

## KNN Prediction

Now that we have our k-value and knn model trained, we can use it to predict our test values. In this portion of code, we will also account for the missing level from the quality factor that we found earlier, so that we can compute our confusion matrix. 


```{r}
knnPredict <- predict(k.value,newdata = knn.test )
knnPredict.2<-droplevels(knnPredict)

#Get the confusion matrix to see accuracy value and other parameter values
confusionMatrix(knnPredict.2, knn.test$quality )
```

Our model was found to be about 56.75% accurate in predicting quality measures for our wine samples. 

## Accuracy

In the code below we are adding the percentage accuracy to the accuracy dataframe for later model comparison. 


```{r}

accuracy[1,1] <- confusionMatrix(knnPredict.2, knn.test$quality )[["overall"]][["Accuracy"]]
```




