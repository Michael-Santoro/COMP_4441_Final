# Data Cleaning and Partitioning

## Data Normalization

Our dataset only contains input variables and outcome variable, so there was no need to remove descriptive data elements.  However, we did normalize the input variables within the dataset to avoid any bias.

```{r}
set.seed(123)

# do not include wine type in partitioned data.
wine <-wine[,1:12]
#Normalization
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x))) }

#De-Normalization
de.normalize <- function(val){
  x <- wine[,1:12]
  return (val*(max(x) - min(x))+min(x))
}

#only normalize variables
wine.normal <- as.data.frame(lapply(wine[,1:11], normalize))

# add quality back 
wine.normal$quality = wine$quality

head(wine.normal)

```


## Data Partitioning

In order to determine whether our models are effective we must partition our dataset into a training and test set. We have decided to use $70\%$ of our data for training and $30\%$ for testing.

```{r}


ind <- sample(2, nrow(wine.normal), replace=TRUE, prob=c(0.7, 0.3))

train <- wine.normal[ind==1, ]
test <- wine.normal[ind==2,]

```

## Comparing Results

We will be comparing accuracy of each of the methods so we will set up an accuracy data frame to compare the effectiveness of each of the methods.

```{r}
accuracy <- data.frame(Accuracy=c(rep(NA,times=3)),row.names = c("K-Nearest","Random Forest","Regression"))

accuracy
```


