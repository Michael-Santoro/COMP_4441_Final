--- 
title: "COMP 4441 Final Project"
author: "Emma Bright and Michael Santoro"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: book.bib
biblio-style: apalike
link-citations: yes
github-repo: rstudio/bookdown-demo
description: "This bookdown contains the write up and analysis of the COMP 4441 Final Project for Michael Santoro and Emma Bright. "
---
--- 
title: "COMP 4441 Final Project"
author: "Emma Bright and Michael Santoro"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: book.bib
biblio-style: apalike
link-citations: yes
github-repo: rstudio/bookdown-demo
description: "This bookdown contains the write up and analysis of the COMP 4441 Final Project for Michael Santoro and Emma Bright. "
---


<!--chapter:end:index.Rmd-->



# Introduction

**Purpose**

Wine is an alcoholic beverage made from fermented grape juice. (Puckette) Generally, a wine's quality is determined by taste, smell, and visual tests performed by wine experts, sommeliers. These tests grade the wine on subjective measures: Acidity, Sweetness, Alcohol, Tannin and Aroma.  Though these tests are subjective, there is overlap between the observation of these characteristics and a wine's physiochemical properties.  For example, a wine's perceived acidity or tartness is based upon the pH of the wine and a wine's perceived sweetness is based upon the residual sugar of the wine.  

The goal of our research is to determine:

Can we predict the subjective quality rating of wine based solely on its physiochemical properties?

An accurate predictive model for determining an objective quality of wine would enable small wineries and vineyards, especially those from less established wine regions, to more accurately price their inventory and ensure a better return on investment.   

**Statistical or Analytical Method**

We will be using the the following statistical methods to create predictive models for quality of wine:

- Elastic Net Regression (LASSO and Ridge)
- Random Forest
- K-Nearest Neighbor



  

<!--chapter:end:01-Introduction.Rmd-->


# Descriptive Statistics

Placeholder


## Dataset
## Exploratory Analysis
### Descriptive Statistics
## Test for Normality

<!--chapter:end:02-DescriptiveStats.Rmd-->


# Data Cleaning and Partitioning

Placeholder


## Data Normalization
## Data Partitioning
## Comparing Results

<!--chapter:end:03-DataCleaning.Rmd-->


# Random Forest 

Placeholder


## Create Random Forest Model
## Predicition and Confusion Matrix - Training Data
## Predicition and Confusion Matrix - Test Data
## Tuning our Model
### Plotting the Error Rate
### Tuning mTry
## Recreate our Random Forest Model
## Rerun Predicition and Confusion Matrix - Training Data
## Rerun Predicition and Confusion Matrix - Test Data
## Variable Importance 
## Accuracy

<!--chapter:end:04-RandomForest.Rmd-->


# K-Nearest Neighbor

Placeholder


## Data Partitioning
## Determine the k-value
## KNN Prediction
## Accuracy

<!--chapter:end:05-KMeans_Testing.Rmd-->

---
output:
  pdf_document: default
  html_document: default
---
# Lasso Regression

## Linear Regression
### What is it?
@Website{RMSE} The simplest form of regression is linear regression, which assumes that the predictors have a linear relationship with the target variable. 
### Assumptions
* Input is assumed to have a Normal distribution and are not correlated with each other.

We saw in the Descriptive Stats section that visually this factor had a strong correlation with quality.

With these assumptions being true we can model quality with the following equation.

$$ q = a_1x_1 + a_2x_2 + a_3x_3 + \dots + a_2nx_n + b + \epsilon$$
Where $a_1, a_2, \dots, a_n$ are coefficients from the model. $x_1, x_2, \dots, x_n$ are the input variables to the model. $b$ is a factor of the model representing the y-intercept and $q$ is equal to the quality output. Finally, $\epsilon$ is the error.
The method that we use to optimize this model is Ordinary Least Squares (OLS).

### Single Variable

In the code-block below we output the summary of just the measurement alcohol into the linear model.
```{r}
lr.1 <- lm(quality~alcohol, data = train)
summary(lr.1)
```
Focusing on the p-value above we can tell that it is unlikely that by chance we will observe a relationship between alcohol and quality. Which fits our standard threshold of $5\%$.
We will use the Root Mean Square Error (RMSE) to compare the results of each of the models. (Glenn) The Root Mean Square Error (RMSE) is the standard deviation of the residuals (prediction errors). Residuals are a measure of how far from the regression line data points are; RMSE is a measure of how spread out these residuals are. In other words, it tells you how concentrated the data is around the line of best fit. We are looking for a value very close to zero here as the formula is as follows:

$$RMSE = \sqrt{\frac{\sum_{i=1}^{N}(predicted_i - actual_i)^2}{N}}$$


```{r}
p.1 <- predict(lr.1, newdata = test)
RMSE.1 <- RMSE(p.1,test$quality)
RMSE.1
```

Since we will be comparing results to other discrette statistical methods we now calculate an accuracy measure.
```{r}

rp.1<-round(p.1)
acc.1 <- mean(rp.1==test$quality)

```

### Multiple Variables
In the code block below we will input all of the variables and examine the output.

```{r}
lr.2 <- lm(quality~., data = train)
summary(lr.2)
```

Reviewing the output we find from the output that the variables that fall under our standard $5\%$ null-hypothesis threshold are: 'volatile.acidity', 'citric.acid', 'chlorides', 'sulphates', and 'alcohol'.

One of the issues with so many variables is there a risk of including variables that do not affect the outcome.


```{r}
p.2 <- predict(lr.2, newdata = test)

RMSE.2 <- RMSE(p.2,test$quality)
RMSE.2
```

```{r}
rp.2<-round(p.2)
mean(rp.2==test$quality)
```


### Less Variables
In the code block below we will input all of the variables and examine the output.

```{r}
lr.3 <- lm(quality~ volatile.acidity + citric.acid + chlorides + free.sulfur.dioxide + sulphates + alcohol, data = train)
summary(lr.3)
```

```{r}
p.3 <- predict(lr.3, newdata = test)

RMSE.3 <- RMSE(p.3,test$quality)
RMSE.3
```
```{r}
rp.3<-round(p.3)
mean(rp.3==test$quality)
```

## Ridge Regression

Ridge regression is an extension of linear regression where the loss function is modified to minimize the complexity of the model. This modification is done by adding a penalty parameter that is equivalent to the square of the magnitude of the coefficients. If we re-write the OLS in Matrix for below:
$$X_tX\beta = X_tY$$
To solbe for the $\beta$ terms to obtain the estimation model, we obtain the following.

$$\beta = (X^{'}X)^{-1}X^{'}Y$$

Ridge regression modifies the above by adding a small value of $\lambda$, to the diagonal elements of the correlation matrix.
$$\beta = (R+\lambda I)^{-1}X^{'}Y$$

We need to find an optimal value for the $lambda$ factor. The 'glmnet' feature makes this easy for us. It does not accept a data frame though so we need to make a matrix of the the input variables. The other thing that the chunk below is doing is creating a vector of $\lambda$ values that we want to try. Finally, we plot the 

```{r}
qual <- train$quality
cols <-colnames(train)
cols <- cols[1:length(cols)-1]
train.mat <- train %>% select(cols) %>% data.matrix()
lambdas <- 10^seq(3, -2, by = -.1)
ridge_reg <- cv.glmnet(train.mat, qual, alpha = 0, lambda = lambdas)
plot(ridge_reg)
```
The lowest point in the curve indicates the optimal lambda: the log value of lambda that best minimized the error in cross-validation. This can be pulled out of the glmnet output.

```{r}
opt_lambda <- ridge_reg$lambda.min
opt_lambda
```


```{r}
ridge_model <- ridge_reg$glmnet.fit
summary(ridge_model)
```

Next, we prepare our test data for prediction.
```{r}
test.mat <- test %>% select(cols) %>% data.matrix()
```

Next, we find the RMSE of the ridge regression.

```{r}
p.4 <- predict(ridge_model, s = opt_lambda, newx = test.mat)

RMSE.4 <- RMSE(p.4,test$quality)
RMSE.4
```
```{r}
rp.4<-round(p.4)
mean(rp.4==test$quality)
```


## Lasso Regression

The method for LASSO regression is much similar to the above.

```{r}
lambdas <- 10^seq(2, -3, by = -.1)

# Setting alpha = 1 implements lasso regression
lasso_reg <- cv.glmnet(train.mat, qual, alpha = 1, lambda = lambdas)
plot(lasso_reg)
```



```{r}
# Best 
lambda_best <- lasso_reg$lambda.min 
lambda_best
```


```{r}
p.5 <- predict(lasso_reg, s = lambda_best, newx = test.mat)

RMSE.5 <- RMSE(p.5,test$quality)
RMSE.5
```

```{r}
rp.5<-round(p.5)
mean(rp.5==test$quality)
```


### Features of the 'glmnet' Package

$\lambda$ is defined once and $\alpha$ where lasso is scaled by $\alpha$ and ridge penalty is scaled by $(1-\alpha$).

### Elastic Net Regression
Elastic net regression combines the properties of ridge and lasso regression. So, we need a technique to cycle through find the most optimal limiting factors.


```{r, echo=FALSE}
# Set training control
train_cont <- trainControl(method = "repeatedcv",
                              number = 10,
                              repeats = 5,
                              search = "random",
                              verboseIter = TRUE)

# Train the model
elastic_reg <- train(quality ~ .,
                           data = train,
                           method = "glmnet",
                           preProcess = c("center", "scale"),
                           tuneLength = 10,
                           trControl = train_cont)



```

```{r}
# Best tuning parameter
elastic_reg$bestTune
elastic_reg <- glmnet(train.mat , qual, alpha = elastic_reg$bestTune[1,1], lambda = elastic_reg$bestTune[1,2])
```


```{r}
p.6 <- predict(elastic_reg, s = elastic_reg$bestTune, newx = test.mat)

RMSE.6 <- RMSE(p.6,test$quality)
RMSE.6
```

```{r}
rp.6<-round(p.6)
acc.6 <- mean(rp.6==test$quality)
```


```{r}
accuracy[3,1] <- acc.6
```


<!--chapter:end:03-LassoRegression.Rmd-->

# Results


```{r}
accuracy
```


<!--chapter:end:08-Results.Rmd-->

# References

P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis.
Modeling wine preferences by data mining from physiochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.

Puckette, Madeline. “What Is Wine Exactly?” Wine Folly, 5 Oct. 2015, winefolly.com/deep-dive/what-is-wine.

“Descriptive Statistics.” Wikipedia, en.wikipedia.org/wiki/Descriptive_statistics. Accessed 29 May 2021.

"Linear, Lasso, and Ridge Regression with R" Deepika Singh, 12 Nov. 2019, https://www.pluralsight.com/guides/linear-lasso-and-ridge-regression-with-r

"Ridge Regression, LASSO, and Elastic Nets" Derek Kane, 23, Feb. 2015, https://www.slideshare.net/DerekKane/data-science-part-xii-ridge-regression-lasso-and-elastic-nets; https://www.youtube.com/watch?v=ipb2MhSRGdw

Stephanie Glen. "RMSE: Root Mean Square Error" From StatisticsHowTo.com: Elementary Statistics for the rest of us! https://www.statisticshowto.com/probability-and-statistics/regression-analysis/rmse-root-mean-square-error/

Breiman, Leo, and Adele Cutler. “Random Forests.” Berkeley Statistics, www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm. Accessed 30 May 2021.

<!--chapter:end:07-References.Rmd-->

