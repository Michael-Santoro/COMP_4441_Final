

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(quantreg)
library(glmnet)
library(caret)
```

## Load Data
We have taken a data-set from Kaggle: https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009 
This data-set inlcudes the measurements from the wine along with its quality rating.

```{r}
wine<-read.table("data/winequality-red.csv",stringsAsFactors = FALSE,
                sep=",",header = TRUE)
glimpse(wine)
```

## Process Reference

The process was followed from this site: https://www.pluralsight.com/guides/linear-lasso-and-ridge-regression-with-r

## Data Partitioning

The below code takes 70% of the data for training and 30% of the code for testing.

```{r}
set.seed(100) 

index = sample(1:nrow(wine), 0.7*nrow(wine)) 

train = wine[index,] # Create the training data 
test = wine[-index,] # Create the test data

dim(train)
dim(test)
```

## Scaling the Numeric Features

```{r}
cols <- colnames(wine)
pre_proc_val <- preProcess(train[,cols], method = c("center", "scale"))

train[,cols] = predict(pre_proc_val, train[,cols])
test[,cols] = predict(pre_proc_val, test[,cols])

summary(train)
```

## Linear Regression
### What is it?
The simplest form of regression is linear regression, which assumes that the predictors have a linear relationship with the target variable. 
### Assumptions
* Input is assumed to have a Normal distribution and are not correlated with each other.

We saw in the Descriptive Stats section *that this was not the case*

With these assumptions being true we can model quality with the following equation.

$$ q = a_1x_1 + a_2x_2 + a_3x_3 + \dots + a_2nx_n + b$$
Where $a_1, a_2, \dots, a_n$ are coefficients from the model. $x_1, x_2, \dots, x_n$ are the input variables to the model. $b$ is a factor of the model and $q$ is equal to the quality output.

### Single Variable

In the code-block below we output the summary of just the measurement alcohol into the linear model.
```{r}
summary(lm(quality~alcohol, data = train))
```
Focusing on the p-value above we can tell that it is unlikely that by chance we will observe a relationship between alcohol and quality.

### Multiple Variables
In the code block below we will input all of the variables and examine the output.

```{r}
lr <- lm(quality~fixed.acidity + volatile.acidity + citric.acid + residual.sugar + chlorides + free.sulfur.dioxide + density + pH + sulphates + alcohol, data = train)
summary(lr)

```

Reviewing the output we find that the inputs: 'volatile.acidity', 'sulphates', and 'alcohol'.

## Regularization

Linear regression algorithm works by selecting coefficients for each independent variable that minimizes a loss function. However, if the coefficients are large, they can lead to over-fitting on the training dataset, and such a model will not generalize well on the unseen test data. To overcome this shortcoming, we'll do regularization, which penalizes large coefficients. The following sections of the guide will discuss various regularization algorithms.

We will be using the glmnet() package to build the regularized regression models. The glmnet function does not work with dataframes, so we need to create a numeric matrix for the training features and a vector of target values.

The lines of code below perform the task of creating model matrix using the dummyVars function from the caret package. The predict function is then applied to create numeric model matrices for training and test.

```{r}
dummies <- dummyVars(quality~., data = wine)

train_dummies = predict(dummies, newdata = train)

test_dummies = predict(dummies, newdata = test)

print(dim(train_dummies)); print(dim(test_dummies))
```

## Ridge Regression

Ridge regression is an extension of linear regression where the loss function is modified to minimize the complexity of the model. This modification is done by adding a penalty parameter that is equivalent to the square of the magnitude of the coefficients.

```{r}
eval_results <- function(true, predicted, df) {
  SSE <- sum((predicted - true)^2)
  SST <- sum((true - mean(true))^2)
  R_square <- 1 - SSE / SST
  RMSE = sqrt(SSE/nrow(df))

  
  # Model performance metrics
data.frame(
  RMSE = RMSE,
  Rsquare = R_square
)
  
}
```


```{r}
x = as.matrix(train_dummies)
y_train = train$quality

x_test = as.matrix(test_dummies)
y_test = test$unemploy

lambdas <- 10^seq(2, -3, by = -.1)
ridge_reg = glmnet(x, y_train, nlambda = 25, alpha = 0, family = 'gaussian', lambda = lambdas)

summary(ridge_reg)
```

```{r}
cv_ridge <- cv.glmnet(x, y_train, alpha = 0, lambda = lambdas)
optimal_lambda <- cv_ridge$lambda.min
optimal_lambda
```


## Lasso Regression

```{r}
lambdas <- 10^seq(2, -3, by = -.1)

# Setting alpha = 1 implements lasso regression
lasso_reg <- cv.glmnet(x, y_train, alpha = 1, lambda = lambdas, standardize = TRUE, nfolds = 5)

# Best 
lambda_best <- lasso_reg$lambda.min 
lambda_best
```

```{r}
lasso_model <- glmnet(x, y_train, alpha = 1, lambda = lambda_best, standardize = TRUE)

predictions_train <- predict(lasso_model, s = lambda_best, newx = x)
eval_results(y_train, predictions_train, train)

predictions_test <- predict(lasso_model, s = lambda_best, newx = x_test)
eval_results(y_test, predictions_test, test)
```


### Features of the 'glmnet' Package

$\lambda$ is defined once and $\alpha$ where lasso is scaled by $\alpha$ and ridge penalty is scaled by $(1-\alpha$).

### Elastic Net Regression
Elastic net regression combines the properties of ridge and lasso regression


```{r}
# Set training control
train_cont <- trainControl(method = "repeatedcv",
                              number = 10,
                              repeats = 5,
                              search = "random",
                              verboseIter = TRUE)

# Train the model
elastic_reg <- train(quality ~ .,
                           data = train,
                           method = "glmnet",
                           preProcess = c("center", "scale"),
                           tuneLength = 10,
                           trControl = train_cont)


# Best tuning parameter
elastic_reg$bestTune
```
```{r}
# Make predictions on training set
predictions_train <- predict(elastic_reg, x)
eval_results(y_train, predictions_train, train) 

# Make predictions on test set
predictions_test <- predict(elastic_reg, x_test)
eval_results(y_test, predictions_test, test)
```

