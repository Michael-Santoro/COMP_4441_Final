---
title: "RandomForest_2"
author: "Mike Santoro"
date: "5/26/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(dplyr)
library(randomForest)
```

## Load Data
We have taken a data-set from Kaggle: https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009 
This data-set inlcudes the measurements from the wine along with its quality rating.

```{r}
wine<-read.table("data/winequality-red.csv",stringsAsFactors = FALSE,
                sep=",",header = TRUE)
glimpse(wine)
```


## Refeerence Website

https://www.blopig.com/blog/2017/04/a-very-basic-introduction-to-random-forests-using-r/

## Split the Data into Traning and Test Set

```{r}
# Set random seed to make results reproducible:
set.seed(17)
# Calculate the size of each of the data sets:
data_set_size <- floor(nrow(wine)/2)
# Generate a random sample of "data_set_size" indexes
indexes <- sample(1:nrow(wine), size = data_set_size)
# Assign the data to the correct sets
training <- wine[indexes,]
validation1 <- wine[-indexes,]
```

## An Important Overfitting Point
1- The size of your data set usually imposes a hard limit on how many features you can consider. This occurs due to the curse of dimensionality, i.e. your data becomes sparser and sparser as you increase the number of features considered, which usually leads to overfitting. While there is no rule of thumb relating to how many features vs.  the number of observations you should use, I try to keep e^Nf < No (Nf = number of features, No = number of observations) to minimise overfitting [this is not always possible and it does not ensure that we wonâ€™t overfit]. In this case, our training set has 75 observations, which suggests that using four features (e^4 ~ 54.6) is not entirely absurd. Obviously, this depends on your data, so we will cover some further overfitting checks later on.


## Perform training:

```{r}
# Perform training:
rf_classifier = randomForest(quality ~ ., data=training, ntree=100, mtry=2, importance=TRUE)
```

```{r}
rf_classifier
```

```{r}
varImpPlot(rf_classifier)
```

```{r}
# Validation set assessment #1: looking at confusion matrix
prediction_for_table <- predict(rf_classifier,validation1)
table(observed=validation1,predicted=prediction_for_table)

```

```{r}
# Validation set assessment #2: ROC curves and AUC
# Needs to import ROCR package for ROC curve plotting:
library(ROCR)
# Calculate the probability of new observations belonging to each class
# prediction_for_roc_curve will be a matrix with dimensions data_set_size x number_of_classes
prediction_for_roc_curve <- predict(rf_classifier,validation1[,-5],type="prob")
# Use pretty colours:
pretty_colours <- c("#F8766D","#00BA38","#619CFF")
# Specify the different classes 
classes <- levels(validation1$Species)
# For each class
for (i in 1:3)
{
 # Define which observations belong to class[i]
 true_values <- ifelse(validation1[,5]==classes[i],1,0)
 # Assess the performance of classifier for class[i]
 pred <- prediction(prediction_for_roc_curve[,i],true_values)
 perf <- performance(pred, "tpr", "fpr")
 if (i==1)
 {
     plot(perf,main="ROC Curve",col=pretty_colours[i]) 
 }
 else
 {
     plot(perf,main="ROC Curve",col=pretty_colours[i],add=TRUE) 
 }
 # Calculate the AUC and print it to screen
 auc.perf <- performance(pred, measure = "auc")
 print(auc.perf@y.values)
}
```







